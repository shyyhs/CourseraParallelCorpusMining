{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/song/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/song/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate import bleu\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "nltk.download('punkt')\n",
    "\n",
    "from scipy import spatial\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from utils import *\n",
    "from laserembeddings import Laser\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from mincostflow import MCFGraph\n",
    "\n",
    "# global variables\n",
    "from bert_score import score\n",
    "def load_word2vec_model(model_path):\n",
    "    return gensim.models.KeyedVectors.load_word2vec_format(model_path, unicode_errors='ignore', limit=500000)\n",
    "word2vec_zh_model = load_word2vec_model('/mnt/zamia/song/lrec/model/word2vec/zh/model.txt') \n",
    "word2vec_en_model = load_word2vec_model('/mnt/zamia/song/lrec/model/word2vec/en/model.txt') \n",
    "word2vec_ja_model = load_word2vec_model('/mnt/zamia/song/lrec/model/word2vec/ja/model.txt') \n",
    "laser_model = Laser()\n",
    "sentbert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "model_dict = {'zh': word2vec_zh_model, 'en': word2vec_en_model, 'ja': word2vec_ja_model, 'laser': laser_model, 'sentbert': sentbert_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sentence2vector(sentence, lang, word2vec_lang1_model, word2vec_lang2_model, config):\n",
    "    if (lang == config['lang1']):\n",
    "        model = word2vec_lang1_model\n",
    "    elif (lang == config['lang2']):\n",
    "        model = word2vec_lang2_model\n",
    "    vecs = []\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            word = word.lower()\n",
    "            vecs.append(model[word])\n",
    "        except:\n",
    "            #print (\"no word in model: \", word)\n",
    "            continue\n",
    "    return vecs\n",
    "\n",
    "def laser_sentence2vector(sentence, lang, model):\n",
    "    return model.embed_sentences(detokenize(sentence, detokenizer, lang), lang)\n",
    "\n",
    "def sentbert_sentence2vector(sentence, lang, model):\n",
    "    return [model.encode(detokenize(sentence, detokenizer, lang))]\n",
    "\n",
    "def sentences2vectors(sentences, lang, config):\n",
    "    if (config['model'] == 'laser'):\n",
    "        vecs = [laser_sentence2vector(sentence, lang, laser_model) for sentence in sentences]\n",
    "    elif (config['model'] == 'sentbert'):\n",
    "        vecs = [sentbert_sentence2vector(sentence, lang, sentbert_model) for sentence in sentences]\n",
    "    elif (config['model'] == 'word2vec'):\n",
    "        vecs = [word2vec_sentence2vector(sentence, lang, word2vec_lang1_model, word2vec_lang2_model, config) for sentence in sentences]\n",
    "    return vecs\n",
    "\n",
    "def connect_vec_list(vec_list1, vec_lists2):\n",
    "    return (vec_list1 + vec_lists2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_predict_ids_from_path(path, n1, n2):\n",
    "    predict_ids = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while (i<n1 and j<n2):\n",
    "        nexti, nextj = path[(i,j)]\n",
    "        predict_ids.append((list(range(i, nexti+1)), list(range(j, nextj+1))))\n",
    "        i = nexti+1\n",
    "        j = nextj+1\n",
    "    return predict_ids\n",
    "\n",
    "def dp_sub(i, j, n1, n2, maxcombine, checked, result, path, f, threshold=0):\n",
    "    if (i>=n1 or j>=n2): return 0\n",
    "    if (checked.get((i,j)) == True): return result[(i, j)]\n",
    "    checked[(i,j)] = True \n",
    "    result[(i,j)] = 0\n",
    "    # zero match\n",
    "    zero_match_1 = dp_sub(i+1, j, n1, n2, maxcombine, checked, result, path, f, threshold)\n",
    "    zero_match_2 =dp_sub(i+1, j, n1, n2, maxcombine, checked, result, path, f, threshold) \n",
    "    if (zero_match_1>result[(i,j)]):\n",
    "        result[(i,j)] = zero_match_1\n",
    "        path[(i,j)] = (i+1,j)\n",
    "    if (zero_match_2>result[(i,j)]):\n",
    "        result[(i,j)] = zero_match_2\n",
    "        path[(i,j)] = (i,j+1)\n",
    "    #dp_sub = max(zero_match_1, zero_match_2)\n",
    "\n",
    "    # non-zero match\n",
    "    for k in range(maxcombine):\n",
    "        if (i+k>=n1): break\n",
    "        for l in range(maxcombine):\n",
    "            if (j+l>=n2): break\n",
    "            if (f[(i, k, j, l)] > threshold):\n",
    "                tmp_res = dp_sub(i+k+1, j+l+1, n1, n2, maxcombine, checked, result, path, f, threshold) + f[(i, k, j, l)]\n",
    "                if (tmp_res > result[(i, j)]):\n",
    "                    path[(i, j)] = (i+k, j+l)\n",
    "                    result[(i, j)] = tmp_res\n",
    "    return result[(i, j)]\n",
    "\n",
    "def dp(f, lang_len, lang_trans_len, config):\n",
    "    # consider zero match\n",
    "    checked = {}\n",
    "    result = {}\n",
    "    path = {}\n",
    "    n = int(config['max_combine']) # how many sentences combination is permitted\n",
    "    threshold = float(config['threshold'])\n",
    "    dp_sub(0, 0, lang_len, lang_trans_len, n, checked, result, path, f, threshold)\n",
    "    predict_ids = recover_predict_ids_from_path(path, lang_len, lang_trans_len)\n",
    "    return predict_ids\n",
    "\n",
    "def maxcostflow(f, lang_len, lang_trans_len, config):\n",
    "    s_point = 0\n",
    "    t_point = lang_len+lang_trans_len+1\n",
    "\n",
    "    src_start = 1\n",
    "    src_end = 1+lang_len\n",
    "    tgt_start = 1+lang_len\n",
    "    tgt_end = 1+lang_len+lang_trans_len\n",
    "\n",
    "    # constract the graph\n",
    "    g=MCFGraph(1+lang_len+lang_trans_len+1)\n",
    "    for i in range(src_start, src_end):\n",
    "        g.add_edge(s_point, i, 1, 0)\n",
    "    for i in range(tgt_start, tgt_end):\n",
    "        g.add_edge(i, t_point, 1, 0)\n",
    "    for i in range(src_start, src_end):\n",
    "        for j in range(tgt_start, tgt_end):\n",
    "            src_id = i-src_start\n",
    "            tgt_id = j-tgt_start\n",
    "            score = 1-f.get((src_id, 0, tgt_id, 0), 0)\n",
    "            score = int(score*1000)\n",
    "            g.add_edge(i, j, 1, score)\n",
    "    # mincostmaxflow\n",
    "    g.flow(s_point, t_point)\n",
    "\n",
    "    # recover the result\n",
    "    predict_ids = []\n",
    "    for e in g.edges():\n",
    "        if (e.src>=src_start and e.src<src_end and e.dst>=tgt_start and e.dst<tgt_end and e.flow==1):\n",
    "            i = e.src-src_start\n",
    "            j = e.dst-tgt_start\n",
    "            predict_ids.append(([i], [j]))\n",
    "    return predict_ids\n",
    "\n",
    "def greedy(f, lang_len, lang_trans_len, config):\n",
    "    match = [-1 for i in range(lang_len)]\n",
    "    predict_ids = []\n",
    "    for i in range(lang_len):\n",
    "        for j in range(lang_trans_len):\n",
    "            if (f.get((i, 0, j, 0), 0) > f.get((i, 0, match[i], 0), 0)):\n",
    "                match[i] = j\n",
    "        predict_ids.append(([i], [match[i]]))\n",
    "    return predict_ids\n",
    "\n",
    "def align_matrix(f, lang_len, lang_trans_len, config):\n",
    "    if (config['algorithm'] == 'dp'):\n",
    "        predict_ids = dp(f, lang_len, lang_trans_len, config)\n",
    "    elif (config['algorithm'] == 'greedy'):\n",
    "        predict_ids = greedy(f, lang_len, lang_trans_len, config)\n",
    "    elif (config['algorithm'] == 'maxmatch'):\n",
    "        predict_ids = maxcostflow(f, lang_len, lang_trans_len, config)\n",
    "    return predict_ids\n",
    "\n",
    "\n",
    "# part of alignment using cosine similarity\n",
    "def get_similarity_matrix_1way(lang_embeddings, lang_trans_embeddings, config):\n",
    "    lang_len = len(lang_embeddings)\n",
    "    lang_trans_len = len(lang_trans_embeddings)\n",
    "    n = int(config['max_combine'])\n",
    "    f = {}\n",
    "\n",
    "    avg_src_embeddings = {}\n",
    "    avg_src_trans_embeddings = {}\n",
    "\n",
    "    for i in range(lang_len):\n",
    "        combined_src_embeddings = lang_embeddings[i]\n",
    "        for j in range(n):\n",
    "            if (i+j>=lang_len): break\n",
    "            if (j!=0):\n",
    "                combined_src_embeddings += lang_embeddings[i+j]\n",
    "            avg_src_embeddings[(i, j)] = get_average_vec(combined_src_embeddings)\n",
    "\n",
    "    for i in range(lang_trans_len):\n",
    "        combined_src_trans_embeddings = lang_trans_embeddings[i]\n",
    "        for j in range(n):\n",
    "            if (i+j>=lang_trans_len): break\n",
    "            if (j!=0):\n",
    "                combined_src_trans_embeddings += lang_trans_embeddings[i+j]\n",
    "            avg_src_trans_embeddings[(i, j)] = get_average_vec(combined_src_trans_embeddings)\n",
    "\n",
    "    #print ('Pre-processing done for avg embeddings')\n",
    "\n",
    "    for i in range(lang_len):\n",
    "        for j in range(n):\n",
    "            if (i+j>=lang_len): break\n",
    "            for k in range(lang_trans_len):\n",
    "                for l in range(n):\n",
    "                    if (k+l>=lang_trans_len): break\n",
    "                    avg_src_embedding = avg_src_embeddings[(i,j)]\n",
    "                    avg_src_trans_embedding = avg_src_trans_embeddings[(k,l)]\n",
    "                    if (config['measure'] == 'cosine_similarity'):\n",
    "                        f[(i, j, k, l)] = get_cos_similarity(avg_src_embedding, avg_src_trans_embedding)\n",
    "                    elif (config['measure'] == 'distance'): \n",
    "                        f[(i, j, k, l)] =  distance(avg_src_embedding, avg_src_trans_embedding)\n",
    "    if (config['measure'] == 'distance'): \n",
    "        # extract all distances\n",
    "        distances = list(f.values())\n",
    "        min_distance = min(distances)\n",
    "        max_distance = max(distances)\n",
    "        diff_distance = max_distance-min_distance\n",
    "        #print (min_distance, max_distance, diff_distance)\n",
    "        for i in range(lang_len):\n",
    "            for j in range(n):\n",
    "                if (i+j>=lang_len): break\n",
    "                for k in range(lang_trans_len):\n",
    "                    for l in range(n):\n",
    "                        if (k+l>=lang_trans_len): break\n",
    "                        f[(i, j, k, l)] = float(max_distance-f[(i, j, k, l)])/diff_distance\n",
    "    return f\n",
    "    \n",
    "def similarity_matrix_merge(f_src2tgt, f_tgt2src, lang_len, lang_trans_len, config):\n",
    "    mix_method = config[\"mix_method\"]\n",
    "    n = int(config['max_combine'])\n",
    "    f = {}\n",
    "    for i in range(lang_len):\n",
    "        for j in range(n):\n",
    "            if (i+j>=lang_len): break\n",
    "            for k in range(lang_trans_len):\n",
    "                for l in range(n):\n",
    "                    if (k+l>=lang_trans_len): break\n",
    "                    s1 = f_src2tgt[(i, j, k, l)]\n",
    "                    s2 = f_tgt2src[(k, l, i, j)] \n",
    "                    if (mix_method == \"average\"):\n",
    "                        f[(i, j, k, l)] = (s1+s2)/2\n",
    "                    elif (mix_method == \"max\"):\n",
    "                        f[(i, j, k, l)] = max(s1, s2)\n",
    "    return f\n",
    "\n",
    "def get_BLEU4(sentence1, sentence2):\n",
    "    \"\"\"\n",
    "    return BLEU4 scores for sentence1 and sentence2\n",
    "    \"\"\"\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    bleu4_score = sentence_bleu([sentence1], sentence2, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    return bleu4_score\n",
    "    \n",
    "def get_similarity_matrix_BLEU_1way(lang1_lines, lang1_trans_lines, config):\n",
    "    lang_len = len(lang1_lines)\n",
    "    lang_trans_len = len(lang1_trans_lines)\n",
    "    f = {}\n",
    "    n = int(config['max_combine'])\n",
    "    for i in range(lang_len):\n",
    "        for j in range(n):\n",
    "            if (i+j>=lang_len): break\n",
    "            for k in range(lang_trans_len):\n",
    "                for l in range(n):\n",
    "                    if (k+l>=lang_trans_len): break\n",
    "                    combined_src_sentence = ' '.join(lang1_lines[i:i+j+1])\n",
    "                    combined_trans_src_sentence = ' '.join(lang1_trans_lines[k:k+l+1])\n",
    "                    f[(i, j, k, l)] = get_BLEU4(combined_src_sentence, combined_trans_src_sentence)\n",
    "                    #print (i, j, k, l)\n",
    "                    #print (f[(i, j, k, l)])\n",
    "    return f\n",
    "\n",
    "def get_similarity_matrix_BERTscore_1way(lang1_lines, lang1_trans_lines, config):\n",
    "    lang_len = len(lang1_lines)\n",
    "    lang_trans_len = len(lang1_trans_lines)\n",
    "    f = {}\n",
    "    n = int(config['max_combine'])\n",
    "\n",
    "    top = 0\n",
    "    map_dict = {}\n",
    "    combined_src_sentences = []\n",
    "    combined_trans_src_sentences = []\n",
    "\n",
    "    for i in range(lang_len):\n",
    "        for j in range(n):\n",
    "            if (i+j>=lang_len): break\n",
    "            for k in range(lang_trans_len):\n",
    "                for l in range(n):\n",
    "                    if (k+l>=lang_trans_len): break\n",
    "                    combined_src_sentence = ' '.join(lang1_lines[i:i+j+1])\n",
    "                    combined_trans_src_sentence = ' '.join(lang1_trans_lines[k:k+l+1])\n",
    "                    combined_src_sentences.append(combined_src_sentence)\n",
    "                    combined_trans_src_sentences.append(combined_trans_src_sentence)\n",
    "                    map_dict[(i,j,k,l)]=top\n",
    "                    top+=1\n",
    "    P, R, F1 =  score(combined_src_sentences, combined_trans_src_sentences, lang='en')\n",
    "     \n",
    "    for i in range(lang_len):\n",
    "        for j in range(n):\n",
    "            if (i+j>=lang_len): break\n",
    "            for k in range(lang_trans_len):\n",
    "                for l in range(n):\n",
    "                    if (k+l>=lang_trans_len): break\n",
    "                    f[(i, j, k, l)] = F1[map_dict[(i,j,k,l)]]\n",
    "    return f\n",
    "\n",
    "def align(lang1_lines, lang2_lines, lang1_trans_lines, lang2_trans_lines, config):\n",
    "    lang_len = len(lang1_lines)\n",
    "    lang_trans_len = len(lang1_trans_lines)\n",
    "    #print (f'There are {lang_len} and {lang_trans_len} sentences')\n",
    "\n",
    "    # load embeddings except BLEU\n",
    "    if (config[\"model\"] not in [\"BLEU\", \"BERTscore\"]):\n",
    "        lang1_embeddings = sentences2vectors(lang1_lines, config['lang1'], config)\n",
    "        lang2_embeddings = sentences2vectors(lang2_lines, config['lang2'], config)\n",
    "        lang1_trans_embeddings = sentences2vectors(lang1_trans_lines, config['lang1'], config)\n",
    "        lang2_trans_embeddings = sentences2vectors(lang2_trans_lines, config['lang2'], config)\n",
    "        #print (\"Calculated all embedding\")\n",
    "\n",
    "    # similarity\n",
    "    if (config[\"model\"] == \"BLEU\"):\n",
    "        f_src2tgt = get_similarity_matrix_BLEU_1way(lang1_lines, lang1_trans_lines, config)\n",
    "        f_tgt2src = get_similarity_matrix_BLEU_1way(lang2_lines, lang2_trans_lines, config)\n",
    "        f_matrix = similarity_matrix_merge(f_src2tgt, f_tgt2src, lang_len, lang_trans_len, config)\n",
    "    elif (config[\"model\"] == \"BERTscore\"):\n",
    "        f_matrix = get_similarity_matrix_BERTscore_1way(lang1_lines, lang1_trans_lines, config)\n",
    "    elif (config[\"model\"] == \"word2vec\" or config[\"multilingual\"]==\"0\"):\n",
    "        f_src2tgt = get_similarity_matrix_1way(lang1_embeddings, lang1_trans_embeddings, config)\n",
    "        f_tgt2src = get_similarity_matrix_1way(lang2_embeddings, lang2_trans_embeddings, config)\n",
    "        f_matrix = similarity_matrix_merge(f_src2tgt, f_tgt2src, lang_len, lang_trans_len, config)\n",
    "    elif (config[\"model\"] in [\"laser\", \"sentbert\"]):\n",
    "        f_matrix = get_similarity_matrix_1way(lang1_embeddings, lang2_embeddings, config)\n",
    "\n",
    "    predict_ids = align_matrix(f_matrix, lang_len, lang_trans_len, config)\n",
    "    return predict_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.904762\n",
      "Recall: 0.904762\n",
      "F1: 0.904762\n"
     ]
    }
   ],
   "source": [
    "# step 1: load config\n",
    "config_file = '/mnt/zamia/song/lrec/alignment/src/configs/config.yaml'\n",
    "config = load_config(config_file)\n",
    "config[\"model\"] = \"BERTscore\"\n",
    "\n",
    "lang1_txt = config['lang1_txt']\n",
    "lang2_txt = config['lang2_txt']\n",
    "lang1_trans_txt = config['lang1_trans_txt']\n",
    "lang2_trans_txt = config['lang2_trans_txt']\n",
    "target_ids_txt = config['target_ids_txt']\n",
    "\n",
    "model = config['model']\n",
    "\n",
    "output_file = config['output_file']\n",
    "\n",
    "word2vec_lang1_model = model_dict[config['lang1']]\n",
    "word2vec_lang2_model = model_dict[config['lang2']]\n",
    "laser_model = model_dict['laser']\n",
    "sentbert_model = model_dict['sentbert']\n",
    "print (f'model loaded')\n",
    "\n",
    "# step 2: load data\n",
    "lang1_lines = read_sentences_from_file(lang1_txt)\n",
    "lang2_lines = read_sentences_from_file(lang2_txt)\n",
    "lang1_trans_lines = read_sentences_from_file(lang1_trans_txt)\n",
    "lang2_trans_lines = read_sentences_from_file(lang2_trans_txt)\n",
    "target_ids = get_target_ids(target_ids_txt)\n",
    "\n",
    "config['algorithm'] = 'dp' # [dp, greedy, maxmatch]\n",
    "# step 3: calculate result using the current config\n",
    "predict_ids = align(lang1_lines, lang2_lines, lang1_trans_lines, lang2_trans_lines, config)\n",
    "\n",
    "# step 4: save result\n",
    "#save_result(exact_rate, partial_rate, output_file, config)\n",
    "precision, recall, f1 = res_compare_detail(predict_ids, target_ids)\n",
    "\n",
    "print ('Precision: %f' % precision)\n",
    "print ('Recall: %f' % recall)\n",
    "print ('F1: %f' % f1)\n",
    "#print (predict_ids)\n",
    "#print (target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "76 76\n",
      "file_num=0, model=word2vec, multilingual=0, measure=cosine_similarity, algo=greedy, max_combine=7\n",
      "Precision: 0.684211\n",
      "Recall: 0.684211\n",
      "F1: 0.684211\n",
      "76 76\n",
      "file_num=0, model=word2vec, multilingual=0, measure=distance, algo=greedy, max_combine=7\n",
      "Precision: 0.671053\n",
      "Recall: 0.671053\n",
      "F1: 0.671053\n",
      "76 76\n",
      "file_num=0, model=word2vec, multilingual=0, measure=cosine_similarity, algo=maxmatch, max_combine=7\n",
      "Precision: 0.723684\n",
      "Recall: 0.723684\n",
      "F1: 0.723684\n",
      "76 76\n",
      "file_num=0, model=word2vec, multilingual=0, measure=distance, algo=maxmatch, max_combine=7\n",
      "Precision: 0.723684\n",
      "Recall: 0.723684\n",
      "F1: 0.723684\n",
      "76 76\n",
      "file_num=0, model=word2vec, multilingual=0, measure=cosine_similarity, algo=dp, max_combine=7\n",
      "Precision: 0.960526\n",
      "Recall: 0.960526\n",
      "F1: 0.960526\n",
      "76 76\n",
      "file_num=0, model=word2vec, multilingual=0, measure=distance, algo=dp, max_combine=7\n",
      "Precision: 0.960526\n",
      "Recall: 0.960526\n",
      "F1: 0.960526\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=0, measure=cosine_similarity, algo=greedy, max_combine=7\n",
      "Precision: 0.776316\n",
      "Recall: 0.776316\n",
      "F1: 0.776316\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=0, measure=distance, algo=greedy, max_combine=7\n",
      "Precision: 0.776316\n",
      "Recall: 0.776316\n",
      "F1: 0.776316\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=0, measure=cosine_similarity, algo=maxmatch, max_combine=7\n",
      "Precision: 0.815789\n",
      "Recall: 0.815789\n",
      "F1: 0.815789\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=0, measure=distance, algo=maxmatch, max_combine=7\n",
      "Precision: 0.802632\n",
      "Recall: 0.802632\n",
      "F1: 0.802632\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=0, measure=cosine_similarity, algo=dp, max_combine=7\n",
      "Precision: 0.934211\n",
      "Recall: 0.934211\n",
      "F1: 0.934211\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=0, measure=distance, algo=dp, max_combine=7\n",
      "Precision: 0.934211\n",
      "Recall: 0.934211\n",
      "F1: 0.934211\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=1, measure=cosine_similarity, algo=greedy, max_combine=7\n",
      "Precision: 0.736842\n",
      "Recall: 0.736842\n",
      "F1: 0.736842\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=1, measure=distance, algo=greedy, max_combine=7\n",
      "Precision: 0.736842\n",
      "Recall: 0.736842\n",
      "F1: 0.736842\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=1, measure=cosine_similarity, algo=maxmatch, max_combine=7\n",
      "Precision: 0.763158\n",
      "Recall: 0.763158\n",
      "F1: 0.763158\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=1, measure=distance, algo=maxmatch, max_combine=7\n",
      "Precision: 0.763158\n",
      "Recall: 0.763158\n",
      "F1: 0.763158\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=1, measure=cosine_similarity, algo=dp, max_combine=7\n",
      "Precision: 0.986842\n",
      "Recall: 0.986842\n",
      "F1: 0.986842\n",
      "76 76\n",
      "file_num=0, model=laser, multilingual=1, measure=distance, algo=dp, max_combine=7\n",
      "Precision: 0.921053\n",
      "Recall: 0.921053\n",
      "F1: 0.921053\n",
      "76 76\n",
      "file_num=0, model=sentbert, multilingual=0, measure=cosine_similarity, algo=greedy, max_combine=7\n",
      "Precision: 0.723684\n",
      "Recall: 0.723684\n",
      "F1: 0.723684\n",
      "76 76\n",
      "file_num=0, model=sentbert, multilingual=0, measure=distance, algo=greedy, max_combine=7\n",
      "Precision: 0.723684\n",
      "Recall: 0.723684\n",
      "F1: 0.723684\n",
      "76 76\n",
      "file_num=0, model=sentbert, multilingual=0, measure=cosine_similarity, algo=maxmatch, max_combine=7\n",
      "Precision: 0.723684\n",
      "Recall: 0.723684\n",
      "F1: 0.723684\n",
      "76 76\n",
      "file_num=0, model=sentbert, multilingual=0, measure=distance, algo=maxmatch, max_combine=7\n",
      "Precision: 0.736842\n",
      "Recall: 0.736842\n",
      "F1: 0.736842\n",
      "76 76\n",
      "file_num=0, model=sentbert, multilingual=0, measure=cosine_similarity, algo=dp, max_combine=7\n",
      "Precision: 0.934211\n",
      "Recall: 0.934211\n",
      "F1: 0.934211\n",
      "76 76\n",
      "file_num=0, model=sentbert, multilingual=0, measure=distance, algo=dp, max_combine=7\n",
      "Precision: 0.907895\n",
      "Recall: 0.907895\n",
      "F1: 0.907895\n",
      "76 76\n",
      "file_num=0, model=sentbert, multilingual=1, measure=cosine_similarity, algo=greedy, max_combine=7\n",
      "Precision: 0.065789\n",
      "Recall: 0.065789\n",
      "F1: 0.065789\n",
      "76 76\n",
      "file_num=0, model=sentbert, multilingual=1, measure=distance, algo=greedy, max_combine=7\n",
      "Precision: 0.065789\n",
      "Recall: 0.065789\n",
      "F1: 0.065789\n",
      "76 76\n",
      "file_num=0, model=sentbert, multilingual=1, measure=cosine_similarity, algo=maxmatch, max_combine=7\n",
      "Precision: 0.092105\n",
      "Recall: 0.092105\n",
      "F1: 0.092105\n",
      "76 76\n",
      "file_num=0, model=sentbert, multilingual=1, measure=distance, algo=maxmatch, max_combine=7\n",
      "Precision: 0.092105\n",
      "Recall: 0.092105\n",
      "F1: 0.092105\n",
      "63 76\n",
      "file_num=0, model=sentbert, multilingual=1, measure=cosine_similarity, algo=dp, max_combine=7\n",
      "Precision: 0.365079\n",
      "Recall: 0.302632\n",
      "F1: 0.330935\n",
      "52 76\n",
      "file_num=0, model=sentbert, multilingual=1, measure=distance, algo=dp, max_combine=7\n",
      "Precision: 0.192308\n",
      "Recall: 0.131579\n",
      "F1: 0.156250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/berry/home/song/.ENV/py38/lib/python3.8/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/berry/home/song/.ENV/py38/lib/python3.8/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/mnt/berry/home/song/.ENV/py38/lib/python3.8/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "def gene_yaml(file_num):\n",
    "    yaml_dict = {}\n",
    "    if (file_num==0):\n",
    "        yaml_dict['lang1'] = 'en'\n",
    "        yaml_dict['lang2'] = 'ja'\n",
    "        yaml_dict['data_folder'] = '/mnt/zamia/song/lrec/alignment/data/7045'\n",
    "\n",
    "    if (file_num==1):\n",
    "        yaml_dict['lang1'] = 'en'\n",
    "        yaml_dict['lang2'] = 'ja'\n",
    "        yaml_dict['data_folder'] = '/mnt/zamia/song/lrec/alignment/data/5523'\n",
    "\n",
    "    if (file_num==2):\n",
    "        yaml_dict['lang1'] = 'en'\n",
    "        yaml_dict['lang2'] = 'zh'\n",
    "        yaml_dict['data_folder'] = '/mnt/zamia/song/lrec/alignment/data/762'\n",
    "\n",
    "    if (file_num==3):\n",
    "        yaml_dict['lang1'] = 'en'\n",
    "        yaml_dict['lang2'] = 'zh'\n",
    "        yaml_dict['data_folder'] = '/mnt/zamia/song/lrec/alignment/data/905'\n",
    "\n",
    "    yaml_dict['lang1_model'] = f'/mnt/zamia/song/lrec/model/word2vec/{yaml_dict[\"lang1\"]}/model.txt'\n",
    "    yaml_dict['lang2_model'] = f'/mnt/zamia/song/lrec/model/word2vec/{yaml_dict[\"lang2\"]}/model.txt'\n",
    "    yaml_dict['lang1_txt'] = f'{yaml_dict[\"data_folder\"]}/{yaml_dict[\"lang1\"]}.txt'\n",
    "    yaml_dict['lang2_txt'] = f'{yaml_dict[\"data_folder\"]}/{yaml_dict[\"lang2\"]}.txt'\n",
    "    yaml_dict['lang1_trans_txt'] = f'{yaml_dict[\"data_folder\"]}/{yaml_dict[\"lang1\"]}.trans.txt'\n",
    "    yaml_dict['lang2_trans_txt'] = f'{yaml_dict[\"data_folder\"]}/{yaml_dict[\"lang2\"]}.trans.txt'\n",
    "    yaml_dict['target_ids_txt'] = f'{yaml_dict[\"data_folder\"]}/ids.target.txt'\n",
    "    yaml_dict['model'] = 'sentbert' # [laser, sentbert, word2vec, bleu(not used)]\n",
    "    yaml_dict['multilingual'] = '1' # 0: do not use corsslingual embeddings 1: use\n",
    "    yaml_dict['measure'] = 'distance' # [cosine_similarity, distance]\n",
    "    yaml_dict['algorithm'] = 'greedy' # [dp, greedy, maxmatch]\n",
    "    yaml_dict['max_combine'] = '6'\n",
    "    yaml_dict['threshold'] = '0'\n",
    "    yaml_dict[\"mix_method\"] = 'max'\n",
    "    yaml_dict['output_file'] = f'{yaml_dict[\"data_folder\"]}/ids.predict.txt'\n",
    "    return yaml_dict\n",
    "\n",
    "def grid_search():\n",
    "    models = [\"word2vec\", \"laser\", \"sentbert\"]\n",
    "    multilinguals = [\"0\", \"1\"]\n",
    "    measures = [\"cosine_similarity\", \"distance\"]\n",
    "    algorithms = [\"greedy\", \"maxmatch\", \"dp\"]\n",
    "    max_combines = [\"7\"]\n",
    "    \n",
    "    #models = [\"sentbert\"]\n",
    "    #multilinguals = [\"0\", \"1\"]\n",
    "    #measures = [\"cosine_similarity\", \"distance\"]\n",
    "    #algorithms = [\"greedy\", \"maxmatch\", \"dp\"]\n",
    "    #max_combines = [\"7\"]\n",
    "\n",
    "    # best search\n",
    "    #models = [\"laser\", \"word2vec\"]\n",
    "    #multilinguals = [\"1\"]\n",
    "    #measures = [\"cosine_similarity\", \"distance\"]\n",
    "    #algorithms = [\"dp\"]\n",
    "    #max_combines = [\"7\"]\n",
    "\n",
    "    #models = [\"word2vec\"]\n",
    "    #multilinguals = [\"1\"]\n",
    "    #measures = [\"cosine_similarity\"]\n",
    "    #algorithms = [\"dp\", \"greedy\"]\n",
    "    #max_combines = [\"7\"]\n",
    "\n",
    "    # BLEU\n",
    "    #models = [\"BLEU\"]\n",
    "    #multilinguals = [\"1\"]\n",
    "    #measures = [\"BLEU\"]\n",
    "    #algorithms = [\"greedy\", \"maxmatch\", \"dp\"]\n",
    "    #max_combines = [\"7\"]\n",
    "\n",
    "    models = [\"BERTscore\"]\n",
    "    #models = [\"word2vec\", \"laser\", \"sentbert\", \"BLEU\"]\n",
    "    multilinguals = [\"0\", \"1\"]\n",
    "    measures = [\"cosine_similarity\", \"distance\", \"BLEU\"]\n",
    "    algorithms = [\"greedy\", \"maxmatch\", \"dp\"]\n",
    "    max_combines = [\"7\"]\n",
    "    settings = []\n",
    "    for file_num in range(4):\n",
    "        for model in models:\n",
    "            for multilingual in multilinguals:\n",
    "                for algorithm in algorithms:\n",
    "                    for measure in measures:\n",
    "                        for max_combine in max_combines:\n",
    "                            if (model in [\"word2vec\", \"BLEU\", \"BERTscore\"] and multilingual == \"1\"):\n",
    "                                continue\n",
    "                            if (model in [\"word2vec\", \"laser\", \"sentbert\" ] and measure == \"BLEU\"):\n",
    "                                continue\n",
    "                            if (model in [\"BLEU\", \"BERTscore\"] and measure!=\"BLEU\"):\n",
    "                                continue\n",
    "                            settings.append((file_num, model, multilingual, algorithm, measure, max_combine))\n",
    "    print (len(settings))\n",
    "\n",
    "    for (file_num, model, multilingual, algorithm, measure, max_combine) in settings:\n",
    "        global config\n",
    "        global word2vec_lang1_model, word2vec_lang2_model, laser_model, sentbert_model\n",
    "\n",
    "        config = gene_yaml(file_num)\n",
    "        lang1_txt = config['lang1_txt']\n",
    "        lang2_txt = config['lang2_txt']\n",
    "        lang1_trans_txt = config['lang1_trans_txt']\n",
    "        lang2_trans_txt = config['lang2_trans_txt']\n",
    "        target_ids_txt = config['target_ids_txt']\n",
    "        lang1_lines = read_sentences_from_file(lang1_txt)\n",
    "        lang2_lines = read_sentences_from_file(lang2_txt)\n",
    "        lang1_trans_lines = read_sentences_from_file(lang1_trans_txt)\n",
    "        lang2_trans_lines = read_sentences_from_file(lang2_trans_txt)\n",
    "        target_ids = get_target_ids(target_ids_txt)\n",
    "\n",
    "        word2vec_lang1_model = model_dict[config['lang1']]\n",
    "        word2vec_lang2_model = model_dict[config['lang2']]\n",
    "        laser_model = model_dict['laser']\n",
    "        sentbert_model = model_dict['sentbert']\n",
    "\n",
    "        config['model'] = model\n",
    "        config['multilingual'] = multilingual\n",
    "        config['measure'] = measure\n",
    "        config['algorithm'] = algorithm\n",
    "        config['max_combine'] = max_combine\n",
    "        predict_ids = align(lang1_lines, lang2_lines, lang1_trans_lines, lang2_trans_lines, config)\n",
    "\n",
    "        precision, recall, f1 = res_compare_detail(predict_ids, target_ids)\n",
    "        #print (len(predict_ids), len(target_ids))\n",
    "        print (f'file_num={file_num}, model={model}, multilingual={multilingual}, measure={measure}, algo={algorithm}, max_combine={max_combine}')\n",
    "        print ('Precision: %f' % precision)\n",
    "        print ('Recall: %f' % recall)\n",
    "        print ('F1: %f' % f1)\n",
    "        model_line =f'file_num={file_num}, model={model}, multilingual={multilingual}, measure={measure}, algo={algorithm}, max_combine={max_combine}' \n",
    "        #res_line = f'F1: {f1} Precision: {precision} Recall: {recall}'\n",
    "        res_line = f'{f1:.3f} {precision:.3f} {recall:.3f}'\n",
    "\n",
    "        with open(\"grid_search.txt\", \"a\") as f:\n",
    "            f.write(model_line + '\\n')\n",
    "            f.write(res_line + \"\\n\")\n",
    "\n",
    "grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lang2_lines[0])\n",
    "embeddings = laser_model.embed_sentences(lang2_lines[0], lang='ja')\n",
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to verify that using tokenized sentence or not doesn't affect the result for sentbert model\n",
    "# for English, totally no matter, for Japanese, it does a little bit, so we use tokenized sentence for laser and sentbert model\n",
    "def distance(vector1, vector2):\n",
    "    return np.linalg.norm(vector1-vector2)\n",
    "en_line = lang1_lines[0]\n",
    "en_line_untok = detokenize(lang1_lines[0], detokenizer, lang='en')\n",
    "ja_line = lang2_lines[0]\n",
    "ja_line_untok = detokenize(lang2_lines[0], detokenizer, lang='ja')\n",
    "en_embed = sentbert_model.encode(en_line)\n",
    "en_untok_embed = sentbert_model.encode(en_line_untok) \n",
    "ja_embed = sentbert_model.encode(ja_line)\n",
    "ja_untok_embed = sentbert_model.encode(ja_line_untok) \n",
    "\n",
    "en_cosine_similarity = get_cos_similarity(en_embed, en_untok_embed)\n",
    "ja_cosine_similarity = get_cos_similarity(ja_embed, ja_untok_embed)\n",
    "en_distance = distance(en_embed, en_untok_embed)\n",
    "ja_distance = distance(ja_embed, ja_untok_embed)\n",
    "print (en_cosine_similarity, ja_cosine_similarity, en_distance, ja_distance)\n",
    "\n",
    "en_embed = laser_model.embed_sentences(en_line, lang='en')\n",
    "en_untok_embed = laser_model.embed_sentences(en_line_untok, lang='en')\n",
    "ja_embed = laser_model.embed_sentences(ja_line, lang='ja')\n",
    "ja_untok_embed = laser_model.embed_sentences(ja_line_untok, lang='ja')\n",
    "\n",
    "en_cosine_similarity = get_cos_similarity(en_embed, en_untok_embed)\n",
    "ja_cosine_similarity = get_cos_similarity(ja_embed, ja_untok_embed)\n",
    "en_distance = distance(en_embed, en_untok_embed)\n",
    "ja_distance = distance(ja_embed, ja_untok_embed)\n",
    "print (en_cosine_similarity, ja_cosine_similarity, en_distance, ja_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = lang1_lines[0]\n",
    "e1 = laser_sentence2vector(sentence, 'en', laser_model)\n",
    "e2 = sentbert_sentence2vector(sentence, 'en', sentbert_model)\n",
    "e3 = word2vec_sentence2vector(sentence, 'en', config)\n",
    "print (len(e1), len(e2), len(e3))\n",
    "print (len(e1[0]), len(e2[0]), len(e3[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTscore test\n",
    "import bert_score\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9054, 0.9948, 0.9117])\n"
     ]
    }
   ],
   "source": [
    "sent1 = [\"I am a student.\", \"He likes to play basketball.\", \"Dog is a animal.\"]\n",
    "sent2 = [\"I am a teacher.\", \"She likes to play basketball.\", \"Cat is a animal.\"]\n",
    "# reverse sent2\n",
    "sent2 = sent2[::-1]\n",
    "P,R,F = score(sent1, sent2, lang=\"en\")\n",
    "print (F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9813)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a5b1f497b43f809beb67eb52f8facbcbc5c67b0e5efa672e4e6301455755b13"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('py38': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
