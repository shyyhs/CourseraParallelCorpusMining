{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/song/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import basename, dirname\n",
    "import sys\n",
    "import string\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate import bleu\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from scipy import spatial\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#word2vec_path = \"/share03/song/word2vec/en/model.txt\"\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, unicode_errors='ignore')\n",
    "\n",
    "lang1 = 'zh-CN'\n",
    "lang2 = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec model loaded\n"
     ]
    }
   ],
   "source": [
    "lang1_word2vec_path = \"/mnt/elm/song/lrec/word2vec/{}/model.txt\".format(lang1)\n",
    "lang2_word2vec_path = \"/mnt/elm/song/lrec/word2vec/{}/model.txt\".format(lang2)\n",
    "#lang1_model = gensim.models.KeyedVectors.load_word2vec_format(lang1_word2vec_path, unicode_errors='ignore')\n",
    "#lang2_model = gensim.models.KeyedVectors.load_word2vec_format(lang2_word2vec_path, unicode_errors='ignore')\n",
    "print (\"word2vec model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec_list(sentence, model):\n",
    "    vecs = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            word = word.lower()\n",
    "            vecs.append(model[word])\n",
    "        except:\n",
    "            #print (\"no word in model: \", word)\n",
    "            continue\n",
    "    return vecs\n",
    "\n",
    "def get_vecs_from_lines(sentences, model):\n",
    "    vecs = []\n",
    "    for sentence in sentences:\n",
    "        vec_list = get_vec_list(sentence, model)\n",
    "        vecs.append(vec_list)\n",
    "    return vecs\n",
    "\n",
    "def connect_vec_list(vec_list1, vec_lists2):\n",
    "    return (vec_list1 + vec_lists2)\n",
    "\n",
    "def get_average_vec(vec_list):\n",
    "    avg_vec = np.zeros(100)\n",
    "    #avg_vec = np.zeros(len(vec_list[0]))\n",
    "    for vec in vec_list:\n",
    "        avg_vec+=vec\n",
    "    if (len(vec_list)!=0):\n",
    "        avg_vec/=len(vec_list)\n",
    "        return avg_vec\n",
    "    else:\n",
    "        print (\"zero len\")\n",
    "        return np.zeros(100)\n",
    "    \n",
    "def get_cos_similarity(hy_vec, ref_vec):\n",
    "    return (1 - spatial.distance.cosine(hy_vec, ref_vec))\n",
    "\n",
    "def check_length(len1, len2):\n",
    "    if (float(len1)/len2 < 0.5 or float(len2)/len1 < 0.5):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "f = []\n",
    "dp = []\n",
    "flag = []\n",
    "decide = [] \n",
    "match = []\n",
    "lang1_len = 0\n",
    "lang2_len = 0\n",
    "lang1_vecs = []\n",
    "lang2_vecs = []\n",
    "trans_lang1_vecs =[]\n",
    "trans_lang2_vecs =[]\n",
    "\n",
    "def dp(i ,j):\n",
    "    def get_sim(lang1_vec, trans_lang1_vec, lang2_vec, trans_lang2_vec, bidirection_flag):\n",
    "        direction1_sim = get_cos_similarity(lang1_vec, trans_lang1_vec)\n",
    "        direction2_sim = get_cos_similarity(lang2_vec, trans_lang2_vec)\n",
    "        if (bidirection_flag==0):\n",
    "            sim = direction2_sim\n",
    "        if (bidirection_flag==1):\n",
    "            sim = direction1_sim\n",
    "        if (bidirection_flag==2):\n",
    "            sim = (direction1_sim + direction2_sim)/2\n",
    "        if (bidirection_flag==3):\n",
    "            sim = max(direction1_sim , direction2_sim)\n",
    "        return sim\n",
    "        \n",
    "    if (i>=lang1_len or j>=lang2_len):\n",
    "        return 0 \n",
    "    if (flag[i][j] == 1):\n",
    "        return f[i][j]\n",
    "    flag[i][j] = 1\n",
    "    \n",
    "    \n",
    "    select_i_j2 = -1 \n",
    "    select_i2_j = -1 \n",
    "    select_i_j = -1 \n",
    "    select_i_j0 = -1 \n",
    "    select_i0_j = -1 \n",
    "    # 1-2\n",
    "    # location constrains\n",
    "    if (location_flag == 1):\n",
    "        if (abs((float(i)/lang1_len) - (float(j)/lang2_len))>0.1):\n",
    "            return -1\n",
    "    if (multimatch==True):\n",
    "        if (j+1<lang2_len):\n",
    "            len1 = len(trans_lang2_vecs[i])\n",
    "            len2 = len(lang2_vecs[j])+len(lang2_vecs[j+1])\n",
    "            if ((length_filter_flag == 0) or (check_length(len1, len2))):\n",
    "                lang1_vec = get_average_vec(lang1_vecs[i])\n",
    "                trans_lang1_vec = get_average_vec(connect_vec_list(trans_lang1_vecs[j], trans_lang1_vecs[j+1]))\n",
    "                lang2_vec = get_average_vec(connect_vec_list(lang2_vecs[j], lang2_vecs[j+1]))\n",
    "                trans_lang2_vec =  get_average_vec(trans_lang2_vecs[i])\n",
    "                sim = get_sim(lang1_vec, trans_lang1_vec, lang2_vec, trans_lang2_vec, bidirection_flag)\n",
    "                if (not ((sim_filter_flag == True) and (sim<sim_threshold))):\n",
    "                    select_i_j2 = dp(i+1, j+2) + sim\n",
    "\n",
    "        # 2-1\n",
    "        if (i+1<lang1_len):\n",
    "            len1 = len(trans_lang2_vecs[i])+len(trans_lang2_vecs[i+1])\n",
    "            len2 = len(lang2_vecs[j])\n",
    "            if ((length_filter_flag == 0) or (check_length(len1, len2))):\n",
    "                lang1_vec = get_average_vec(connect_vec_list(lang1_vecs[i], lang1_vecs[i+1]))\n",
    "                trans_lang1_vec = get_average_vec(trans_lang1_vecs[j])\n",
    "                lang2_vec = get_average_vec(lang2_vecs[j])\n",
    "                trans_lang2_vec = get_average_vec(connect_vec_list(trans_lang2_vecs[i], trans_lang2_vecs[i+1]))\n",
    "                sim = get_sim(lang1_vec, trans_lang1_vec, lang2_vec, trans_lang2_vec, bidirection_flag)\n",
    "                if (not ((sim_filter_flag == True) and (sim<sim_threshold))):\n",
    "                    select_i2_j = dp(i+2, j+1) + sim \n",
    "    # 1-1\n",
    "    lang1_vec = get_average_vec(lang1_vecs[i])\n",
    "    trans_lang1_vec = get_average_vec(trans_lang1_vecs[j])\n",
    "    lang2_vec = get_average_vec(lang2_vecs[j])\n",
    "    trans_lang2_vec =  get_average_vec(trans_lang2_vecs[i])\n",
    "    sim = get_sim(lang1_vec, trans_lang1_vec, lang2_vec, trans_lang2_vec, bidirection_flag)\n",
    "    if (not ((sim_filter_flag == True) and (sim<sim_threshold))):\n",
    "        select_i_j = dp(i+1, j+1) + sim\n",
    "    \n",
    "    # zero match en sentence\n",
    "    select_i_j0 = dp(i, j+1)\n",
    "    \n",
    "    # zero match ja sentence\n",
    "    select_i0_j = dp(i+1, j)\n",
    "    \n",
    "    best_score = -1\n",
    "    best_index = -1\n",
    "    #print (i, j)\n",
    "    #print (select_i_j2, select_i2_j, select_i_j, select_i_j0, select_i0_j)\n",
    "    for idx, score in enumerate([select_i_j2, select_i2_j, select_i_j, select_i_j0, select_i0_j], 1):\n",
    "        if (score>best_score):\n",
    "            best_score = score\n",
    "            best_index = idx\n",
    "    f[i][j] = best_score\n",
    "    decide[i][j] = best_index \n",
    "    return f[i][j]\n",
    "    \n",
    "def align(lang1_lines, lang2_lines, trans_lang1_lines, trans_lang2_lines):\n",
    "    # initialize dp array\n",
    "    global f, flag, decide, match, lang1_len, lang2_len \n",
    "    global lang1_vecs, lang2_vecs, trans_lang1_vecs, trans_lang2_vecs\n",
    "    lang1_len = len(lang1_lines)\n",
    "    lang2_len = len(lang2_lines)\n",
    "    f = [[0 for i in range(lang2_len)] for j in range(lang1_len)]\n",
    "    flag = [[0 for i in range(lang2_len)] for j in range(lang1_len)]\n",
    "    decide = [[0 for i in range(lang2_len)] for j in range(lang1_len)]\n",
    "    match = [-1 for j in range(lang1_len)]\n",
    "    \n",
    "    # for English\n",
    "    lang2_tokenized_lines = []\n",
    "    for line in lang2_lines:\n",
    "        new_line = word_tokenize(line.strip())\n",
    "        lang2_tokenized_lines.append(new_line)\n",
    "\n",
    "    trans_lang2_tokenized_lines = []\n",
    "    for line in trans_lang2_lines:\n",
    "        new_line = word_tokenize(line.strip())\n",
    "        trans_lang2_tokenized_lines.append(new_line)\n",
    "\n",
    "    lang2_len = len(lang2_tokenized_lines)\n",
    "    trans_lang2_len = len(trans_lang2_tokenized_lines)\n",
    "    lang1_vecs = get_vecs_from_lines(lang1_lines, lang1_model)\n",
    "    trans_lang1_vecs = get_vecs_from_lines(trans_lang1_lines, lang1_model)\n",
    "    lang2_vecs = get_vecs_from_lines(lang2_tokenized_lines, lang2_model)\n",
    "    trans_lang2_vecs = get_vecs_from_lines(trans_lang2_tokenized_lines, lang2_model)\n",
    "    \n",
    "\n",
    "    dp(0, 0) # use avg_cos_mat\n",
    "    \n",
    "    \n",
    "def get_align_from_decide():\n",
    "    predict = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    global lang1_len, lang2_len\n",
    "    while (i<lang1_len and j<lang2_len):\n",
    "        if (decide[i][j] == 1):\n",
    "            predict.append(([i],[j,j+1]))\n",
    "            i+=1\n",
    "            j+=2\n",
    "        elif (decide[i][j] == 2):\n",
    "            predict.append(([i,i+1],[j]))\n",
    "            i+=2\n",
    "            j+=1\n",
    "        elif (decide[i][j] == 3):\n",
    "            predict.append(([i],[j]))\n",
    "            i+=1\n",
    "            j+=1\n",
    "        elif (decide[i][j] == 4):\n",
    "            j+=1\n",
    "        elif (decide[i][j] == 5):\n",
    "            i+=1\n",
    "    return predict\n",
    "    \n",
    "def get_res_from_decide(lang1_lines, lang2_lines, trans_lang1_lines, trans_lang2_lines):\n",
    "    sentence_pairs = []\n",
    "    similarities = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    global lang1_len, lang2_len\n",
    "    while (i<lang1_len and j<lang2_len):\n",
    "        lang1_sentence = ''\n",
    "        lang2_sentence = ''\n",
    "        trans_lang2_sentence = ''\n",
    "        similarity = -1 \n",
    "        if (decide[i][j] == 1):\n",
    "            lang1_sentence = lang1_lines[i].strip()\n",
    "            lang2_sentence = lang2_lines[j].strip() + ' '+ lang2_lines[j+1].strip()\n",
    "            trans_lang2_sentence =trans_lang2_lines[i].strip()\n",
    "            i+=1\n",
    "            j+=2\n",
    "        elif (decide[i][j] == 2):\n",
    "            lang1_sentence = lang1_lines[i].strip() + ' '+ lang1_lines[i+1].strip()\n",
    "            lang2_sentence = lang2_lines[j].strip()\n",
    "            trans_lang2_sentence =trans_lang2_lines[i].strip()+ ' '+ trans_lang2_lines[i+1].strip()\n",
    "            i+=2\n",
    "            j+=1\n",
    "        elif (decide[i][j] == 3):\n",
    "            lang1_sentence = lang1_lines[i].strip()\n",
    "            lang2_sentence = lang2_lines[j].strip()\n",
    "            trans_lang2_sentence = trans_lang2_lines[i].strip()\n",
    "            i+=1\n",
    "            j+=1\n",
    "        elif (decide[i][j] == 4):\n",
    "            j+=1\n",
    "        elif (decide[i][j] == 5):\n",
    "            i+=1\n",
    "        if (lang1_sentence!=''):\n",
    "            vec1 = get_average_vec(get_vec_list(trans_lang2_sentence, lang2_model))\n",
    "            vec2 = get_average_vec(get_vec_list(lang2_sentence, lang2_model))\n",
    "            similarity = get_cos_similarity(vec1, vec2)\n",
    "            sentence_pairs.append([lang1_sentence, lang2_sentence])\n",
    "            similarities.append(similarity)\n",
    "    return sentence_pairs, similarities\n",
    "\n",
    "def save_results(sentence_pairs, lang1_file, lang2_file):\n",
    "    with open(lang1_file, \"w\") as f1, open(lang2_file, \"w\") as f2:\n",
    "        for i, sentence_pair in enumerate(sentence_pairs):\n",
    "            lang1_sentence, lang2_sentence = sentence_pair\n",
    "            f1.write(lang1_sentence.strip()+'\\n')\n",
    "            f2.write(lang2_sentence.strip()+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0], [0]), ([1], [1]), ([2], [3, 4]), ([3], [5]), ([4], [6]), ([5], [7]), ([6], [8]), ([7], [9]), ([8], [10]), ([9], [11]), ([10], [12]), ([11], [14]), ([12], [15]), ([13], [16]), ([14], [17]), ([15], [18]), ([16], [19]), ([17], [23, 24]), ([18], [29]), ([19], [30]), ([20], [31, 32]), ([21], [33])]\n",
      "22 18 22\n",
      "0.8181818181818182 1.0\n"
     ]
    }
   ],
   "source": [
    "similarity_metrics = 'cs'\n",
    "multimatch = 1 # 0: 1-1, 1: maximum 1-2\n",
    "bidirection_flag = 0 # 0 use english, 1 use chinese, 2 average, 3 max\n",
    "sim_filter_flag = 1\n",
    "length_filter_flag = 1\n",
    "sim_threshold = 0 \n",
    "#sim_threshold = 0.92\n",
    "location_flag = 0\n",
    "\n",
    "def get_target_ids(target_name):\n",
    "    target = []\n",
    "    lines = open(target_name).readlines()\n",
    "    for line in lines:\n",
    "        t1, t2 = line.strip().split()\n",
    "        lang1_id = [int(i)-1 for i in t1.split(',')]\n",
    "        lang2_id = [int(i)-1 for i in t2.split(',')]\n",
    "        target.append((lang1_id,lang2_id))\n",
    "    return target\n",
    "\n",
    "def res_compare(predict, target):\n",
    "    def have_common(list1, list2):\n",
    "        return (len(list(set(list1).intersection(list2)))>0)\n",
    "    tot_num = len(target)\n",
    "    partial_match = 0\n",
    "    exact_match = 0\n",
    "    for p in predict:\n",
    "        exact_flag = 0\n",
    "        partical_flag = 0\n",
    "        for t in target:\n",
    "            if (p[0] == t[0] and p[1]==t[1]):\n",
    "                exact_flag = 1\n",
    "            if have_common(p[0],t[0]) and have_common(p[1], t[1]):\n",
    "                partical_flag = 1\n",
    "        partial_match += partical_flag\n",
    "        exact_match += exact_flag\n",
    "    e_rate = float(exact_match)/tot_num\n",
    "    p_rate = float(partial_match)/tot_num\n",
    "    print (tot_num, exact_match, partial_match)\n",
    "    return e_rate, p_rate\n",
    "\n",
    "def alignment_test():\n",
    "    test_basename = \"762_03_siamese-network\"\n",
    "    lang1_name = \"{}.{}.txt\".format(test_basename, lang1)\n",
    "    lang2_name = \"{}.{}.txt\".format(test_basename, lang2)\n",
    "    trans_lang1_name = \"{}.trans_{}.txt\".format(test_basename, lang1)\n",
    "    trans_lang2_name = \"{}.trans_{}.txt\".format(test_basename, lang2)\n",
    "    target_name = \"905_target_ids.txt\"\n",
    "    target = get_target_ids(target_name)\n",
    "    #print (target)\n",
    "\n",
    "    with open(lang1_name, \"r\") as f:\n",
    "        lang1_lines = f.readlines()\n",
    "    with open(lang2_name, \"r\") as f:\n",
    "        lang2_lines = f.readlines()\n",
    "    with open(trans_lang2_name, \"r\") as f:\n",
    "        trans_lang2_lines = f.readlines()\n",
    "    with open(trans_lang1_name, \"r\") as f:\n",
    "        trans_lang1_lines = f.readlines()\n",
    "        \n",
    "    align(lang1_lines, lang2_lines, trans_lang1_lines, trans_lang2_lines)\n",
    "    predict = get_align_from_decide()\n",
    "    print (predict)\n",
    "    exact_match_rate, partial_match_rate = res_compare(predict, target)\n",
    "    print (exact_match_rate, partial_match_rate)\n",
    "\n",
    "alignment_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ef get_vec(sentence, model):\n",
    "#    res = np.zeros(100)\n",
    "#    num_of_words = len(sentence)\n",
    "#    for word in sentence:\n",
    "#        try:\n",
    "#            res += model[word]\n",
    "#        except:\n",
    "#            num_of_words -= 1\n",
    "#    res/=num_of_words\n",
    "#    return res\n",
    "\n",
    "#def get_vecs_from_lines(lines, model):\n",
    "#    vecs = []\n",
    "#    for line in lines:\n",
    "#        vecs.append(get_vec(line, model))\n",
    "#    return vecs\n",
    "\n",
    "\n",
    "#def check_position_similarity(i, trans_len, j, en_len):\n",
    "#    relative_trans_loc = float(i)/trans_len\n",
    "#    relative_en_loc = float(j)/en_len\n",
    "#    relative_err = abs(relative_trans_loc - relative_en_loc)\n",
    "#    return relative_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
