アルゴリズムはどこにでも存在します。
あなたがソフトウェアを書いたり、ゲノム解析をしたり、 交通渋滞を予想したり、自動映画レコメンデーションを開発したり、または、 単純にネットサーフィンをしているときにも、あなたはアルゴリズムを扱っているのです。
計算機科学のありとあらゆる分科がアルゴリズムを使っているので、 アルゴリズムとデータ構造のコースというのは、どんな計算機科学カリキュラムでも、欠くことのできない重要な要素となっているのです。
なぜなら、ユーザは一瞬で検索結果が見られることを期待しており、 それが、たとえ何兆ものウェブページを検索しなくてはならないものでも同じように期待しているからです。
十分によく考えられていないアルゴリズムは、サーチエンジンによって索引付けされたウェブページやフェイスブックのポストを すべて処理するのに文字通り何世紀もかかってしまうことがあります。
したがって、アルゴリズムの改善というのは、こういったシステムを実用的にするために必要なものです。
これが、テクノロジー企業が面接でたくさんのアルゴリズムに関する質問を する理由なのです。
データサイエンスの問題、たとえばインターネット検索結果のランク付け、 交通事故の予想や、ユーザへのおすすめ映画の仕組みなどは、 高度なアルゴリズムが、非常に質の高い検索や 高精度の予測を達成したり、適切なおすすめをするために使われています。
しかしながら、大容量のデータを処理する単純な機械学習アルゴリズム、たとえば線形回帰でさえも 一般には難題といえます。
たとえばディープニューラルネットワークなどの高度なアルゴリズムを巨大なデータに 適用すると、きわめて高精度な予想をすることができます。
最近では、視覚や発語認識などのいくつかの分野で、人間を凌駕するように なってきていますが、そいったアルゴリズムを大きなデータセットに対して、 数年単位ではなく数時間単位で動かせるようにするのは大変なことです。
あなたの体の中にある数兆個もの細胞は、それぞれが複雑だが よく理解されていないアルゴリズムを実行しています。
そして自分でアルゴリズムを設計し、実装することは重要なことです。
あなたをアルゴリズム設計のプロにするために、このクラスには100近くもの プログラミングの宿題が用意されています。
その中で どのようにして、大きく難しい問題を数秒で解けるような速いアルゴリズムを実装し、テストし、 デバッグすればよいかを学習することが出来ます。
このクラスでお会いできることを楽しみにしています。
実はあなたは今計算機科学の基礎的な並べ替え問題を解決する5つのアルゴリズムを 見ていたのです。
実際、4つのアルゴリズムがほとんど終わろうかというときに、残りのひとつはまだかなりの時間がかかりそうに見えます。
この専門科目では、こういったすべてのアルゴリズムを実装する機会があり、 それによって、次の面接でアルゴリズムやプログラミングの 質問に答えられるような技術を習得することが出来るでしょう。
この講義は基本的にWindowsコンピュータを使っている皆さんのためのものです。
MacやLinuxを使っている人たちはこの講義は必要でありません。
RtoolsはWindowsでRパッケージを構築するのに 必要なツールのコレクションです。
今すぐに行うという訳ではありませんが, 後ほどデータトライアルの開発において,皆さんは行う ことになりますので,そのクラスを修了するならば Rtoolsをインストールできるようにする必要があります。
皆さんは最新のバージョンの 利用可能なRtoolsを見つけ, ".exe"のダウンロードのリンクを選ぶのですが,皆さんのRのバージョンに対応してなくてはなりません。
もし手持ちのRのバージョンが 分からない場合,Rを再起動したときに表示される 最初のテキストの中に Rのバージョンが表示されます。
もし最新バージョンのRをお持ちなら, 最新バージョンのRtoolsをダウンロードしましょう。
もしこのコースのためにRをインストールしたのでしたら, 最新のRtoolsを使えばよいでしょう。
ダウンロードが終了したら, 実行ファイルを開いてインストールを開始してください。
基本的に,皆さんがあまり詳しく知らない場合, 各ステップでデフォルトの設定通りにインストールを進めてください。
そして,皆さんのパスの設定をインストーラーが 編集できるようにボックスにチェックを入れておいてください。
かなり注意深く見る必要がありますが,この下のところにあります。
インストーラーがパスを編集できるようにボックスにチェックを入れておくのを忘れないようにして下さい。
コンソールに「find.packages("devtools")」と入力すると, お持ちがどうかが分かります。
インストールするときは,以前の講義にあったように, 「install.packages("devtools")」を使ってください するとdevtoolsパッケージがインストールされます。
devtoolsのインストールが済んだら,「library(devtools)」でロードでき, 「find_rtools()」とタイプするとTRUEを返してくるはずです。
この講義は基本的にWindowsコンピュータを使っている皆さんのためのものです。
MacやLinuxを使っている人たちはこの講義は必要でありません。
RtoolsはWindowsでRパッケージを構築するのに 必要なツールのコレクションです。
今すぐに行うという訳ではありませんが, 後ほどデータトライアルの開発において,皆さんは行う ことになりますので,そのクラスを修了するならば Rtoolsをインストールできるようにする必要があります。
皆さんは最新のバージョンの 利用可能なRtoolsを見つけ, ".exe"のダウンロードのリンクを選ぶのですが,皆さんのRのバージョンに対応してなくてはなりません。
もし手持ちのRのバージョンが 分からない場合,Rを再起動したときに表示される 最初のテキストの中に Rのバージョンが表示されます。
もし最新バージョンのRをお持ちなら, 最新バージョンのRtoolsをダウンロードしましょう。
もしこのコースのためにRをインストールしたのでしたら, 最新のRtoolsを使えばよいでしょう。
ダウンロードが終了したら, 実行ファイルを開いてインストールを開始してください。
基本的に,皆さんがあまり詳しく知らない場合, 各ステップでデフォルトの設定通りにインストールを進めてください。
そして,皆さんのパスの設定をインストーラーが 編集できるようにボックスにチェックを入れておいてください。
かなり注意深く見る必要がありますが,この下のところにあります。
インストーラーがパスを編集できるようにボックスにチェックを入れておくのを忘れないようにして下さい。
コンソールに「find.packages("devtools")」と入力すると, お持ちがどうかが分かります。
インストールするときは,以前の講義にあったように, 「install.packages("devtools")」を使ってください するとdevtoolsパッケージがインストールされます。
devtoolsのインストールが済んだら,「library(devtools)」でロードでき, 「find_rtools()」とタイプするとTRUEを返してくるはずです。
前回までの講義では、ソーシャルメディア、電子メール、 文書などのデータソースに関するお話を伺いました。
全ては、通常データソースとは考えられていなかったかもしれません。
この講義では、ビッグデータとビッグデータ分析を定義します。
まずは、構造化データ、非構造化データおよび半構造化データの違いについてみていきましょう。
例えば、 ある大手銀行が顧客満足度を向上させることを目指していました。
これまでこの銀行は、二つの主な指標として、CSAT、いわゆる顧客満足度と、 NPS、いわゆるネット・プロモーター・スコアを測定していました。
私たちはこの銀行と共に広範囲な 様々なデータソースを取得し、結び付けることに取り組みました。
まず、調査を活用し、 自由記述欄の自然言語を解析しました。
これによって、顧客の主な懸念分野について十分に理解できました。
次に、顧客サービスコールセンターに掛けられた電話を全て記録しました。
会話をテキスト化し、テキスト分析を用いて会話分析を行うことにより、 電話が掛けられた主な理由と発信者と 受信者の心の状態を明らかにすることができました。
最後に、ソーシャルメディア・チャネルを分析し、 クライアントの顧客が銀行とその競合他社についてどのように述べているかを評価しました。
これによって、顧客と銀行のやりとりと銀行に対する感情の全体像を把握することができ、 クライアントがより適切な意思決定を行うことに役立ちました。
デジタル化やIoTにより、データソースの数が増加し、 データの価値と複雑性が増したことで、入手可能な情報量が急増しました。
現在、私たちは、ビッグデータを、データセットの集まりがあまりに巨大で極めて複雑なため、 基本的なデータベース管理ツールや伝統的なデータ処理アプリケーションを使用して処理することが 困難になっている状況を説明する用語として使用しています。
大きなデータセットは、構造化、半構造化、もしくは非構造化のいずれかの形態の 多数のデータフォーマットから構成されている場合があります。
構造化、非構造化、半構造化データが何を指すかを考えてみましょう。
電話帳に掲載されている名前、住所、電話番号の一覧を思い浮かべてください。
これは、構造化データの一例です。
顧客名、年齢、識別子等の十分に定義されたデータで、 正式に収集することができます。
構造化データの最も有名なプラットフォームとして、 Oracle、Microsoft SQL server、Microsoft Accessなどがあります。
ビッグデータは構造化データソースと関連付けることができますが、常にそうとは限りません。
次に、非構造化データについて考えてみましょう。
非構造化データは、個々の構成要素に分解されません。
YouTubeに投稿された動画や音声の録音の集まりもそうです。
また、何百万もの電子メールや画像、ソーシャルメディアの投稿も該当します。
課題は、どのようにこの非構造化データを入手して、 有用に利用できるのかということです。
一方、半構造化データを理解するため、 非構造化データを表すWord文書に、メタデータを追加したものを考えます。
こうして、半構造化データを取得できます。
半構造化データは、リレーショナルフォーマットやその他の標準フォーマットのような 構造化フォーマットに従っていません。
半構造化データには、データ要素を区別するためのタグやその他のマーカーが含まれます。
ビッグデータは単にデータに関係するだけではなく、 データの相互関連性にも関係しています。
ビッグデータセットは、相互に結び付いており、 それらの結びつきから洞察を得られます。
インターネットの利用可能性、相互関連性、高速な接続速度、移動性が 日々膨大な量のデータポイントの生成に貢献しています。
企業は、顧客データか内部データかに関わらず、 これらの巨大なデータセットの潜在的な価値を実現させようとしており、 ますます廃棄される情報が少なくなっています。
しかし、現在のデータ処理・分析方法では、 巨大なデータを経済的に処理・分析することができません。
2001年に、業界アナリストであるDoug Laney氏は、Gartner社と共に、 現在ビッグデータの主流となっている定義を、4つのV、すなわちVolume(量)、 Velocity(速度)、Variety(多様性)、Veracity(正確さ)と定義しました。
Volumeはデータセットの大きさです。
新たな情報が、日々、場合によっては1時間に一回生成されており、 テラバイトやペタバイト単位のデータセットが作成されています。
多数の要因がデータ量の増加に起因しています。
すなわち、どのように大量のデータの関連性を決定するのか、 関連データから価値を創造するためにどのように分析を使用するか、という問題です。
これは、データが生成・使用される速度を表しています。
新たなデータは毎秒生成されています。
場合によっては、即座に分析する必要があります。
RFID タグ、センサーおよびスマートメーターによって、 準リアルタイムに膨大な量のデータを取扱う必要性が高まっています。
データのVelocityに十分に素早く対応することは、 多くの企業の課題となっています。
これは、データの多様性を表しています。
データセットは、時間と共に変化しています。
ソーシャルネットワーキング、メディア、テキストなどです。
今では、あらゆる種類の形式で生成されます。
数値データや伝統的なデータベース等の構造化データ、 基幹業務アプリケーションから生成されるデータ、 非構造化データとしては、テキスト文書、電子メール、 ビデオ、オーディオ、株式ティッカー、金融取引のデータがあります。
多種多様なデータの管理、統合、統制は、 多数の企業が依然として取り組んでいる課題です。
データのVeracityは、データの偏りやノイズ、異常が関係します。
保存され取り出されたデータは、分析対象の問題にとって有用でしょうか?
データの量や速度と比べると、 データ分析の正確性は、最大の課題です。
データ・アナリティクスの戦略の対象を決定する際に、チームや 提携先からと共に、システム内のデータ蓄積の際に、 データをきれいに保ち、データ汚染の防止するプロセスの策定が必要となります。
データの定義よりもさらに重要なのは、データが何の達成を約束するかです。
効果的に利用すれば、データを洞察や知能に変換することができます。
戦略的な、業務上の意思決定を行い、 実行するために必要な時に必要な所に提供されます。
データ・分析について考えるうえで、もう一つのVを考慮しなければなりません。
それはValue(価値)です。
適正な戦略的洞察を得るために 適切なデータがある場合にのみ、データを入手することによって価値が生まれます。
企業は、データから大きな価値を生み出すことができます。
例を挙げると、あるオンライン小売業者が、 自社のレコメンデーションエンジンを向上させたいと考えていました。
自社のウェブサイトを通じた5つの異なるパスのうち1つを決定するに際して、 現行のソフトウェアは、静的な一連のルールに依拠していました。
そのため、その小売業者は、顧客の個々のプロファイル、ページ毎の閲覧時間、 入力したキーワード、類似した他の顧客が過去に取った行動に基づいて提案できるよう、 このソフトウェアを改修することを望んでいました。
生成されるデータの半分以上が、動画と音声形式のものです。
2020年までに、 全世界のインターネットのトラフィックは、毎月200エクサバイトを超えるでしょう。
エクサバイトは10億ギガバイトです。
全世界のモバイルデータトラフィックは、毎月30エクサバイトに増加し、 年平均成長率は50%増加すると予測されています。
インターネットにアクセスする利用者の合計数は35億人を超えるとされています。
モバイル機器の合計数は100億を超えるとされます。
このような事実について、少しの間考えてみましょう。
これら全てのデータは企業にとって何を意味するのでしょうか?
企業はデータをどのように活用すればいいのでしょうか?
ビッグデータは、ビジネス上の意思決定を行う際のゲームチェンジャーです。
現在、企業がどのようにソーシャルメディアを活用しているか見てみましょう。
伝統的には、企業は、イメージ向上と消費者動向をより適切に予測するために、 ソーシャルメディアの顧客収集力を利用しています。
しかし、マーケティングや広報活動以外に ソーシャルメディアの潜在力を活用している企業はわずかです。
一つの例を見てみましょう。
ある大手投資銀行は、コンプライアンスリスクについて懸念を抱いていました。
規制当局は、顧客から直接寄せられた苦情だけでなく、 当該金融機関のソーシャルメディア・チャネルをモニタリングしています。
そのため、この投資銀行は、顧客がソーシャルメディアや その他の公開討論会で訴えた苦情を定期的にモニタリングする ソーシャルメディア・ダッシュボードを構築しました。
ダッシュボードは、特定のテーマに対するメッセージ件数の変化率、 並びに同一のテーマに対するマインドの変化を捕捉します。
これによって、自行または競合他社に関連したデータ量または マインドが変化した場合に、常に反応や対応を速やかに行えるようになりました。
ビッグデータは、構造化データ、非構造化データおよび半構造化データから成ります。
生成されるデータ量は、驚異的な割合で増加しています。
データの鍵となるのは、全てのデータの相互関連性です。
この研修では多くの情報を取り扱いました。
ビッグデータの概念は、インタラクティブPDFを使用して復習できます。
次の講義では、クライアントの問題解決にビッグデータをどのように活用したかについて PwCのプロフェショナルからお話を伺います。
幸いなことにPwCでは、高度なデータ・分析を活用して、 日々私たちのクライアントに洞察を提供することができます。
データ・分析を通じて、クライアントは実際に課題を認識することができ、 データが構造化かまたは非構造化かに関わらず、データを有用な情報や洞察に変換して、 課題を解決することができます。
世界中のあらゆる高度なデータ・分析も、 最終ユーザーや意思決定者に利用されなければ意味がありません。
本コースの前半でお話ししたデータ・分析のフレームワークについて考えてみてください。
データを収集し分析した後、 そのデータを最終ユーザーに有用な方法で提示できるようにする必要があります。
言い換えると、データを視覚化する必要があります。
まず、データの視覚化の基本原則から始めましょう。
1より大きい数値はスクリーン上の表にいくつありますか。
分かりますでしょうか?
それでは、1より大きい数値がいくつあるか答えてください。
要するに視覚化とは、複雑な洞察を単純化することにあります。
1より大きい数値を単にハイライトするだけで、問題をよりスピーディに解決するのに役立ちます。
簡単に言うと、これがデータの視覚化であり、 図またはグラフでデータを表示することです。
私たちは、ユーザーにグラフを用いてデータを説明し、情報を明確かつ有効に伝達します。
目新しいことは何もありません。
何世紀にもわたって、人々は、情報をより簡単かつスピーディに理解するために、 図や地図といった視覚化された説明を頼りにしてきました。
データをより多く収集・分析するほど、 企業のあらゆる階層の意思決定者は、視覚化された分析結果の閲覧を可能にする 表示データの視覚化ソフトウェアを進んで導入するでしょう。
このようなソフトウェアは、意思決定者が大量の変数間の関連性を検出し、 他者にコンセプトおよび仮定を伝達し将来を予測するのに役立ちます。
脳は視覚化された情報をより速やかに処理することができるため、視覚化は重要です。
ユーザーは、平面構造では明らかでないパターンまたは傾向を見分けることができます。
端的に言えば、データの視覚化は、実施されている分析を誰もが閲覧して 理解できるようにする技法と言えます。
例えば、 顧客の所在地に基づく顧客行動を調べたいとします。
郵便番号別に異なる行動を識別するためには、 データテーブルや顧客セグメントのテーブルを調査する必要があるでしょう。
分析・データの分類および調査には膨大な時間を要するでしょう。
データを地図上にプロットして、特定の顧客行動が生じている特定の店舗 または所在地のデータを正確に示すことができれば、 これを特定の顧客を特定の所在地に呼び込むための戦略の策定に利用することができます。
そして、クライアントとのミーティングに地図を持参し、 クライアントの全店舗と顧客に関するデータを重ね合わせて、顧客に提示することができます。
また、ストーリーを語ることができ、このストーリーを通じて、クライアントは 事業戦略を調整することが可能になります。
今日のデータ・分析は、 様々なビジネス上の課題の解決を支援しているのです。
データの視覚化が可能になると、 情報の意味を誰もが理解できるような分析を提供することが可能になります。
次の講義では、現実のビジネス上の問題を解決するために、 データの視覚化ツールの活用方法に関する例をいくつか見ていきます。
次の課題では、Sense HATを使用します。
そこで、ここではNodeRED内でSense HATをどのように使用するか簡単に見ていきます。
ですので、セットアップ手順に従って、最新の ラズベリー・パイイメージに更新すれば、NodeREDパレットのラズベリー・パイセクションに Sense HAT用のノードが2つあることが分かります。
ノードを選択すると、NodeREDの情報タブにインストラクションがすべて表示されます。
簡単に見てみましょう。
では、パレットにドラッグしてきて、設定を開くと、 様々なセンサ群を有効にするオプションがあることが分かります。
ですので、動きセンサや 環境センサー、ジョイスティックを有効にしたり無効にしたりできます。
環境センサはだいたい1秒に1度 温度、湿度、気圧を送信します。
ジャイロスコープ計と加速度計の情報です。
ジョイスティックイベントは、皆さんが実際にジョイスティックを動かしたときのみ 送信されます。
ですので、必要なセンサを選択してフローを初期化するだけです。
LEDマトリックスへの出力には、Sense HAT出力ノードを利用します。
情報タブを調べると、実際に出力を制御するための ペイロードの整形方法の詳細がすべて書いてあります。
Sense HATをお持ちでない場合、インストール可能な追加ノード Sense HATシミュレータとして動くものがあります。
これは追加設定しないとインストールされていませんので、 コマンドラインに移動し、.node-redディレクトリに移動して npmコマンドを利用してパッケージをインストールする必要があります。
そのノードをインストールして、node-redを再起動すると、 パレットのラズベリー・パイセクションに追加ノードが2つ見つかると思います。
これらが、シミュレータノードです。
ただし、本物のハードウェアと通信しておらず、 ハードウェアをシミュレートしているだけです。
センサーのデータが何であるかを制御できるようにするには、 Webインタフェースもあり、そこで環境センサーを制御できます。
送信したい値を設定できます。
この授業では化学化合物の分析について話します。
タイトルのスライドにある通り、 このトピックと、 異なる元素が含まれるものの分析についての授業内容は似ています。
しかし、似ている点はあるけれども、 非常に大きな違いが 元素の分析と化合物の分析との間にあります。
それではどういった環境下で 化学化合物を含む試料の分析をするのでしょうか?
繊維についての講義をあとでしますが、 ひとつ大事なのは あなたが繊維の試料を手に入れた時にする必要があるのは、繊維が何からできているのか決定することです。
そこで、繊維が何であるのかわかるように化合物の分析をしなければなりません。
さらに、大抵の繊維は着色されているので、 何の化合物がその色素に含まれているのか調べる必要があるかもしれません。
そしてこれは別の とある化合物の分析をしたいと思っている例です。
こういった違法の疑いのある薬物とは、 実際にその違法薬物の物質そのものであったり、 被疑者の体液、 すなわち違法薬物を摂取した疑いのある人物の体液の分析であったりします。
化合物の分析は犯罪科学にとって重要な部分です。
化合物の分析で厄介な、 元素分析では我々が直面しなかったような 事は、分析に使う試料が 実際に複雑な混合物だということです。
例えば、血液や尿といった 試料ならば、大変多くの異なる化合物が含まれており、 そのうち大抵のものが至る所にあります。
例えば尿は我々の新陳代謝の副産物を全て含んでいて、 犯罪科学者は非常に、非常に複雑な混合物を分析することになります、 ほんのいくつか、あるいはたったひとつの検知すべき化合物だけのためにです。
それで、化合物の分析をするときは、 我々が話し合うべき、二つの異なる事柄があります。
一つ目は、どうやってこの混合物を分離して 我々の探す化合物を見つけ出すかです。
これができて初めて、我々は その化合物が何であるか分かり識別できるのです。
試料は、 固定相のある場所に導入され、 ここで紫色のバーで示されています。
次に、移動相が必要となり、 移動相は液体、または 気体であり、固定相の中を通過します。
そして移動相が固定相を通過するにつれて、 我々が分析を試みている混合物も通過します。
この実験をすると、時間が経つにつれて混合物は 個々の構成要素 媒体を通過するときの速さに寄って分離されます。
そして、長く実験をするほど、 分離が進み、当然 これらの構成要素は 他のものより先に固定相の終点に到達します。
今、試料を異なる構成要素に分けまして、 それらが何であるか識別するどんな分析もできる状態になりました。
いろいろなクロマトグラフィーがあります。
もっともシンプルで、簡単で、安いのは 薄層クロマトグラフィーと言われる手法です。
TLCプレートは不活性物質を裏打ちした非常にシンプルな板で、 ガラスが典型的ですが、アルミニウムやプラスチックで出来たものもあります。
TLCのすごいところは必要な器具が非常にシンプルだということです。
これはジャム瓶ではありませんが。
さて、これはガラスが裏板のTLCプレート、そして この側がシリカの固定相です。
さて私が行うのは、 TLCプレート上に試料がどこからスタートしたかという線を引くことです。
そして、試料にこれを選びました。
理由は、すべての内容物に色が付いていて 実験が終わった時に目で見えるからです。
試料を、 キャピラリーに取ります。
それを 線を引いておいたTLCプレートに付けます。
よろしい、移動相がTLCプレートの一番上までほとんど上がったら、 ビンから出します。
溶媒がどこまで上がったか鉛筆で印をつけます。
これが溶媒前線といわれるもので、結果を見ることができます。
あるいは、少なくとも二つの構成要素があって、 それらは色が付いていて、可視光の元に見ることができます。
それで、色のついた化合物ならば、 ただプレートを見るだけで、どこにその斑点があるかわかります。
しかし多くの有機物の化合物は色が付いていません。
もし紫外線の元に斑点が浮かび上がらなかったら、一連の プレートに導入できる化学物質を使い、他の方法で 見えない斑点を見える色の斑点にします。
ここに典型的なTLCプレートがあり、紫外線下で可視化されていて、 まだわからない混合物がC線にあります。
見ての通り、TLCプレートのC線に2つの斑点があり、 おそらく2つの構成要素がここにあるのでしょう。
化合物A、化合物Bという既知のものが TLCプレートに付いていて、 C線にある2つの斑点は化合物AとBに対応していると分かります。
このことで、TLCが化合物の判定に使える手法だと言えるでしょうか?
この質問に対する答えは明解ではなく、 なぜならTLCはいわゆる推定のテストというものだからです。
われわれは構成要素が有機化合物であるとして、この混合物を分析しています。
世には何百万といった有機化合物が作られ、 特徴を述べられたり、レポートされたりしています。
いまやTLCプレートは数センチメートルの長さのものでしかなく、 そのためこんな小さいTLCプレートが分離して その何百万もの化合物を全て区別するなんて絶対に不可能です。
それに、腹立たしいほどよくあることなのですが、2つの違った 化合物がTLCプレートのちょうど同じ場所で斑点になることが実際にあります。
それゆえ、TLCで展開して、 未知の混合物からなる斑点が このTLCプレートで展開した標準物質に対応しているからと言って、 標準物質がこの混合物にも入っているということはできないのです。
あの化合物がその混合物に入っているかもしれないとなったら、 さらに進んだテストをして何の物質なのか判定すべきということにもなり得るからです。
だからTLCは良いのです、シンプルで早く、安い、 ただ必ずしもすべてのケースに決定的な解をもたらすというわけではありません。
よろしい、もっとTLCに接近して見てみましょう。
左にTLCプレートがあり、3つの標準化合物があります、 ピンクの化合物、グレーの化合物、緑の化合物、 そして黒い点で印をつけた未知の混合物です。
このTLCプレートを展開したとすると、おそらく 右側にあるような結果が得られ、溶媒が 上がったところは溶媒の前線としるしづけます。
それで、3つの標準化合物が移動したのがわかり、 未知の混合物は4つの異なる斑点に分離されました。
だから、推定ですが、4つの構成要素がこの未知の混合物にありそうです。
さてこの緑の標準指標は 混合物のうちの一つの斑点と対応しているので、おそらく緑の化合物がそこにあると言えましょう。
黒の標準物質もまた混合物の一つの斑点に対応しているので、 その化合物もこの混合物に入っていると言えるでしょう。
ピンクの標準物質は、見ての通り、 混合物のどの斑点にも対応していません。
ここではっきり言えるのは、 ピンクの化合物は 我々が分析しているこの混合物には入っていないということです。
少なくともこの固有な技法の感度においては、入っていないと言えるということです。
しかし、ここに2つの別の斑点も見えます。
黄色い斑点と赤の斑点です。
それで言えるのは、この未知の混合物には少なくとも2つの 化合物が他にあり、それは何なのかということすらわかりません。
TLCについて話した時、 ”あ、斑点が上にある””斑点が下にある”というだけでは不十分だという話でした。
本当に必要なのは数値的な 手法で斑点の場所を記述できること、 そして我々が使うのはRfという数値、すなわち保持因子です。
この化合物のRf値はXをYで割ったものと定義されます。
TLCというのはとてもシンプルで、とても簡単な技法で、 しかし全く 十分に正確とは言えません。
機械学習とはなにか?
あなたは知らないうちに、日に何十回と使っているでしょう。
Facebookやアップル社の写真認識アプリが、写真からあなたの友達を認識する時も 機械学習が使われています。
電子メールを読むたびに スパムフィルターは大量のスパムから守ってくれます。
これが機械学習なのです。
これは、特別なプログラムをしないで、コンピュータに学ばせるという科学なのです。
私が今携わっている研究プロジェクトの一つに、ロボットが 家の清掃をするというものがあります。
ロボットはあなたがどんな物を取り上げ、かつどこに置くのか観察する事が出来ます。
それにより、あなたがその場にいない時も、同じ動作をしてくれます。
このプロジェクトへの参加を楽しんでいる理由の一つは、 私たち人間が出来る全ての事を、同様に 出来る本当のインテリジェントマシンを作るという AIすなわち人工知能における課題があるからです。
多くの科学者は、人口知能を進歩させる最良の方法は 人間の脳の働きを真似するニュートラルネットワークという学習アルゴリズムを用いる事だと考えています。
このクラスでは機械学習を学び 自身でそれを実装するのです。
ウェブサイトにサインアップして、授業に参加してくれると幸いです。
前回のビデオでは 機械学習の問題に 直面している時は アルゴリズムを改善するたくさんの方法がありうる、という話をした。
このビデオでは エラー(誤差)分析の概念を話していく。
それはこれらの決断を行うにあたって よりシステマティックなやり方を 行う助けとなってくれる物だ。
そして学習の問題に 私がとりかかる時は いつもまる一日、 文字通り長くとも24時間くらいで とても汚くて早く物を得ようとする。
そしてとても手早く汚い、 動く物を実装し、 それをクロスバリデーションのデータで テストする。
そのトレーニングとテストの誤差による 学習曲線をプロットし、 それで、あなたの学習アルゴリズムが 高バイアスに陥っているのか それとも高バリアンスか はたまたそれ以外の何かかを見分けようとする。
そしてそれを用いて、 もっと多くのデータやもっと多くのフィーチャーや そういった事が役に立ちそうかを決定しようと試みる。
これが良いアプローチである理由は しばしば 学習問題を始めた時には 前もって、何が必要なのか、 より複雑なフィーチャーが必要なのか はたまたより多くのデータが必要なのか、 またはそれ以外の何かが必要なのか 知る方法がまったく無いからだ。
そして前もって エビデンス無しで 学習曲線を見ること無く あなたはどこに時間を使うべきかを 見出すのは、信じられないほど 難しいってだけ。
そしてしばしば、 とてもとても早く汚い実装でさえ、それを行う事で 学習曲線をプロット出来て これらの意思決定の助けと出来る。
つまりもしお望みならこれを、 コンピュータプログラミングにおいての いわゆる 未熟な最適化を避ける方法みたいな物と考えても良い。
そしてこれは、 単純に、直感を用いるよりも エビデンスに我らが時間を費やすべき場所を ガイドさせるようにすべきだ、 と言っているだけだ。
学習曲線をプロットする事に加えて、 もう一つ、しばしば行うととても有効な事として、 エラー分析、と呼ばれる物がある。
エラー分析とは、 例えばスパムフィルターなどを 作っている時に、 私はしばしば、 クロスバリデーションセットを見てみて、 そして私のアルゴリズムがエラーになったe-mailを 人力で見ていく、という事をする。
つまり、アルゴリズムが誤って分類した、 スパムと非スパムのe-mailを 調べていって、 そして誤分類された手本に なんらかのシステマティックなパターンを特定出来ないか、見てみるのだ。
そしてしばしば、これを行う事で、 新しいフィーチャーをデザインする事を インスパイアしてくれたり、 現在のシステムの 短所などを教えてくれて そしてそれを改善する為に 必要な事への インスピレーションを与えてくれる。
そしてこの手本に対し、 アルゴリズムはとても高いエラー率となっているとしよう。
クロスバリデーションの手本を 100通も誤分類しているとする。
その時、そこで私が行うのは、 手動でこれらの100個のエラーを精査していき、 手動でそれらをカテゴライズする、という事だ。
それは例えばe-mailの種類が何なのか、などに 基づいて、あるいは アルゴリズムがそれらを正しく分類する為の 助けとなりそうな手がかり、フィーチャーに基づいて。
そしてミスラベルされたe-mailのうち 53通が、いわゆる フィッシングe-mailだったとする、 それは基本的にはあなたにパスワードを提供するように 説得しようとする物だ。
そしてそれらを正しくカテゴライズ出来るような より良いフィーチャーを得られるか見てみる事にも。
そしてまた、私は、 どんな手がかりがあれば、 どんな追加のフィーチャーがあれば アルゴリズムがe-mailを分類する助けとなるかを、見てみても良い。
そしてもう一度、私は手動で 見ていって、 例えば5つのケースのこれを、 そして16のケースのこれを、 そして32のケースのこれを、 そしてその他たくさんのそれ以外の種類のe-mailも、同様に見つけたとする。
そしてもしこれが あなたが実際にクロスバリデーションセットで得られた物だとするなら、 その場合はこれはつまり 意図的なミススペルは 現実には十分にレアな問題だと告げている訳だから、 それはつまり、それを検出するアルゴリズムを 書こうと時間を費やすだけの価値が 実際には無い、という事を教えてくれる。
だが、もしあなたが スパマーがたくさんの 普通で無い句読点等の記号を用いている事を見つけたなら、 それは句読点等の記号に基づいた より洗練したフィーチャーを 開発するのに実際に時間を費やすのは やってみるだけの価値があるという事を示す 強いサインとなりうる。
つまり、この種のエラー分析は、 実際に手動で アルゴリズムの犯した過ちを 精査していくプロセスだが、 これは、しばしば、 辿るともっとも実りの多い道を示す助けと、なってくれる事がある。
そしてこれはまた、私がしょっちゅう アルゴリズムの早くて汚い実装を 推奨する理由も説明している。
我らが本当にしたい事は、 アルゴリズムにとってもっとも分類が困難な手本はどれなのかを 見つけ出す事だ。
そしてだいたいにおいて、 別々のアルゴリズム、別々の学習アルゴリズムであっても、 それらは良く、 類似したカテゴリーの手本を、困難だ、とみなす物だ。
そして早くて汚い実装を 用いる事は、 ある種のエラーを特定する為の 手早い方法を提供してくれる事が多く、 そして何が困難な手本なのかを手早く特定してくれるので、 それに労力を集中出来る。
最後に、学習アルゴリズムを開発する時には、 もう一つ有用な豆知識としては、 あなたの学習アルゴリズムを 数値的に評価する方法を 確認する事だ。
今言った事で私が意味しているのは、 もしあなたが学習アルゴリズムを開発する時には、 あなたの学習アルゴリズムを 評価した時に、単一の実数が帰ってくるような 方法を確保出来たら 信じられない程役に立つ、という事だ。
この特別な概念については、 後のビデオでもっと詳細に議論するが、ここに具体的な例がある。
例えば、我らは discount, discounts, discounter, discountingなどを 同一の単語として扱うべきかを 決断したいとしよう。
それを行う一つの方法としては 例えば単純に、単語の最初の数文字だけを見る、 というのが考えられる。
例えば、仮にあなたが、 単語の最初の数文字だけを見るとすると、 これらの単語は だいたい似たような意味である事が 分かるだろう。
自然言語処理において、 これを行う方法としては、実際には ステムソフトウェア(語幹を取り出すソフトウェア)と言われるソフトウェアを用いる事だ。
そうすれば この種のステミングを行ってくれる かなりリーズナブルなソフトウェアを 教えてくれるだろう、 それを使えば、これら全ての discountやらdiscountsやらを同じ単語とみなしてくれる。
だが、ステムのソフトウェアを使う、というのは、 基本的には単語の 最初の数文字を見る、 というような事をするんであって、それは良くなる事もあるが悪くなる事もある。
何故悪くなる事があるのかというと、 例えばこのソフトウェアは universeとuniversityを 誤って同じ単語と みなしてしまうかもしれない。
何故なら これら2つの単語は、どちらも とても似た文字で、同じアルファベットで始まっているから。
だから、スパム分類器に ステムソフトウェアを使った方が良いかを 決めるのは、 いつも簡単という訳には行かない。
さらに、エラー分析は この種のステムが良いアイデアかを 決定するには、 あまり役に立たない。
その代わりに、 あなたの分類器を助けるのに ステミングソフトウェアが 良いかどうかを判断する一番良い方法は、 とても手早く、実際に試してみて うまく行くかどうかを確認出来る方法を持っておく事だ。
そしてこれを行う為には、 数値的にあなたのアルゴリズムを評価する方法を確立しておく事は とても役に立つ事だ。
具体的には、 これを行うもっとも自然なやり方としては、 ステム有りと無しで、アルゴリズムの クロスバリデーション誤差を見てみる事だ。
つまり、もしあなたが ステム無しでアルゴリズムを走らせて、 5パーセントの誤差だったとして、 つまり5パーセントの分類誤差だったとして、 そしてステム有りでもう一度走らせて 例えば3パーセントの 分類誤差だったとすると、 この誤差の減少は 一瞬であなたに、 ステムを使うのは良さそうなアイデアだ、と教えてくれる訳だ。
この特定の例に関しては とても自然な単一実数による 評価指標が存在している。
我らは後に、 この種の単一実数による評価指標を得るのに もうちょっと作業が必要な例を見る事になる。
だが後のビデオで見るように、 それだけの作業をしてメトリクスを得ておく事は この種の、例えばステムを使うべきかどうか、 というような意思決定を、 より素早く行う事を可能にしてくれる。
例えばこのmomという単語、 大文字のMと 小文字のmがある。
それとも別の単語とみなすべきか?
これは同じフィーチャーをみなすべきだろうか?
だからあなたが学習アルゴリズムを 開発している時には、 しょっちゅう、たくさんの新しいアイデアとか 新しいバージョンの学習アルゴリズムなどを試す事になるだろう。
もし毎回新しいアイデアを 試す都度、 人力でたくさんの手本を精査する羽目になると すぐに、ステムを使うべきか?
だが単一の実数値による 評価指標があれば、 単にそれを見て、エラーが増えたか減ったか、を見れば良い。
そしてそれを用いて、 より素早く、 新しいアイデアを試す事が出来て、 ほとんど即座にあなたの新しいアイデアが 学習アルゴリズムのパフォーマンスを 改善するか悪化させるかを知る事が出来る。
そしてこれがしばしば、 あなたをより早く、前進させてくれる。
さて、エラー分析を行うにあたっては、 テストセットでは無く、クロスバリデーションセットで行う事を 強く推奨する。
だが、世の中には テストセットでこれをやってしまう人もいる、 数学的には確実にそっちの方が 適切で無いやり方なのだが... クロスバリデーションセットで エラー分析を行う、という 推奨されている やり方に比べると。
新しく機械学習の問題に取り組む時には、 ほとんど毎回私が推奨するのは、 あなたの学習アルゴリズムの 早くて汚い実装を行う事だ。
私はほとんど確実に、 早くて汚い実装に、時間を使わなさ過ぎる、という人は見た事が無い。
私が目にするのは、逆に 最初の、早くて汚いはずの実装に 時間をかけすぎる、という方ばかりを 目にする。
そうでは無く、自分の出来るかぎり早く、 何かしらを実装しよう。
そしてひとたび最初の実装を得たなら、 これはきっと、その次にあなたが 何に時間を使うべきかを決定する、 とてもパワフルなツールとなるだろう。
何故なら、まずそれの出力するエラーを見て、 この種のエラー分析を行って 何が誤りを生んでいるのかを見て、 それを使ってさらなる開発をインスパイアしていける。
次に、あなたの早くて汚い実装が、 単一実数の評価指標と 共に用いる事が出来たなら、 これは様々なアイデアを行き来する 乗り物となってくれる。
そして素早く 別々のアイデアが、 アルゴリズムのパフォーマンスを改善するかを 試していく事を可能にしてくれて、 ゆえに、何は要らないか、 何はアルゴリズムと共に使うと良いかを 素早く意思決定していく事を 可能にしてくれる。
PCAのアルゴリズムにおいては、 n次元のフィーチャーを引数に取り それをある数kの次元のフィーチャーの表現へと縮小する。
この数kがPCAアルゴリズムの パラメータとなる。
この数字kはまた、 主成分の数、あるいは保持する主成分の総数 とも呼ばれる物だ。
そしてこのビデオで、 PCAのパラメータkを選ぶのに 人々がどんなやり方で 選ぶ事が多いのかの ガイドラインを与えたい。
kを選ぶ為に、 それは主成分の数を選ぶという事だが、 その為に有用な幾つかのコンセプトをここに示す。
PCAがやろうとする事は、 二乗射影誤差の平均を 最小化しようとする事だ。
そしてこれの意味する所は、 「平均では、我らの手本は 全てがゼロのベクトルから、どれだけ離れているか?
」 我らがkを選ぼうとする時には、 kを選ぶのに良く使われる 経験則としては、 これらの値の比が、 0.01未満となるように選ぶ、という物がある。
言い換えると、 我らがkを選ぶ とても良くやるやり方は、 二乗射影誤差の平均を求める、という事。
それはxと射影との 距離の平均を、 データ全体の分散で割った物。
この全体の分散は、データがどれほど変化するかを表す。
我らはこの比率を、 例えば0.01未満にしたい、とする。
言い換えると1%未満、と考えても良い。
そしてほとんどの人々が kを選ぶという事を考える時には、 kを直接選ぶのではなく、 多くの人の考え方としては、 この値が幾つか、 という事。
そしてもしこれが0.01なら、 PCAの用語を用いてこれを 違う言い方で言うと、99%の分散が保持されている、と言う。
このフレーズ、「99%の分散が保持されている」というのは、単に この左側の量が0.01未満だと言っているに過ぎない。
だから、もしあなたが PCAを使っていて、そして他の人に、 どれだけの数の主成分を 残したのか、について伝えたい時は、 私は、kを 99%の分散が保持されるように選んだ、 と言う方が、より一般的だ。
それの意味する所は、 平均の二乗射影誤差を 全体の分散で割ると、 それがたかだが1%だという事だ。
一方でもしあなたが誰かに 「100個の主成分を得た」とか、 「kはイコール100で、 元の次元は1000次元のデータだった」 とか言っても、 これを聞いた人にとっては 解釈が難しい。
そこでこの数字、0.01というのを人々は良く使う。
つまり、値の範囲として、 90, 95, 99, そして低くても85%くらいまでの中に 含まれる値は、 値としてはかなり典型的な範囲だろう。
95から99の値が 人々が用いる値としては もっとも一般的な範囲だと思う。
多くのデータセットにおいて、 99%の分散を保持するのに、 凄いデータの次元を削減出来て、 しかもほとんどの分散を保持したままに出来る事に しばしば驚く事になろう。
だからデータを 大量に圧縮しつつ、 多くの分散、例えば 99%とか95%の分散を保持する事も 可能となる。
ではこれをどう実装したらいいだろう?
これが一つ、使えそうなアルゴリズムだ。
もしあなたがkの値を 選びたいとすると、 k=1から始める。
そして分散が99%保持されているかどうかをチェックする。
そしてまたこの手続き全体を 繰り返して、そして この式が満たされているかをチェックする、 これは0.01未満だろうか。
そして満たしていなかったら、またこれを繰り返す。
k=3を試そう、 次にk=4を試そう、 そうやって、例えば k=17まで行った所で、 99%のデータが保持されている、という事を 見出したとする。
これはkの 99%の分散を保持する最小の値を探す 方法の一つだ。
幸運な事に、PCAを実装する時には、 この手順の所で、 実際にはこれらの事を 同じように計算する、もっと簡単な方法を可能にする 量を与えてくれる。
ここに描いたこれらの大きなOは、 これの意味する所は、 この行列の 対角成分から外れた成分は 全てゼロとなる、という事だ。
そして証明出来る事として、 ここでその証明をして見るつもりは無いが、 ある所与の値kに対し、 ここにある値は、 もっとシンプルに計算出来る、 という事が、知られている。
そしてその値は、 1引くことの i=1からkまでの 和をとる事のsii、 割ることの 和を取る事のi=1からnまでの sii。
これを言葉で説明すると、 あるいはこれを説明する為に 別の見方で見てみると、 例えばk=3だとすると、 分子を計算する為に 我らがやる事は、 和を取る事のi=1から3までの siiを計算する、 つまり、単にこの最初の三つの要素の和を計算する。
そして分母は、 この対角成分全ての和だ。
そして1から引くことのその比、 それがここの量を 与えるのだ。
そこで我らは、 たんにこれが0.01以上かどうかを テストする事が出来る。
または同じ事だが、 i=1からkまでのsiiの和を 割ることの i=1からnまでのsiiの和 これが0.99以上かどうかを テストしても 構わない。
そこであなたが出来る事としては、 単純にちょっとずつkを増加させていって、 k=1をセットし、k=2をセットし、 k=3をセットし、、、と続けていって、 そしてこの量をテストしていって、 99%の分散を保持する事を保証する中で、 最小となるkの値を見てみる。
もしこれをやれば、 svd関数はたったの一回 呼ぶだけで良い。
そしてこの方法により、 PCAを最初から、何度も何度も 走らせる必要無く、 kの値を選ぶ事が出来る。
単にsvdを一回走らせるだけで、 これらの対角成分の値を与えてくれる、 これらの値全て、s11、s22とsnnまでの値全て。
その場合でも、あなたがやった事を 他の人に説明したいと思ったら、 あなたのPCAの実装のパフォーマンスを 他の人に伝える良い方法としては、 実際にこの量を 計算してみる事だ。
以上で、数字のkを選ぶ 効率的な手続きを 提供出来ただろうか。
次に続く一連のビデオで リコメンダーシステムについて お話したい。
そこには2つほど理由が、、、 私がリコメンダーシステムについて扱いたいと思うのは、2つほど理由がある。
1つ目は単に、それが 機械学習の重要な適用例だからだ。
ここシリコンバレーでは現在 より良いレコメンダーシステムを作ろうとしているグループがたくさんある。
ここで以下のようなwebサイト 例えば アマゾンとかNetflixとか eBayとか Appleの作ったiTunes Geniusとかについて考えてみると 世の中のたくさんのwebサイトや システムが新製品を使うように リコメンド(推薦)しようとしている。
Amazonは新しい本をあなたにリコメンドするし、 Netflixは新しい映画をあなたに 薦めようとする、などなど。
だからレコメンダーシステムの パフォーマンスを改善する事は これらの会社の 基本となるラインに かなりの大きさの直接的な インパクトを与える。
だが何が起きているかを見てみると、 多くのテクノロジーの会社にとって、 これらのシステムを構築する能力は多くの会社にとって優先度が高い。
そしてそれがそれらをこのクラスで扱いたい理由の一つだ。
レコメンダーシステムを扱いたい 2つ目の理由は クラスの終わりまで あと少しのビデオとなってきたので 機械学習において全般的に重要な 考え方を話して あなたと共有しておきたい。
そして、このクラスで既に見て来たように 機械学習において、フィーチャーは 重要だ、 あなたがフィーチャーとして何を選ぶかは 学習アルゴリズムのパフォーマンスに大きな影響を与える。
だから機械学習には いくつかの問題、全てでは無いが、 いくつかの問題については、 良いフィーチャーのセットを自動的に学習してくれることに 挑戦するアルゴリズムが 存在する。
他にもこの例となってる物はあるが、 レコメンダーシステムを通して フィーチャーを学習するというアイデアを ちょっと深く理解出来るだろうし、 少なくとも一つ、実例を 見る事になる、 私が思うに機械学習においても重要なアイデアである所の。
問題はこうだ: あなたはwebsiteなり会社なりで、 映画を売るかレンタルするかするとしよう、 別に映画以外でも構わないが。
つまりユーザーは、 星一つとか、2つとか、3つとか、4つとか、星5つをつけられる。
この例をもう少し 良い物にする為に、 0も許すことにして、星0から 5までとする。
これらのwebサイトの多くは星1から5を使ってるんだけれども。
ここではアリスは本当に Love That Lastsが好きで それを星5個にレーティングし、 Romance Foreverも好きで星5個とレーティングするとする。
キャロルとデイブ、つまりユーザー3と4は アクションムービーが大好きで それらには高いレーティングを与えているが、 ロマンスや恋愛の類の 映画はそんなに 好きではない。
具体的には、レコメンダーシステムの問題では 以下のようなデータが与えられる。
つまりy(i, j)は 0から5までの数字で、 それはユーザーが その映画につけた星の 星0個から星5個に対応する。
さて、レコメンダーシステムというのは、 このデータ、 これらr(i, j)とy(i, j)が 与えられた時に、 これらのデータを見て レーティングが欠けてる映画を探し出して これらのクエスチョンマークの値が いくつとなるかを予測する問題、と 言う事が出来る。
さて、このデータを見ると、 AliceとBobは 二人ともロマンティック映画が好きなようで Aliceはきっとこれに星5つを、 Bobはこれに4.5とかそんな感じの高い値を つけるだろう、と 思われる。
そのユーザーが好みそうな映画を予測しようとする訳だ。
以上がレコメンダーシステムの問題の定式化となる。
次のビデオでは この問題に取り組む学習アルゴリズムを開発する。
このビデオでは、新しい 大規模の機械学習の シチュエーションである、 オンライン学習の場合について話したい。
オンライン学習の状況は、 問題を、以下のように定義出来る物だ: 連続的な流れ、または 連続的なストリームのデータが 流入し続けていて、そしてアルゴリズムに そこから学習させたい、という。
こんにちでは、大きなwebサイトの多くは または、大きな会社のwebサイトの多くで、 各会社は様々なバージョンの オンライン学習アルゴリズムを どんどんやって来たり戻ってきたりする ユーザーの群れから学習するのに 使っている。
具体的には、サイトに次々とやってくる 連続的なユーザーによって生成される 連続的なデータのストリームが あるとすると、 そこにあなたは オンライン学習のアルゴリズムを用いて データのストリームから ユーザーの嗜好を学習して、 あなたのwebサイトの 意思決定を最適化するのに 使う事が出来る場合がある。
さて、我らはユーザーに オファーしたい、と思う提示価格を 最適化する為に使えるような 学習アルゴリズムを求めているとする。
具体的には、ユーザーの特徴を捉えた なんらかのフィーチャーを 考え出したとする。
ユーザーの年齢層などで分かってる事とか、 荷物の送付元と送付先とか、つまり どこに荷物を送り届けたいのか、などを捉えた物。
そして彼らにオファーする 荷物の配達の価格。
そして我らがやりたいのは、 荷物を送る事を 選択する確率を 学習したい、 これらのフィーチャーが与えられた時に 我らの配達サービスを使ってくれる確率を。
だからもし我らが ある価格を所与として、その価格で ユーザーが我らのサービスを使う事に 同意してくれる確率を見積もる事が出来るなら、 その時は我らはユーザーが 高い確率で我らのサービスを使ってくれるような 価格でありながら、同時に 我らの側にも公正なリターンが、 我らの側にも公正な利益が 彼らの荷物を配達する事で得られるような 価格を選ぶ事を試みる事が出来る。
つまりもし特定の価格やその他のフィーチャーを 提示された時の y=1となる条件付き確率を求める事が出来たら、 それを実際に用いて 新しいユーザーが来た時に 適切な価格を提供する事が出来る。
つまりy=1となる確率を モデリングする為に 我らに出来る事といえば、 ロジスティック回帰かニューラルネットワークか、 とにかく何かしらそれ系のアルゴリズムを使う事だ。
どれでも良いが、ロジスティック回帰を使う事から始めてみよう。
今、継続的に運営されている webサイトを所有していたとして、 オンライン学習アルゴリズムがやるのはこんな事だ。
無限の繰り返しを書いて、 これは単に、我らのwebサイトが 起き続けていると 言っているに過ぎない。
webサイトに起こる事といえば、 たまにユーザーが来て そして訪れたユーザーに対して その客なりユーザーなりに 対応したあるペア、 x, yを 得る事になる。
フィーチャーxは このユーザーに指定された 起点と目的地、 それと彼らに今回 オファーした価格、 そしてyは1から0で、 それは彼らが我らの配送サービスを 利用したかどうかを 表す値。
今回やる事は、手本を取り出したら、 その手本を使って こんな感じで学習を行い、 そしてその手本を捨て去ってしまう。
そんな訳だから、 このiでインデックスされる 固定されたトレーニングセットの 記述無しでやっていける。
そしてもしあなたが本当に大規模な webサイトを運営していて、 連続的なやって来るユーザーのストリームが あるなら、 この種のオンライン学習アルゴリズムは 現実にも極めて合理的なアルゴリズムだろう。
そんなにたくさんのデータがあるなら、 データは本質的にはタダだから、 そんなデータは本質的には 無制限だから、 その場合は本当に トレーニング手本を 一回よりも多く見る理由は全く無い。
もちろん、もしちょっとの数のユーザーしか いなかったら、その時は このようなオンライン学習アルゴリズムを 用いるよりも、データの全てを 固定したトレーニングセットに保存して、 そのトレーニングセットに対して なんらかの学習アルゴリズムを走らせる方が賢いだろう。
だがもし本当に連続的なデータのストリームを 持っているなら、その時は オンライン学習アルゴリズムはとても効率的だ。
また、この種のオンライン学習アルゴリズムの 興味深い効果の一つに、 ユーザーの嗜好に適応する事が出来る、 というのがある事は、指摘しておくべきだろう。
特に、時間とともに 経済状況が変わって、 ユーザーはより 価格感応性が高まって、 そして喜んで払っても良いと思う価格が、、、 あー、高い価格を払うのを、より嫌がるようになる。
あるいは、より価格感応度が下がれば、彼らはより高い価格でも、払って良いと思うようになる。
その場合もこの種のオンライン学習のアルゴリズムは ユーザーの嗜好の変化や あなたにお金を払ってくれる ユーザー層の変化に 適用し続ける事を 可能にしてくれる。
それが可能な理由は、 ユーザーのプールが変化した時に、 これらのアルゴリズムは パラメータのシータを、ゆっくりと適用させていく、 あたなのパラメータをどんな物であれ最新のあなたの ユーザーのプールに合わせて。
これはオンライン学習アルゴリズムを適用したいような、 もう一つの例だ。
これは商品検索のアプリケーションで、 ユーザーに 良い検索結果のリストを与えるよう学習するために 機械学習のアルゴリズムを用いたい。
1080pは電話機に 携帯電話に ついていて欲しいビデオカメラの 仕様だ。
そしてwebサイトの レイアウトの方法は、 ユーザーがクエリをタイプした時には、 もしそれが検索のクエリなら、 10台の別々の電話機を ユーザーに提示する為に 選びたい。
我らがやりたい事は、 100台の電話機の中から どの10台の電話機を ユーザーがこんな検索クエリを行ったら 表示すべきかを 見出す助けとなるような学習アルゴリズムが欲しい。
各電話機と特定のユーザーのクエリが 与えられたとして、 フィーチャーベクトルxを構築出来る。
フィーチャーベクトルxは電話機の 様々な性質を捉える事となるだろう。
それは例えば 電話機とユーザーの検索のクエリがどの位近いか、を捕捉した物だろう、 ユーザーの検索のクエリから どれだけの単語が電話機の名前に マッチしたのかを捕捉した物だろう。
つまりフィーチャーxは、 電話の性質を捉えた物で、 そしてまた、ユーザーのクエリに どれだけ似ているか、あるいはどれだけ良くマッチしたかを 様々な次元から見たような物も捉えた物だろう。
何故なら我らはユーザーに 彼らが買いそうな電話機を 見せたいから、 彼らのwebブラウザ内でクリックしてくれそうな 確率が高い電話機を ユーザーに見せたいからだ。
このようなwebサイトを 運営している人達が この問題に与えた名前は、 これを学習する問題は実際には クリックスルー率の予測値、またはCTRの予測値を 学習する問題、と呼ばれている。
そしてもしクリックスルー率の予測値を 各電話機に対して 推計出来たなら、 これを用いてユーザーに もっともクリックしてもらえそうな電話機を 見せる、という事が出来る、 何故なら100台の電話機から 100台の電話機それぞれの これを計算する事が出来て、 ユーザーがもっともクリックしそうな 10台の電話機を選ぶ事が出来る、 そしてこれはユーザーに、どの10台を見せるかを 決定する極めて合理的な方法だと言えよう。
その他の例にもちょっとだけ触れておこう。
一つには、もしwebサイトを運営していて、 ユーザーに提示する スペシャルオファーをどんな物とするかを 決定したいとする。
これはとても電話機と似ているし、 または別々のユーザーに別々のニュース記事を見せるような webサイトを運営していても、 つまりあたなのサービスがニュースアグリゲーターのwebサイトだとしても、 その場合でもあなたは 同様のシステムを用いて ユーザーに何の記事を 見せるのかを 選択するのに、 彼らがもっとも興味をもちそうで、 クリックしそうな物を選ぶ事が出来る。
実際、もし協調的フィルタリングの システムがあるなら、 協調的フィルタリングシステムが ロジスティック回帰の分類器に 食わせる為の、追加のフィーチャーを 提供してくれて、 その分類器を用いてユーザーにリコメンドする 商品のそれぞれのクリックスルー率を予測する事が出来る。
もちろん、これらの問題はどれも 通常の機会学習の問題として 定式化する事も出来る、という事は指摘しておくべきだろう。
例えば2, 3日webサイトを 運営してみて、 そしてトレーニングセットを保存し分けて 固定されたトレーニングセットとし、 それに対して学習アルゴリズムを走らせる事も出来る。
その場合は、 固定したトレーニングセットを保存し分ける必要は無く、 その代わりにオンライン学習アルゴリズムを用いて、 単純にユーザーがwebサイトにおいて生成するデータを 継続的に学習させてしまう事も出来る。
唯一違う所は 固定したトレーニングセットを スキャンしていく代わりに、 ユーザー達から一つの手本だけを取り出して、 その手本から学習し、 その手本を捨てて、次に進む。
そしてもし何らかの応用に際し、 連続的なデータのストリームがある時は、 この種のアルゴリズムはあなたのアプリの為に 検討してみるに値する物だろう。
もちろん、オンライン学習の 利点の一つにはまた、 変わっていくユーザーのプールとか または予測しようとしている事が ゆっくりと変わっていくような 事である場合、例えばユーザーの嗜好が ゆっくり変わっていくとか、そういう場合は、 オンライン学習アルゴリズムは ユーザーの最新の振る舞いが、それがどんな物であれ それに学習した仮説を ゆっくりと適応させていく事が出来る。
機械学習のための多変量解析へようこそ。
この講座では、機械学習の学習に 着手する際に役立つ、計算に関連する さまざまなトピックについて説明します。
この講座は微積分とその応用を 直感的に理解できるよう設計されました。
可能であれば、コンピュータグラフィックスやアニメーション技術で表示します。
なので、うまくいけば高校数学が苦手だった人たちは、 なぜ苦手だったのか不思議に思うようになるでしょう。
まあ、あなたが想像しているように、 両者は関連しています。
この短い講座ですべての内容をカバーする時間がありません。
一番大事なのは行き詰まったら、何をやるべきかを知ることです。
個人的には、仕事で機械学習かなり使っていますが、 研究の焦点ではありません。
私は機械学習の開発者ではありません。
皆さんがこの講座を楽しんでくださればいいと 思っていますが、何よりも機械学習の分野に飛び込み、 数学を通してその意味を理解すれば幸いです。
さて、そろそろこの専門の 一週間目の最後に近づいてきました。
次の二、三週間で何を学ぶかについて手短に説明します。
一つ目の動画で言った通り、 この専門は五つのコースからできています。
この五つのコースでは、深層学習の構築のために 最も大切な基礎をお教えします。
この初めのコースが終わるころには、 ディープニューラルネットワークをどのように構築して動かすかわかるようになるはずです。
これが、初めのコースの中身の詳細です。
このコースには四週間分の教材があります。
深層学習への入門動画を見たとき、 ちょうど一週間目の終わりに近づいているということです。
それぞれの週の終わりには、 教材を復習できるように 10個の選択問題が用意されています。
この動画を見終わったら、 それらの問題を見てみてください。
二週間目には、ニューラルネットワークプログラミングの基礎を学びます。
フォワードプロパゲーションと バックプロパゲーションと呼ばれるものの構造や、 ニューラルネットワークを効率的に実装する方法を学びます。
私自身、アルゴリズムを学んでコードを書きうまくいったら、 とても満足することができます。
なので、あなたにもそれを味わってほしいです。
ニューラルネットワークのフレームワークを学び終わったら、 三週間目に隠れ層が一つのニューラルネットワークをコードにします。
ニューラルネットワークを実装し動かすための 全ての大切なコンセプトを学ぶことができます。
最後に、四週間目には、 ディープニューラルネットワークや多層のニューラルネットワークを構築し、 うまくいくかどうかを確認します。
この後の動画を見終えたら、深層学習で何が起こっているのかについて 深く理解できているはずです。
あなたたちの中には、 深層学習を活用できるアイデアを持っている人がいるかもしれませんね。
復習せずに受けて大丈夫です、一回目から全て正解する必要はありませんから。
私自身にとっても全ての概念を理解するのに便利なものなので、 あなたにとってもそうであることを願っています。
二週目の動画で会えることを楽しみにしています。
ビジネスモデルキャンバスー起業家とイノベーターのツールーへようこそ 本コースの指導者はダン・ストッツです。
ダンはケネソー州立大学の教員であり、30年以上 マネジメント、リーダーシップ、起業家精神のコースで教鞭を執っています。
兄弟2人とともに出版社を経営しており、 ミシガン州アナーバーにある小さなビジネス開発センターを5年間指揮しました。
ダンは、人々がリーダーやイノベータ、チェンジメーカーとして、持てる能力を最大限発揮できるよう 手助けをすることに情熱を持っています。
ですので、座って、プログラムを楽しみ、ビジネスモデルキャンバスがどのように 皆さんの新しいアイディアを利益を生む現実のものに変えていくのか学んでいきましょう。
私はケネソー州立大学の 経営・起業学部の組織メンバーです。
皆さんは、ケネソー州立大学についてあまり知らないかもしれませんね。
私たちは、ジョージア州アトランタのすぐ北に位置する 米国最大の大学の1つであり、33,000人以上の学生がいます。
昨年、USニュース&ワールドレポートが本学をこれからの大学の第4位に格付けし、 つい最近では、米国の最も革新的な大学のトップ20に 格付けされました。
ですので、私たちはCourseraが私たちを選んで、 本講座を設計して届けるようにしてくれたことに喜びと誇りを感じています。
ですので、仕事にかかりましょう。
最初のモジュールでは、 ビジネスモデルキャンバスを構成する9つのブロックについて学びます。
そして、一緒にこのビジネスモデルキャンバスを使うと新しい考えが利益を生む現実のものに 変わっていく助けになるという点でいかに重要な役割を果たすか発見しましょう。
皆さんはもうご存じかもしれませんが、 ビジネスモデルキャンバスは、アレックス・オスターワルダーと イブス・ピグノイアの2人によって発明されました。
彼らの本は、ビジネスモデルを改善したり新しいモデルを作ろうともがいていたりする人にとって、 実用的で、想像力をかき立てるハンドブックです。
アレックスとイブは講演を行っており、 彼らは、45カ国、470人のビジネスモデルキャンバス実践者の助けを得て この本を書いたと言っています。
ですので、ビジネスモデルキャンバスツールは、大学の研究者や イノベータ、起業家達のフィールドテストによって実証されているというわけです。
それでは始めましょう。
まず、短いビデオを見ていきます。
これは、ベスとカールの物語で、 彼らは自分たちのアイディアが大きなビジネスになり得ると考えています。
このビデオは6巻のシリーズの最初のものです。
このビデオはストラテジャー社が製作したもので、 カウフマン財団のご厚意で私たちも利用可能になったものです。
ですので、このまま次のセッションに移動し、ビデオを楽しみましょう。
1袋お菓子を食べた30分後に どうしてお腹がすくのか不思議に思ったことはありますか?
それは食べ物のGI値や選んだ食べ物の組み合わせ と関係しています。
2つのスナックを比べてみましょう。
最初にキャンディーバーを見てみましょう。
代わりのスナックは、炒めたブロッコリーをのせた 玄米のボウルです。
それらは炭水化物で、体が素早く分解し、吸収することができるものです。
つまり、キャンディーバーを食べた後の血糖値の図を見てみると、 このような形になります。
これは食間です。
キャンディーバーを食べてすぐは 血糖値はかなり素早く上がり始めます。
血糖値がこのように上がるとき、 体はそれを感じ、インスリンと呼ばれる それに比例したホルモン量を出し始めます。
その時差により、 血糖値が実際はあるべき値よりも下がっていることがあります。
一方で、ブロッコリーと玄米をスナックとして選ぶと、 体はその食べ物の炭水化物を分解するために 一生懸命働かないといけません。
グラフはこのような感じになります。
ブロッコリーと玄米のすぐ後は キャンディーバーを食べたときよりも、ゆっくりと上がります。
これはインスリン値もキャンディーバーを食べたときのように 急上昇しないということです。
インスリンは血中に入った糖分量に比例して 血中に入った速度で出されます。
おそらく正常な血糖値よりも 下にはならないでしょう。
チキンと玄米、全粒粉パンとチーズ などが良い組み合わせです。
炭水化物と食物繊維、タンパク質、 健康的な脂肪少しを組み合わせれば、 血中に排出される糖分がゆっくりになります。
