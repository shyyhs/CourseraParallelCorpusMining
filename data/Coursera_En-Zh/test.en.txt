What exactly is Google Cloud Platform ?
How is it organized , and what makes it unique ?
Google has a huge expertise in running software at scale .
You 'll be familiar with Google Search , Maps , Gmail , and G Suite , YouTube , Photos .
These and many more applications are part of everyone 's day-to-day life through the Google Chrome browser and Android cell phone .
And now Google has shared its high quality infrastructure with the rest of us with Google Cloud Platform .
You 'll be able to provision your virtual machines in Google 's datacenters , and connect them over Google 's private global fiber network .
Google Cloud Platform is a great place for people who rely on Microsoft Server operating systems and applications .
The concept of cloud computing began with colocation .
Instead of operating your own datacenter , you rented space in the colocation facility .
This was the first wave of outsourcing .
With colocation , the transfer of ownership was minimal .
You still owned the machines and you maintained them .
Traditionally , colocation is not thought of as cloud computing , but to begin the process of transferring IT infrastructure out of your organization .
Today , cloud computing involves virtualized datacenters , virtual machines and APIs .
You automate infrastructure procurement instead of purchasing hardware . With virtualization , you still maintain the infrastructure .
But now the hardware is in a different location .
Virtualization does provide a number of benefits .
The development teams can be faster .
And you can turn capital expenses into operating expenses .
The next wave of cloud computing is a fully automated elastic cloud .
This involves a move from user maintained infrastructure to automated services .
In a fully automated environment , developers do not think about individual machines .
The service automatically provisions and configures the infrastructure used to run your applications .
Google 's uniquely positioned to propel organizations into the next wave of cloud computing .
Google 's first product on Cloud Platform , App Engine , showed in 2008 that anyone could develop and deploy , scalable , highly available web applications without provisioning service .
In Google Cloud Platform , all resources are organized by projects that provide a container for all the products on the platform .
And the project serves as an important identifier for billing of management of users in groups .
Resources within a project are either global , regional , or zonal .
It might help to think of this zone as being similar to a logical datacenter with independent power network .
And so zones should be considered a single failure domain within a region .
In order to deploy fault tolerant applications with high availability , you should deploy your applications across multiple zones in a region to help protect against unexpected failures .
To protect against the loss of an entire region due to natural disaster , you should have a disaster recovery plan .
And know how to bring up your application in the unlikely events that your primary region is lost .
The Cloud Platform services and resources can also be zonal , regional or managed by Google across multiple regions .
If a zone becomes unavailable , all of the zonal resources in that zone are unavailable until service is restored .
An example of a zonal resource is a Google Compute Engine instance that resides within a specific zone .
Regional resources are deployed with redundancy within a region .
An example of a regional resource would be a regional bucket for storing data in Google Cloud Storage .
For example , buckets in the United States location for Google Cloud Storage keep data at REST inside of the United States .
But at REST state can be stored in or move to any cloud storage region within the United States .
Google Cloud Platform includes a wide variety of products , services , and APIs to enable you to build your applications , and these are organized into product families .
The Compute family includes an Infrastructure as a Service product , Compute Engine , a Platform as a Service product , App Engine , and a hybrid product , Container Engine .
Many Google Cloud Platform storage products operate at petabyte scale including Bigtable , and NoSQL key-value datastore .
Cloud Storage for storing blob data , and Cloud Spanner are horizontally scalable SQL database with ACID transactions .
Google 's Big Data products include BigQuery , which enables high performance analytics at petabyte scale , Dataflow to run pipelines for batch and real time transformation .
Finally , Google 's machine learning expertise is available via Cloud ML Engine to train your own machine learning models and host the train models from line or batch prediction together with best of breed APIs for analyzing images , text , audio and video .
From the perspective of managing Windows workloads , each of the Google Cloud Platform products is available via REST API with client libraries .
In addition , you 'll be able to run your applications on Google 's Compute family products .
With Compute Engine offering Windows Server , you 'll have support for any Windows application .
First , the Cloud Console is an easy to use web application where you can select a product using the Products and Services menu , then configure a server using just a few clicks .
Alternatively , there 's a set of tools available via CloudShell , a command prompt accessible from your web browser .
You can also install the Google Cloud SDK locally to run scripts from your own machine as well .
Of particular interest to all of us who are Windows systems operations professionals is a comprehensive support for PowerShell for managing Google Cloud Platform , including cmdlets to manage virtual machines and a file system provider for Google Cloud Storage .
We mentioned previously that on Google Cloud Platform , you 'll always create projects to manage resources .
For Google Cloud Platform enterprise customers , there 's an additional container , the organization , which is created when you signed up with Google as a G Suite customer .
Each developer or systems operations professional will have a login to access Google Cloud Platform resources .
There are number of distinct options for customers with active direction , including having separate G Suites accounts , synchronized user names from Active Directory by Google Cloud Directory Sync , passwords with Google Cloud Password Sync , or single sign-on from Azure to G Suite .
Once your users are all set up , then they log in to access project resources .
There 's a flexible identity and access management system that will enable you to configure role membership for groups of users to enable them to have just the access levels that they need to do their work .
Traditionally , traders on the platforms were of two kinds .
Or customers of traditional banks that were white label clients to Saxo Bank .
Following a long journey , starting as a technology company infrastructure private , Saxo Bank eventually opened up to their trading platform to open up your eye to third party developers .
The idea was that the external development community could be new services on top of the trading platform . But just because you have doors on your house does n't mean that people come knocking on it .
After launching the OpenAPIs , it became clear to Saxo Bank that they needed to work more actively with the development community .
So how can Saxo Bank build this developer community that creates new innovative services on top of the financial trading platform ?
There are also a few additional texts in the readings that follows these videos . In these options , the particular diagnosis questions are , why would any company , startup , traditional financial company , or an IT company , benefit from becoming a developer on Saxo Bank 's trading platform ?
And then what are the major FinTech trends that Saxo Bank should seek to leverage through the developer community ?
The prescription questions is , what should Saxo Bank do to build the developer community ?
If you choose these options for your case analysis , make sure that you weave these questions together into a story about how Saxo Bank transformed and leveraged the new possibilities in the digital financial industry .
Good luck .
My name is Ana Luz Porzocanski and I 'm Director of the Center for Biodiversity and Conservation at the American Museum of Natural History .
This course is an introduction to ecology , the science of ecology , and to Ecosystems Dynamics using a systems thinking lens .
Ecology is the scientific field that studies the relationships between living things in ecosystems in nature and in turn , how they relate to the non-living elements of an environment .
Every week , you will read essays and watch videos about different topics in ecology .
We designed this course with the Next Generation Science Standards in mind .
Throughout the course , you will see connections to disciplinary core ideas , crosscutting concepts and practices .
The study of ecosystems is important because human life depends on these ecosystems and ecology allows us to understand how they 're structured and how they function .
And this in turn informs many other scientific fields including conservation , which is increasingly important given how humans dominate the planet today .
Welcome to the course and I hope you enjoy it .
In the last video , you learned how to execute pilot projects to gain momentum for the in-house AI team and provide broad AI training .
But you want your business , not just gain momentum in the short-term using AI , but in the long term be a very valuable and maybe even defensible business .
To recap , this is the five-step AI transformation playbook , and in this video , we 'll dive more deeply into these final two steps . Step four of the AI transmission playbook is to develop an AI strategy , which , I hope for you may mean to leverage AI to create an advantage specific to your industry sector .
One unusual part of this playbook is that developing the AI strategy is step four not step one .
When I shared this with many CEOs consistent request of please feedback ago was , can you please put the strategy as step one ?
Because I want to figure out what is my company strategy , then I want to find the resources , and then execute on the strategy .
But I 've found that companies that tried to define the strategy as step one , before getting your feet wet , before trying out AI knowing what a feasibility AI project . Companies like that tend to end up with sometimes very academic strategies that are sometimes not true to life .
We read that data is important , he say , " My strategy is to focus on collecting a lot of data , but for your company , that data may or may not be valuable , and may or may not be a good strategy for your company . So , I tend to recommend to companies to start the other steps first , execute the pilot projects .
I think this will work much better for your company than if you tried to formulate an AI strategy , before your company including specifically the executive team has some slightly deeper understanding of what AI can and can not do for your industry sector . In addition , you might consider designing a strategy that is aligned with the virtuous cycle of AI .
Let me illustrate that with an example from web search . One of the reasons that web search is a very defensible business , meaning is very difficult for new entrants to compete with the incumbents with the existing large web search engines , is this : If a company has a better product , maybe a slightly better product , then that web search engine can acquire more users .
Having more users means that you collect more data because you get to observe what different users click on when they search for different terms , and that data can be fed into an AI engine to produce an even better product . So , this means that the company with somewhat better product , ends up with even more users , ends up with even more data , and does an even better product with this link being created by modern AI technology .
It makes it very difficult for a new entrant to break into this self-reinforcing positive feedback loop , called the virtuous cycle of AI . Fortunately though , this virtuous cycle of AI can be used by smaller teams entering new verticals as well .
So , I think today is very difficult to build a new web search engine to compete with Google , or Baidu , or Bing , or Yandex .
But if you are entering a new vertical , a new application area where there is n't a entrenched incumbent , then you might really develop a strategy that lets you be the one to take advantage of this virtuous cycle .
This machine would take pictures of crops and figure out which is a crop and which is a weed , and use precision AI to kill off just the weeds , but not the crop .
So , I knew some of the founders of Blue River while they were Stanford students are taking my class .
So , to get the project started , they actually just use strap in his and sweat , they use their personal cameras and went out to a bunch of farms , and took a lot of pictures of crops in these agricultural fields .
So , they started to collect pictures of heads of cabbage and weeds around the cabbage . Once they had enough data , starts off with a small data set , they could train a basic product .
The first product , frankly was n't that great . It was trained on a small data set , but it worked well enough to start to convince some farmers , some users to start to use their product , to tow this machine behind the tractor , in order to start killing weeds for the farmers .
Once this thing was running around the farms through the process of taking pictures of heads of cabbage and killing off weeds , they naturally acquired more and more data .
Over the next few years , what they did was they were able to enter this positive feedback loop , where having more data allows you to have a better product . Having a better product allows you to convince more farmers to use it .
Having farmers use it allows you to collect more data . Over several years , entering a virtuous cycle , can allow you to collect a huge data asset that then makes your business quite defensible .
In fact , at the time of acquisition , I 'm pretty sure that they had a much bigger data asset of pictures of heads of cabbage lying on a field than even the large tech companies had , and does actually makes the business relatively defensible from even the large tech companies that have another web search data , but do not have nearly as many pictures as this company does of heads of cabbage lying in the agricultural fields . One more piece of advice .
A lot of people think that some of the large tech companies are great at AI , and I think that 's true .
Some of the largest tech companies are very good at AI , but this does n't mean you need to or should try to compete with these large tech companies on AI in general because lot of AI needs to be specialized or verticalized for your industry sector .
So , for most companies to be in your best interest to build AI specialized for your industry , and to do great work in AI for your application areas , rather than try to compete or feel like you need to compete left and right with the large tech companies on AI over the place which just is n't true for most companies .
We are going to live in an AI power world and the right strategy can help your company navigate these changes much more effectively .
You should also consider creating a data strategy .
Leading AI companies are very good at strategic data acquisition .
For example , some of the large consumer facing AI companies will launch services , like a free email service , or a free photo-sharing service , or many other free services that do not monetize , but allows them to collect data in all sorts of ways that lets them learn more about you , so they can serve you more rather than adds , and thereby monetize their data in a way that is quite different than direct monetization about that product .
The way you acquire data is very different depending on your industry vertical , but I have been involved in what feels like these multi-year chess games , where other corporate competitors and I are playing multi-year games to see who can acquire the most strategic data assets .
You might also consider building a unified data warehouse .
If you have 50 different data warehouses under the control of 50 different vice presidents , then is almost impossible for an AI engineer or for a piece of AI software to pull together all of this data in order to connect the dots .
For example , if the data warehouse for manufacturing is in a totally different place than the data warehouse for customer complaints , then how can an AI engineer pull together this data to figure out , whether the things that might happen in manufacturing , that causes you to ship a faulty cell phone , that causes a customer to complain two months later .
So , a lot of leading AI companies have put a lot of upfront effort into pulling the data into a single data warehouse because this increases the odds that a engineer or a piece of software , can connect the dots and spot the patterns between how a elevated temperature in manufacturing today may result in a faulty device that leads to a customer complaint two months in the future , thus letting you go back to improve your manufacturing processes .
There are many examples of this in multiple industries .
You can also use AI to create network effects and platform advantages .
Today , companies like Uber , and Lyfts , and Ola , and DiDi , and Grab seemed like they have relatively defensible businesses because they are platforms that connect drivers with passengers , and is quite difficult for a new entrant to accumulate both a large rider audience and a large passenger audience at the same time .
Social media platforms like Twitter and Facebook are also very defensible because they are very strong network effects where having a lot of people on one platform makes that platform more attractive to other people .
So , it 's very difficult for a new entrant to break in .
If you are working in a business with these types of winner take all dynamics or winner take most dynamics , then if AI can be used to help you we 're growing faster .
For example , with a celebrating user acquisition , then that can pass translates into a much bigger chance that your company will be the one to succeed in this business vertical .
So , it 's hard to give strategy advisers completely general to every single company . But I hope that these principles give you a framework for thinking about what might be some key elements of an AI strategy for your company .
Now , AI can also fit into more traditional strategy frameworks . For example , Michael Porter , many years ago have written about low cost and high value strategies .
If your company has a low-cost strategy , then perhaps AI can be used to reduce costs for your business or , if your company has a high value strategy to deliver really , really valuable products with a higher cost , then you might use AI to focus on increasing the value of your products .
So , AI capabilities can also help argument existing elements of a broader corporate strategy .
Lastly , as you 're building these valuable and defensible businesses , I hope that you also build only businesses that make people better off . AI is a superpower .
This is a very powerful thing that you can do to build a great AI company , and so I hope that whatever you do , you do this only in ways that make humanity better off .
The final step of the AI transmission playbook is to develop internal and external communications .
AI can change a company and its products , and its important to communicate appropriately with the relevant stakeholders about this . For example , this may include investor relations to make sure that your investors can value your company appropriately as an AI company .
For example , AI is entering health care , which is a highly regulated industry because government has a legitimate need to protect patients , and so for AI to affect these highly regulated industries , I think is important for companies to communicate with government , and to work collaboratively with them in public-private partnerships to make sure that AI solutions bring people the benefits it can , while also making sure that governments can protect consumers and protect patients .
So , this would be true for health care or be true for self-driving cars , it would be true for finance and many other AI industry verticals .
If your products change , then consumer or user education will be important .
AI talent is very scarce into this world and so , if you are able to showcase some of your initial successes that could really help with talent and recruiting . Finally , internal communications is also important if you 're making a shift in your company , then many people internally may have worries , some legitimate and some less rational about AI and internal communications , so reassure people where appropriate can only be helpful .
With these five steps , I hope it gives you a vision for how you might be the hope a company become good at AI . If you 're interested in reading the detailed AI transmission playbook , you can also download it from this landing AI website .
I 've seen companies become much more value and much more effective by embracing and become good at AI , and I hope these ideas they hope you take a first step toward helping your company good at AI . Having said that , I have also seen many common pitfalls , the companies run into when trying to implement AI across the enterprise .
Let 's take a look at some of these common pitfalls in the next video so that hopefully , you can avoid them . Lets go on to the next video .
As I mentioned before , I love to share stories about language learners .
For example after I teach students about Francois Gwan I often invite students to think about what strategies he should have used .
Another useful way to help students use successful strategies is by looking at the good language learner studies .
A number of studies have tried to identify what good language learners do .
Good language learners predict . Good language learners will often guess or be invited to guess what the topic will be about .
Students who are good at picking up on social and contextual clues often become better language learners .
Use selective attention .
Good language learners often pay attention to only a particular aspect of a lesson .
They will decide , in advance , to ignore distracting information and pay attention only to what they believe is important .
Prepare , learners who prepare in advance often do better .
For example , a learner who writes down notes about what he or she wants to say , generally will excel over a student who simply tries to speak with no preparation .
Look ridiculous , learners who are willing to look foolish often make better language students , they do n't worry about what they look like .
Practice , learners who practice are better able to master the skill of language .
Practice should be at the core of any real language class .
Monitor , monitoring refers to correction of your own speech for accuracy and pronunciation , grammar , vocabulary , and appropriateness .
A learner who pays attention to both form and meaning is often the best language learner . Ask questions , learners who learn to ask questions to those around them and get answers to those questions are more likely to be successful .
Good language learners are able to find answers form dictionaries , teachers , students and other sources .
Good language learners can relate information such as new vocabulary to a physical action .
And this helps deepen memory .
Play , good language learners practice reordering a meaningful sentence by using words in a new way .
They play with language .
Now , did you notice that as we went through these strategies , that some of the good language learner strategies are similar to good teacher strategies .
Even if your learners never actively learn any of these strategies you can still encourage the use of them within your classroom .
Help students predict , ask questions use imagery and play .
By the way , this leads us to our next metaphor about teachers .
We hope you 'll agree that learners need to know not just what to learn , but how to learn .
And as a language teacher , you play a serious role in helping give them correct strategies .
In a sense , you are guiding them along a correct path by encouraging them to go in certain directions .
So the metaphor we want you to remember is that , as a teacher , you are often a trail guide .
Helping your learners along a path with obstacles and choices .
You help learners to avoid certain pitfalls that Francois fell into , and discover strategies that they should use instead .
Thanks for watching Teach English Now .
As we stated before , your job is to engage students in English language outside of the classroom .
A mother bird , after a while , recognizes that it is time for her birds , whom she loves , to fly on their own . She has nourished them .
She has provided them shelter and that is why , she 's kicking them out .
I love you , and boop , I 'm giving you the boot . And that is what we teachers must constantly think about .
It should be a foundational principle on which you build your teaching style .
You must always recognize that your learners will leave you soon , and that the more skills and strategies you give them to help them fly on their own , the better .
One of your primary goals as a teacher is to help learners gain autonomy .
Autonomy , as you know , refers to the ability for someone to act without guidance .
In language learning , it refers to the ability to move about in different contexts such as visiting a country , or writing a letter to a friend . It also refers to a learner 's ability to find resources and information quickly , such as knowing where to look for answers on the Internet , and just being able to communicate freely .
Ultimately , you wish for learners to be empowered to go places , share ideas , and enjoy what it is like to be part of our global community . You want them to visit Australia and see the koalas , go to London and eat some fish and chips , and head on over to the United States and meet a Hollywood celebrity .
Language is a passport that students can use in order to move freely about the world , and English is one of the most powerful of all languages in terms of the ability to interact with multiple cultures all over the world .
Well , let 's flip that script today and say that students are the actors , acting for themselves because they are empowered , autonomous learners of a language and they can express themselves , understand others , and participate in this global society . Thanks for watching .
A person that , to be honest , we really should talk about more .
You . Before we talk about you , let 's have a small discussion about the teaching profession in general .
Over the last 20 years or so , I keep seeing a very disturbing problem .
Good teachers , great teachers , keep leaving our profession .
Teachers in general are tired .
But teachers who understand the importance of acquisition in language learning and the need for students to be able to communicate in English must be engaging , create stimulating activities , and constantly meet the changing demands of unique learners .
You become an actor , a coach , a psychologist , a guide .
I 'm tired just thinking about it .
Teacher burnout means that a teachers like a candle , try to give and give until they themselves have nothing left . And over the last 20 years I have seen a lot of teachers last anywhere from three to five to ten years , and the quit .
Construction workers .
Well of course in education there are always concerns about salary , lack of support and so forth .
But I would suggest that one of the biggest problems comes from the profession itself .
As I focus on helping students communicate in English , I have learned that I have to be a pronunciation expert , a reading expert , a writing expert , a grammar expert , a speaking expert and so on .
In the last few years , I have been asked to teach TOEFL , TOEIC , ILS , GRE and GMAT .
I 've taught English to aviation pilots , nurses , elementary school students , junior high students , high school students , college students , and business professionals .
How in the world can I teach to all of these different groups and be an expert on all of these different things ?
And each year , it seems , there are new books , new standards , new curriculum and new ways to do the same thing .
So how do we step off the merry-go-round and avoid teacher burnout ?
In the next few videos , we 're going to talk about how we can make sure that as you strive to help your learners succeed , you still matter .
We will discuss what you can do to avoid teacher burn out and what you can do to advance in this most wonderful of careers .
Have you noticed I enjoy metaphors ?
Because one of our first metaphors about teachers , all the way back in module one , was that teachers should be actors .
And one of the biggest problems I have with this metaphor is that there are ways that teachers should definitely not be compared to actors .
Sure there is some truth to the metaphor .
Teachers do have to use their hands , their body , their face .
Sure , teachers do have to think about how to impact their students , just like an actor has to impact an audience . And sure , teachers do have to pretend they feel great , after a long night of grading homework .
But the truth is , if you are trying to perform for your students , pretend you feel something that you do n't , this is truly a fast track to burning out . Some teachers are quiet and composed , some are loud and gregarious .
Whoever you might be , be the best of that type .
So our last word of advice for you to avoid teacher burnout , is to find your core .
Find out why you do the things you do .
As I explained earlier , people will join you when they see that there is a cause you believe in .
Your students will join you , other teachers will join , administrators will agree with you , when you find a core value , a core belief that you know is true .
What do you do to avoid teacher burnout ?
Never act , be sincere .
Again , do n't confuse this with a lazy attitude to get out of learning new techniques .
Of course , you should challenge yourself , and get out of your comfort zone , and learn new things .
Finding your core means knowing which metaphors resonate with you , which messages hit hardest with you .
It means to know your own teaching philosophy , and then do all you can to be the best within your sphere of knowledge and understanding .
I sincerely hope that you find strength in the metaphors and information we have provided .
Welcome to the course . My name is Sharad Borle , and I 'm a faculty of Management at the Jones Graduate School of Business at Rice University in Houston , Texas .
The course is a part of the business statistics and analysis specialization .
And the specialization through these various courses aims to develop a robust and practical understanding of various aspects of business statistics , using the popular software Excel .
The spreadsheet software Excel is an industry standard .
And getting to apply all these skills , the business statistics skills , using the software would go a long way in its applicability to the industry .
It is a great source of pride and joy when students come back and tell me how they were able to apply these courses at their work place .
It also helps in creating these courses and making them relevent to today 's business . The course that you are taking will give you ample opportunities in applying these various statistical tools to a variety of business applications .
My own academic background is a very quantitative one , an undergraduate in engineering , an MBA in quantitative techniques , and finally a PhD from the Carnegie Mellon University in Pittsburgh .
My own research deals with application of various statistical tools and quantitative methods to various issues and problems facing the industry .
Some are super popular for a reason , they 're just so useful and powerful .
Learning to confidently use these functions will be critical to your success as an Excel user , so do n't skip this week 's quizzes and challenges .
Creating great looking dashboards in Excel that summarize and present your data is definitely another fundamental skill to learn .
This week you have learned some of the key building blocks , such as reporting functions , sparklines , and advanced charting to design amazing dashboards in the future . So get stuck into this week 's quizzes , challenges , and of course the toolbox to make sure you consolidate these important skills and techniques .
Hello , welcome to first class of Financial Accounting Foundations here at the University of Illinois .
First , we a going to define what accounting is , and then we will go through what financial accounting is . And finally , we will discuss in detail three main financial statements .
I 'm from Los Angeles originally , and I 've lived across many different cities in the United States , but I 'm very much enjoying my time here at Yale in New Haven , Connecticut .
I 've had the pleasure of working with Professor Shiller on his Coursera course for all of our learners around the world . I 'm thrilled to be your Coursera teaching assistant for this course .
These videos provide further explanation and reflection from the lectures . So for example , Professor Shiller talks about the idea of regressions in his lecture videos .
So in our chalk talks , we 'll delve a little bit deeper into what regressions are and try to give some examples .
The second way you 'll see me throughout this course is what we like to call a TA timeout . So you 'll be watching a lecture video and suddenly I 'll pop up onto the screen .
And so I 'm doing this provide an explanation or a definition of something I think that you should know , and afterwards I 'll disappear and the lecture video will just continue .
I hope that you find both the chalk talks and TA timeouts helpful as you navigate financial markets .
We sincerely hope that you enjoy this course .
Give me a bigger screen . " So , a difference in culture something say a difference in age So , we see a much more willingness to adopt FinTech amongst Gen Z At a glance of Gen Y the later age groups are much more willing to change and do new things adopt new apps , but sometimes this age group does n't work the same across different cultures Sometimes , culture difference can be larger than age differences So , you may see that within China , for example there 's a surprisingly high adoption rate for new payment systems amongst older people , who are saying " Sure , I 'll pay with my smartphone .
No problem .
It 's so much better than cash , " and you would go to the US and say " If I look across American society or European society and say , who 's adopting cashless society ?
Who 's jumping on to Venmo ?
Who 's using these apps for payment with their phone ? " It tends to be Gen Z maybe Gen X , not Mature Older people are saying " It took me a while to get used to a credit card , I 'm okay with that , but do n't ask me to jump on to using PayPal for everything and certainly do n't ask me to move to Venmo , instead of my bank , that 's too much , too big , too far , not going to go there . " But in China , " Sure , Alipay .
We do n't worry about sharing data or oversharing of data .
Thank you
Welcome back to our course on FinTech Risk Management We 're going to talk today about our third area of FinTech Risk Management IT and Compliance , and Assurance and we have two weeks on this Weeks three and four of our video we 're going to talk about IT because FinTech is at least half tech if not more than half tech Finance matters , and we 've talked about finance compliance Corporate governance matters , and that 's part of risk management But IT can really screw you up and it 's an important area or aspect of FinTech Percy is going to take the lead in this discussion >> >> So , why do n't you give us an overview of where we 're going to go in this the third week of our course >> Okay , thanks , Ted .
So , the first focus is going to be business application development So , no matter whatever the software whatever the system that you 're developing specifically we 're talking about FinTech So , we could focus on the - we discussed before about the controls Then if you want to have controls that we have to reduce the risk , and control so those controls has to be embedded to the system that by the time that you develop the system So , that 's very important to make sure that by the time that we develop any FinTech system the controls are in place at the time that we developed it >> Now , a finance person when you hear the word compliance and assurance you think about regulation When you 're an IT or technology person you hear compliance and assurance , why is that different ?
>> So , the basically , the main difference what my focus in this module is going to be like say for example , that we are going to given access to different people of different access right to your system in different information , different data So , we should have different access levels So , we should have different data privacy , data protection the authentication mechanisms something like mobile metrics >> So , we 're worried about risk we 're worried about control ?
So by the time when we develop a FinTech system we should make sure that all these controls be embedded by the time we developed system >> But one of the biggest problems in software , and software development software implementation , software risk analysis is doing the wrong thing >> That 's true .
So , that 's something we could >> We will also have to think about processes for change >> Exactly , so that 's very important here Then we will look at the change management process So , when we do the process to develop process make sure that nobody will be able to make unauthorized changes to the system So , it 's very important that only the authorized changes may be able to happen to your system So , this keyword is approval and authorization >> That sounds expensive >> Yes usually , but we should have somebody to look into that >> Because if we do n't have processes and control it could get more expensive >> Exactly , and then there are a lot surprises comes to you and then probably it 's going to be too much for you Then think about change process so it takes longer time to make one change So , when you go for normal change we call it as the authorization documentation , and finally implementation >> We have a process >> Yes .
Say , for example you have your e-banking system and it 's dulled in the middle of the night >> That 's bad >> That 's bad .
Hi , guys , welcome back Can you guys remember ?
The last video we asked question from you guys If any case , we give the system development to small organization but very talented people and after some time this company bankrupt You guys pay $ 10 million for the new system development Remember , as soon as that we do the outsourcing you guys are going to lose the control So , every maintenance has to be done by them because they do n't give you guys source code simply what they give you guys is the system itself But in this case , if any case that a small organization go bankrupt you you do n't have source code you ca n't do any maintenance the entire $ 10 million could be waste So we have a solution for that The solution for this kind of scenario is going to be as you have to sign a special agreement with them we call it Software Escrow Agreement What is Software Escrow Agreement is ?
Say for an example , company A is developing a system Sorry , the outsource developer is developing system for your company which is company A you guys pay $ 10 million So , once you pay $ 10 million they develop system for you guys But at the same time , they do n't give you guys source code Then both parties , developer and you will have a third party which is maybe a Big Four companies You guys , as company A as well as developer both trust that third party and the developer is going to keep the source code in the third party But you will not have any access to that but third party is going to keep the source code with them If any case that the developer is not been able to continue the maintenance to you continue helping to do the maintenance to your organization then according to the agreement the third party is going to release the source code to your company so that you guys can continue the maintenance by yourself So , that is what we call Software Escrow Agreement So that 's something to be very careful you need to consider in a system development But if any case , if you use a development like Microsoft I do n't think that you have any doubt that Microsoft is going to be bankrupt So , that kind of scenarios we do n't have to bother about any source code Escrow Agreement as well So that 's something important here So , now , we look at how to choose the company So , we can not make the decision simply based on the cost but we had considered the reliability as well At the same time we have to consider of Escrow Agreement Is that all ?
Not really But at the same time once the system is in place once we have the system you need to do the maintenance First of all , when system is given to you guys you have to implement the system That means system has to be installed to your company computers So whether that developer is going to help you to do the installation , that 's something important The same time , system is brand new to all the users so we had to prepare the user manuals where the developer is going to help you guys to develop the manual That 's again very important here At the same time , nobody knows how to use the system So , in this case , we are to consider that training to be given to the users So whether the developer is willing to give the training to the users for your users , that 's again very important here So , what you should do is the developer is train some of your employees we call trained trainees so that those employees would be training the others So , this is very important here The decision we make which company to develop a new system for us ?
So , we have to have an entire considerations looking at the entire consideration So , we make the final judgment that 's very important here Now , next phase is going to be the system development So once we design the system then we are looking at either 4A or 4B The 4A means develop the system by ourselves in last development , remember ?
4B means configure the system by the outsourced to work So , now , we look at the in-house development and 4A development phase What is the most important thing we should do in the development phase ?
The most important thing we should do in the development phase is the testing because next phase is going to be implementation So before we implement we should try to check whether there are any errors any bugs within your system If we find any thing , bugs , any problems any issues , we 're trying to solve it before we use a system .
That 's very important here So for this system testing , we are to come up with a plan We 're not simply just input whatever the things to check the output of the system So , we have to come up with a plan this plan , and then we are to do the testing So , there are different , we have test plan testing , and final acceptance test Now , we look at one by one So , what is the unit testing means and the integrated testing means ? So , a system may have many modules and that are linked together So , unit testing means just test one module Look at the input , and then give input and then observe the output and see if the system performs correctly Then , we look at the integration testing Integration testing means two systems has interaction Interaction means basically output of module one could be input to module two so that is called interface between module one and module two So , what we do is we look at the interface between two modules that 's what we call integrated testing Finally , we do have the entire system testing , whole testing So , what is entire system testing ?
What we exactly do when you do the entire system testing ?
Think about it Entire system testing , what we do is we do different kind of ones First one , we call recovery testing That means you purposely crash the system and see whether how long how fast , how easy that you can recover the system because remember system can crash anytime So , that would happen it 's a matter of time So that we should make sure that we can recover the system fast Secure testing means how good a security features within your own system ?
The most important testing is final accepting testing So , all done , test planning unit test , integration testing and system testing is done by IT people But final accepting test has to be done by the users Remember , users are the most important people for your system so they are the one who 's going to use the system and if they do n't like it your system is going to fail very soon So , final accept test has to be done by the users to make sure that users are happy with your system so that system can be used for a long time to come The next is going to be the system implementation So , when we implement a system we have to have a lot of consideration to be make So , in the next video we are going to look at issues that are related to the system implementation So , I look forward to see you guys again very soon
Hi guys . Welcome back again So , today , we will be continuing looking at the system development life cycle and then see the complaints and assurance issues for system development life cycle So it 's time for us to looking at the Phase 5 which is implementation Implementation means that we only develop the new system So it 's time for us to retire the old system and come up with a new system That 's what we do in the implementation phase , right ?
So implementation , we call it changeover Changeover means basically that we have the old system and then we replace with the new system Sometimes we call it cutover techniques as well So what is the main consideration ?
So when we think about , okay retiring the old system and bringing the new system So what are the main considerations to take ?
The main considerations are basically two folds So we look at the cost as well as we look at the risks That those other things because cost is something that of course senior management concern So they would like to reduce the cost as much as possible at the same time of course , new system the real testing of the new system is the one that you 're really using it So since that you have done some testing before but we are not sure whether still have bugs wether it 's still working the way it 's supposed to be because we can come up with a lot of examples in the world that sometimes new systems at the very early stage of new systems gives a lot of troubles So we can see the many many examples that I can share with you guys probably later on So then the two dimensions we look at here the cost and the risk At the same time , we look at is there any limitations of different system change on methods ?
Now , let 's look at what are the main techniques we have in terms of system development So there are three main techniques We got parallel changeover phased changeover , and abrupt changeover The next few minutes we 'll be looking at this three changeover techniques and then at same time we look at their costs and risks as well as is there any limitation of these three or special consideration rather in these three different technique There 's a special case of parallel operation So parallel changeover means we run both old and new systems in every site But special case in pilot operation what we do is we take selected sites and we run the new system only selected sites but all system can run in all the sites So we do it , we try both old and new systems in pilot sites for some time and then we stop the old system and we 'll run the new system in every site So that 's what we call pilot operation The cost , risk , and limitation of parallel changeover What do you think ?
Do you have to think about limit ?
All right , let 's look at here The risk is lower Why ?
Because if there 's any problem happens with your new system we always have a backup We can always go back to old system because we had been using old system for quite long time But even though there might be some illegal issues with that but use it for long time So the risk is lower but cost is the highest So we have to run both old and new systems so that means we two set up people two set of hardware , two sort of software So it 's going to be the highest cost Any limitations ?
Yes . Sometimes we may not have enough resources to run both old and new system at the same time Resources means we need two sort of people we need two place we need two sort of hardware , servers , software So in some cases we may not have enough resources If we do n't have enough resources then that 's going to be limitation of this parallel changeover Now , we look into the next one The next one we call phase changeover What is phase changeover means ?
Phased changeover means is suitable for systems that we can divide into different modules So large system comes with many modules So what we do is in a phased changeover we take for example Module 1 put the new Module 1 and observe how it goes and then we add the Module 2 Module 3 , Module 4 and then we finally replace the entire system That 's what we call phased changeover We have old systems modules and then we keep adding new system modules That 's called phased changeover Now , what do you think about risk , cost and limitations of phased changeover ?
Now , let 's look at here The risks and costs of the phase changeover depending on two different factors The first factor is the number of modules The more the modules means little bit becoming longer time , right ?
At the same time second aspect would be what we call interaction among modules What does interaction means ?
That we discussed before that output of one module can be input to another module , right ?
So that 's what we call interaction on modules Sometimes interaction could be extremely high So in this case , that is going to be very very high risk in that situations Any limitations ? Yes .
The phased changeover may not be applicable to every systems because we have to find the boundary of Module 1 to replace Module 1 , right ?
If any case your system is very complex interaction among modules are extremely high we ca n't find the boundary So in this case that if you can divide into modules those systems , complex systems are very difficult to have this phased approach So that 's the limitation of the phased changeover Finally , we 'll look at the abrupt changeover What is abrupt changeover means ?
Abrupt changeover means we decided that this particular date we retire the old system and then we come up with the new system that 's what we call abrupt changeover Okay , now , then we look at here the cost , risk and any considerations for abrupt changeover Now , what do you think about the cost risk ?
Can you figure out any considerations for abrupt changeover ?
All right So in this , case risk highest because we are not going to use old systems So we retire and then start the new system So if the new system starts having troubles so it 's going to be high risk and the cost is lower because at one given point of time we are going to use only one system It 's going to be the lowest considerations So considerations means that for example we decided the date that we finally signed our new system is good after doing testing then can we decide any date to replace or retire the old system and come up with a new system ?
I do n't think so . Think about when you go to university when you take courses So you are doing at the beginning of every semester you register for the courses to the course register system So is it a good idea You guys begin with course registration from old system and the middle of the course registration we move to the new system Is it a good idea ?
Of course not So what we need do is to we need to end to the end of the cost ranging systems so we call end of the add-drop period If it 's financial reporting we should end the year end financial reporting and when the new financial reporting year starts so then we come up with a new system So we have to wait to the end of the processing cycle at the end of the financial reporting period at the end of the add-drop period for the cost and consideration So that 's how we should do that So the next question I would like to ask is what kind of system development methodologies is good to use for abrupt changeover ?
Remember , if you want to have a new system there are different ways to have a new system We develop by ourselves We call in-house development or maybe somewhere else exposed developed for us we call it outsource .
Which one is better ?
Hi everybody . Let 's look at the summary what we have learned in our week four So first what we have done was we tried to define the disaster So disaster means that we will not be able to continue our critical business processes for long period of time The same time remember that the criticality of a process may depending on time of the disruptions Remember our example , ATM machine is down what time is more critical ?
I for example , as a angel investor got a pitch from a restaurant a rapidly growing restaurant very successful and a very good restaurant In this particular restaurant set has an incentive for investors If you invest 10,000 dollars in our convertible debenture , or convertible bond we will give you 1,000 dollars of dining coupons in addition to interest and equity in return These 1,000 and dining coupons are transferable So you can give them to family and friends you can let your wife take her friends out and I mentioned this to my wife and she said , " Yes we should fund that one " .
I want to go out to dinner . I like that restaurant I can take my friends and it 's free " .
All of these things work sometimes in some ways But if our goal is to prevent fraud we may have problems Signatures can be defrauded IDs , pictures , magnetic strips codes on the back of a card can be stolen On average , in the United States last year 2017 there was one credit card fraud approximately every two seconds That 's a lot of fraud across society Tens of millions of credit card IDs have been stolen Does that make you not want to use a credit card ?
Well , you might worry about your security your ability to protect yourself but you also may say " Yeah , but it 's really convenient .
Is that a fraud ? " But it could be risky and there may be other ways to use biometric fraud They 're better than passwords but they 're not perfect Documents are often relied upon passports , shipping documents , other information These documents may prove that you own a car or that you own property of other types but fraud can happen with these , too You can have misrepresentation or forgeries and so what else can we do ?
How can we do this in an online space ?
Well , in a digital world facial recognition can be better and some ways better than the old village is you may be able to say , " Yes .
Thank you
Welcome back to our course on FinTech Foundations and Overview In this session on FinTech technologies we 're going to look at the issue of big data and data analytics And we 're going to take a look at what does this mean both in terms of the opportunity and how this technology or capability enables FinTech applications to develop With me I have Dr Hilton Chan adjunct Professor of HK university and CEO of his own company and the FinTech space So , as we talk about big data how important is data data mining , data analytics to fintech ?
>> Yes , I mean in the old days you know a lot of the data residing with the government or some of the big companies they have control of all the data like the banks the telecommunication companies et cetera But now with Internet a lot of the data are being uploaded onto the Internet including news , a lot of articles we read about a person , blogs you know There comes a wealth of information wealth of knowledge that is out there >> And sometimes the data is with these Internet companies like for example Alibaba knows a lot about you Uber knows a lot about you Airbnb may know a lot about you and so they may have the data and other databases that the big banks or telecoms or insurance companies do n't have >> Yes , that 's right .
I mean nowadays everybody will use search engines Obviously , you leave down a digital trail on a lot of the search engines Before I want to visit Australia I probably would key in those information and you know as for what are the scenery sports in Australia , hotels , et cetera So , that is kind of so-called for knowledge before I actually do the planning then I 'll really release some of my quoted data on the Internet , on the search engine So the search engine will be able to pick it up and know that how many people in a summer are planning to come to Australia So , those information are not even available to the government 's visa office in the old days >> How does that help FinTech ?
>> Yes , for example , if those information are available then you will consider some of the stocks you know So , whether those informations will affect the tourist industry in Australia or whether let 's say if there is a certain disaster in a particular country would that have any impact upon the tourism impact upon certain stocks in the market ?
Yes Yes And because we know that in some of the countries that 'll mean Tourism is big Tourism , foreign students , immigrants and then all these are things that they all have impact upon the currency as well , yes >> We may be able to gauge public sentiment based on data mining as well >> Yes >> Okay .
So , that can matter but one of the challenges of data mining is that data is often of poor quality >> Yes . >> So , what do we do about this problem of data quality or unclean data if you will or chaotic data ?
Now very often that on the Internet right now there is what we consider as not only factual data There is a lot of opinions a lot of rumors and a lot of kind of quote and quote And then you do n't even know what is the original source of that And then also , stated bill point at other data and then once you find out the original source the story could be totally different et cetera Now these are unavoidable >> So lots of fake news >> Lots of those information being unconfirmed and that are happening >> Right >> Of course there 's the good and the bad part of it I mean , the exciting part is for technology people is that how could you design a search engine or design some sort of engine to analyze the ocean of data to identify some of the good ones or the quantum ones that fits your problem solving requirement >> So , we have more and more data but that data may not be easy to use ?
>> Yes , and that 's why it comes up with a lot of data analytics models a lot of engines and people apply statistics on different models to try to find out , for example I mean one something very simple basic you may say if different sorts of data are pointing to that direction we consider that a piece of data will be something closer to factual But on the other when people understand that and people trying to create a so-called quoted a fake fax from different source so that it would see the engine , right ?
I mean , very often that they create a lot of negative names It seems that negatives are from a different direction Let 's say all the property market is going to be in bubble or whatever And people take advantage of the short response that they do n't have to the market could have a short response and bounce back to normal , right ?
But the people creating these noise just need to make money off that short response >> Right >> So the slight drop or slight increase especially a slight drop Increase is probably difficult to push the price up but drop is easier to do So that 's why a lot of these people are purposely using different techniques to kind of influence the people 's behavior so that they can take advantage of that in the financial market >> Can we use this in other contexts besides FinTech ?
Could we influence politics ?
>> Oh yes , that 's a very good example Because , I mean people are so remote from the candidate , right ? They do n't personally know the candidate , right ?
>> Yes >> So , it 's through , you know , kind of rumors opinions and other people what they are saying and then by seeing what is the video clip , you know on only part of his life or whatever a snapchat of something and then just kind of exploded >> An example of using social media in an interesting way with data is bombarding the Russian people with data about how wonderful Vladimir Putin is reading his own song , giving his own hype And you got a great candidate People think he is wonderful not everybody but a lot of people do >> Yes >> People I know who are from Russia will say " no he 's really pretty cool .
You should see him wear a shirt on . " >> Yes >> I do n't know if I want to see President Trump without a shirt on but okay maybe some people have But , at any rate so we can get a lot of hype and buzz How important is this increasing amount of data this better tools for data analytics in FinTech markets ?
The human body runs on food . Once , food shortage was the major concern .
After the Second World War , technological advances in food production led to a new era that was characterized by an overabundance of inexpensive food , and relatively little physical activity . In the decades that followed , other socio-cultural shifts continued to contribute to the changing way we ate .
Women who had previously controlled most of the average family 's food preparation , now enter the workforce in significant numbers .
And the processed food industry began to capitalize on our need for fast convenient food .
This meant that fewer meals for being cooked at home , and since convenient foods were generally higher in calories than home-cooked meals , the average persons caloric intake increase dramatically .
Academic studying the intersection of food and health like Michael Pollan , have written extensively about the implications of this cultural shift on the way we eat today .
First , we eat less healthy food , more salt , and fat , and sugar .
But we also eat more food because processed food is often designed as snack food , and marketed to us as a way to eat continually through the day .
>> The changes in the way , we , as a society ate led to the emergence of obesity as a recognized chronic disease with well-defined health consequences , and medical recommendations were made to try and address this growing health crisis .
In the second half of the 20th century , a lot of attention was focused on reducing saturated fat , and total fat in our diets .
And the processed food industry responded by giving us what we wanted .
But , they still had a vested interest in selling their products , so they found other ways to make the reduced fat products taste good . One way they did this was by adding significantly more sugar , and other forms of sugar , like corn syrup to almost everything we ate .
This not only made the reduced fat foods more appealing , but it also increased their shelf life .
So the food industry had a huge incentive to add corn syrup , and other sweeteners to packaged foods .
The resulting increase in our intake of simple sugars fueled our modern epidemics of obesity and diabetes .
These shifts in our food consumption patterns have led us to a point in history where our physiological adaptation , our ability to store energy as fat , has become maladaptive .
The balance between food availability and energy expenditure has been disrupted , and its left us with an exponential increase in the incidence of obesity over the past 60 years . An epidemic that the World Health Organization has labeled a worldwide public health crisis .
One of the biggest pitfalls of the typical Western Diet is the fact that it lacks the amount and variety of plant based foods that human beings need to protect their long term health and maintain a healthy weight .
The average American consumes less than two cups of fruits and vegetables everyday , less than half of the recommended daily intake .
So , for the majority of people who wish to improve their diets , one of the most important things they can do is to increase their daily consumption of fresh fruits and vegetables .
But , it 's also important that these foods are prepared in a way that supports our health and that 's where knowing how to cook tasty vegetable dishes can be such a powerful skill to have . One of the best ways to start any cooked vegetable dish , or any savory dish for that matter , is to chop and saute onions in a small amount of olive oil .
And if you like the flavor you can add a little bit of garlic as well . >> Onions are the basis of so many dishes and in fact I think one of the great impediments to cooking is people 's dislike of chopping onion and you just have to get over that .
Form this flavor rich starting point you can add chopped veggies like zucchini or spinach with a bit of salt and pepper to make a delicious veggie breakfast omelette .
You can make a simple tomato sauce by adding chopped or blended tomatoes and a touch of salt and you can make a colorful vegetable stir fry with almost any vegetable you happen to have in the fridge .
Just chop it up and flavor with some soy sauce or plum sauce or any other sauce you happen to like .
There are some safety precautions involved like keeping your fingers away from the blade of the knife and stabilizing vegetables by placing them flat surface down while chopping .
But in reality , the health risks associated with vegetable chopping are probably far less serious than the health risks associated with not eating them .
>> And you do n't have to eat organic to eat well , it 's the first decision is to eat those fruits and vegetables .
And if you can afford organic great , some organic foods taste better , they all have less pesticide and that 's important .
But it 's less important than the fact that you 're eating fruits and vegetables .
Better to eat pesticide laden fruits and vegetables than none at all .
I think that quality is one of the most used words in the food and beverage business .
When I talk to managers and entrepreneurs , the number of times that the word quality is used I think is extraordinarily high .
The point is that quality is a very vague and ambiguous concept because quality is like a box , you can fill this box with many different things .
The most important thing to understand is that there is always a difference between intrinsic quality and perceived quality .
In this case the company will put a lot of emphasis on suppliers , procurement , and raw materials ; so everything which is upstream , which is before making the product .
but actually as we said consumers are not always able or capable to distinguish different products in terms of their intrinsic quality because consumers reason on perceived quality and this is very important for a managers and entrepreneurs tend to resist the idea that consumers live in a world of perceptions .
What does that mean ?
Put as an extreme , it means that there is no objective reality when you think in terms of consumers .
We consumers , as human beings , live in our perceptions .
If we perceive something , that thing exists .
This matters a lot in terms of quality because what is relevant to us as consumers is percieved quality , the quality we perceive when we think , when we consume , when we buy products and services .
The determinants of perceived quality are different from the determinants of intrinsic quality .
As we said intrinsic quality depends on everything which is up stream : the quality of the materials , the quality of the supplier , the quality of the sources of supply .
What are the determinants of perceived quality ?
Nothing has to do with this upstream part of the product . What is important for the perceived quality is the information that consumers have , the knowledge they have , the competence they have .
If I as a consumer have a limited access to a number of information sources and the information on which I can rely is very limited and so my knowledge and competence will be determined by this , and so my perception will the influenced .
My idea of quality , the quality I perceive is completely different from the quality perceived by another consumer who has a lot of information , knowledge , and competence .
The point is that a company should keep its intrinsic quality aligned to the perceived quality , and managers and entrepreneurs should try to learn as much as possible what the determiners of the perceived quality of the consumers they want to serve are .
This is the only way to match the intrinsic quality of the product with the perceived quality of consumers .
In every market customers are different .
We can assume that every market is made of different customers and why are customers different ?
When they buy value propositions , they are expecting different value , but we put ourselves in the shoes of the companies , managers , and entrepreneurs , how can we deal with such a difference ?
How can we serve so many different customers in the market ?
So one very important strategic decision that a company has to make is market segmentation .
That is to say making evidence of that difference and trying to leverage on that difference in order to provide consumers with different value propositions .
So the idea of market segmentation is very clear .
In the market there are many different segments which are basically groups of customers made by customers which are more similar within the segment and more different from customers within other segments .
So the idea of market segmentation is exactly this : grouping customers together according to some similarity .
Why is this so ? Because at the end of the story , companies should be able to serve different segments of the market with different value propositions .
Since different segments are made of customers who expect different benefits and to make different sacrifices when buying products and services , serving these different groups of customers means designing different value propositions which are more effective in satisfying their expectations .
So basically market segmentation is a process .
When a company starts the process of segmenting a market , it has to go through different steps : The first one is identifying the segmentation criteria .
What are the segmentation criteria ? Segmentation criteria are exactly the characteristics according to which different customers can be grouped together .
The second step of the segmentation process is customer profiling .
What does that mean ?
It means that once I ve identified different groups of customers , different because they have different expectations , I can try to identify other variables which can better describe those customers from customers who belong to other sectors .
The third step is targeting , that is to say , once I have identified different segments in the market I have to choose which ones I want to sell to .
A company can decide to serve the whole market , all of the segments in the market , or it can decide to sell to only a few segments in the market .
The point is , what are the segmentation criteria that a company can choose in order to segment the market ?
Criteria can be very different , they can be linked to the individual characteristics of customers .
Let 's remember that the question we want to ask is ,  Why do customers expect different benefits and to make different sacrifices ?  Some of these differences are linked to the differences in individual characteristics of customers .
There are demographic features like age , gender , residence , and geography ; there are social characteristics like the social networks they belong to , profession , primary social groups , and secondary social groups ; there are values , so it can be based on the values that are important in their life .
Some are psychological characteristics and traits .
Are they introverted or extroverted ?
Are they innovative or against innovation , maybe they re very sensitive to innovation or less sensitive to innovation .
There are many different individual characteristics that can be used as segmentation criteria .
But the most important one , is the benefits they seek .
So basically we can group different customers according to the different weight they give to different combinations of benefits .
Once I have identified different groups of customers , different segments in the market , according to the different combinations of benefits they seek , then I can add new variables in order to profile those customers .
Let 's take , for example , the beer the industry . There could be one segment which is very sensitive to taste , I ll make it very simple , another segment which is very sensitive to convenience , another segment which is very sensitive to the origin and heritage , and another segment which is very sensitive to the quality in terms of matters of production , so you can identify different segments seeking different benefits .
The point could be , in terms of customer profiling , what are the distinguishing features of customers who seek convenience ?
Why are these customers different from other customers ?
Why are these customers different from other customers ?
Maybe it 's a matter of age , maybe it s a matter of income , maybe it s a metter of expertise , so there are some individual characteristics which can distinguish certain customers from other ones .
Once I have identified the segments , I have to choose , so the company has to choose what kind of segments of the market it wants to target .
Targeting is , again , another very important strategic decision because when I target a segment I decide what part of the market I want to compete in .
Maybe I want to compete in the overall market , I want to compete with some specific groups of customers in the market , or maybe I want to compete on one single segment .
This is a decision which is very important for the company .
Once it is made it is very difficult to change it in the short term .
The point is , how can I decide what segments to target ?
In this case the most important evaluation to do is assessing the attractiveness of each target customer .
How can I have already attractiveness of one single segment .
Attractiveness can be evaluated in many different ways but the typical three ways a company is used to evaluating the attractiveness of a segment aree : financial attractiveness , a segment is financially attractive if it is big enough and consistently big with the expectations of the effectiveness and efficiency of the company .
It is growing enough , so it 's a matter of rate of growth , and so it can allow the company to grow with the segment itself .
The potential market is interesting .
Customers are not served by many competitors and so from a financial point of view , the investments needed to compete in that segment are not so huge .
The second feature is competitive attractiveness , that is to say , an assessment of how fierce is the competition on that segment ?
So I able could be the company to build a competitive advantage in that segment .
How many competitors are competing in that segment ?
How strong are they ?
What are the value propositions ?
Are customers appreciating their value propositions ?
Are customers loyal to their value propositions ?
The third thing to be considered in terms of assessment of the attractiveness is the non-financial returns of competing and serving one segment , that is to say , the returns in terms of image for example .
If I serve a segment which is recognized as made up of expert consumers , my image as a supplier would be improved or again expert consumers usually expect highly sophisticated value propositions .
So if a service certain segment of the market , I can be recognized as a supplier who is able to satisfy highly sophisticated customers .
We have only talked about end consumers , but we can also talk about business customers .
For example , if I m a wine producer and I m able to be in the wine list of top restaurants , this will contribute a lot to my image and to my reputation .
On the other side , if I m only able to serve low end restaurants , obviously my image is consequently affected .
The attractiveness can basically be assessed in three main ways : from a financial point of view that is basically what kind of financial returns I can get out of that segment , from a competitive point of view which basically means what kind of competitive advantage can I expect by serving that segment so which kind of market share which kind of sales can I get when I sell that segment , and thirdly and lastly which kind of non-financial returns they can I get by serving that segment ?
Usually in the food and beverage business there are two most important non-financial returns which are the image of my company and the reputation that a company can get in terms of the competence it has in serving specific segments .
Food and beverage products can not be consumed only by drinking them or eating them , they can also be considered as content to be listened to , watched on TV , or read about .
To explore this topic today we are with Nils Hartmann , the director of original productions of Sky Italia .
There 's been an increasing diffusion of cooking , food and beverage formats on TVs around the world , what are the reasons for this success ?
The content in itself is something everyone can relate to .
It s easy and very accessible especially in a country like Italy where we have a great food and cooking tradition .
I think in this country especially you have an endless territory of discovery you can make .
Even in a format like Masterchef , the content , the regionally we play with has a vast territory and gives us lots of content to play with .
As you mentioned Masterchef is broadcasted in many different countries in the world countries with different culinary and food cultures , so is there any difference in the way the format is received or produced in countries with such big differences ?
I think by changing the chefs and by having your home competitors from your country , that alone changes the content completely .
The peculiarity of Masterchef compared to other talent shows like X Factor , a singing contest , is that you have to put one hundred percent trust on the chefs because while , this might seem simple and banal , but in the end it is n't because it s the core of the program while in a singing contest you hear what the guys singing , you can not taste what these guys are cooking so your trust goes into the chefs .
The chefs are the absolute rock stars of the show and obviously they have different backgrounds and are different in each country ; for one .
The other strong part of these shows is really the stories of the contestants .
It 's about cooking , yes , but it 's about the backstory of the one guy , the story of the lady that this is a bit older so people get affectionate and then they go for one of the other contestants , so the stories are really important and they are very Italian stories that are very tied to the country .
and the other element is the regionality of the food .
In Italy we have endless resources and specifics and that the something when the authors of the show put together the different tests the they take into account the all the great resources we have in Italy and that , again , makes it very specific to our country .
What stays the same are just the basic rules , the outline of the program , the content is very national .
Actually MasterChef is being broadcast in different countries and now in some countries the number of editions is different from the number of editions in other countries , how does the format , the program , the reception 's by viewers change throughout the years ?
It 's interesting we have a yearly summit of all the countries that produce MasterChef and what you see is that at a certain point like always in television you want to change something so that the audience does n't get bored and you do nt have a dip .
What we found is that is a very difficult balance because if you change some of the core elements of the show , people will miss that and the audience goes down , so you have to be very careful about what you change and what you do n't change ; but mentioning Australia for them MasterChef is like the Champions League Final .
It appears just like soccer , people do nt get fed up , they just like it and they can relate to it much more than any other show .
Okay .
When we think of food and beverage products and we consume them by drinking or eating .
We know that , for example , to make something you mix ingredients to make a recipe and the recipe could be good or bad , but what makes the recipe of food and beverage as content in a TV show ?
What are the basic elements ?
because as you said people can not taste , so is it a matter of creativity from a production point of view ?
Like all television it 's all about good storytelling so even in these shows although you do n't have the script , what you do is you write the script in the editing , you choose the contestants because they have good and strong backstories and then when you put it all together you shoot hours and hours of footage every day and then what gets put into a program is a very small part .
That is a very careful storytelling process and I think that 's what makes it work .
There 's a interesting analogy , the loyalty index of a program like MasterChef is incomparable to any other program ; a movie is usually around sixty percent when it 's a good movie .
That means that people will watch it and then go in and out of the movie and go somewhere else , while MasterChef is around eighty percent .
That 's equal only to some of the good scripted series like House of Cards where you really star an episode and watch it to the end because you want to follow the story , so it 's not in-and-out television , and MasterChef has exactly kind of effect so it 's obviously about the good storytelling .
One question I wanted to ask you how do you think a program like MasterChef has impacted the food culinary culture of Italy in the case of MasterChef Italy or any country where it is broadcasted ?
I think what you find is that people go along with the game and they do a mystery box at home and they play around so I think these icon elements of the show really had an impact on what people do at home .
The other interesting effect is how many kids watch the show , and I 'm not talking about Junior MasterChef , I m talking about the real competition .
Kids are really interested in that and I think what you find and I find that with my own sons at home is that you can , through some of the elements they will recognize , bring them to start cooking or playing around with food or fool them by making them cook some vegetables they would otherwise never eat .
I think it does have a very strong impact on society in a positive way .
Restaurant owners , or manufacturing companies , has there been any reaction that you feel that they had ?
There s quite a strong element of sponsorship in such programs .
Obviously , the food industry is interested in sponsoring the show .
It s a difficult balance , and being the owner of the content , I have to fight with our commercial part because if you have too much product placement in there , or the wrong products you will loose the credibility of the program .
If you have an icon like Carlo Cracco who s a Michelin Star cook and he goes out there and does advertising for Patatina San Carlo , well that 's dodgy because people , I think the other element which makes the format work in general , is the credibility and if you start to have too many brands in there or that credibility you could very quickly vanish so you have to be careful about that .
A few days ago I happened to be in a restaurant , and the restaurant owner was telling me that now everyone in Italy feels like a chef of MasterChef , so when they have to interact with their clients , the feeling is that these clients are more ambitious .
Do you think that this could be an effect of MasterChef ? I think there 's terms like impiattamento which in English I do nt know , it s how you put the food on the plate in a visual way , that were terms that probably before MasterChef were nt used , and you probably have people going into a restaurant looking at the aesthetics of it , so I think there 's been an evolution in a good way .
I can imagine that for restaurant owners that is more challenging because people are more aware , and I guess especially if they go out and pay for it they are more demanding .
What do you think about the evolution of food and beverage of cooking shows on TV ?
Is there any thing that you think would be changing or anything that would be staying over time in the next few years ?
I think that we have a new era now of codified food programs where as before it was just someone behind the stove cooking . There s probably going to be a new  - I think there are already is a movement of snack content .
The recipes you would have brought to you by cooks or content online are in a very short and smart way .
I think we 're going to see a lot of that online ; there already is actually . I think there 's always going to be change and there 's always something that is more fashionable and cooking is right now so maybe in another four years we 're going to have something else that is stronger , but I think it 's one of the elements of everyday life so I think it s always going to stay probably just the way the story is told is going to change .
When talking about innovation with food and beverage companies , there is this tendency of managers and entrepreneurs on focusing only on product innovation .
Product innovation is very important by definition .
Product innovation can be applied , it can be deployed in many different ways , But what is important to me is that companies tend to focus a bit more on value proposition innovation .
Innovation in the value proposition .
Because what customers buy is a combination of benefits and benefits are always related to sacrifices .
This is the concept of value proposition : a set of benefits related to a number of sacrifices the consumers have to make in order to get those benefits .
If we consider value proposition as a ratio , the innovation can be pursued in many different ways : by increasing the number of benefits , by decreasing the sacrifices , or by a combination of the two .
Or , you can consider a different way of decreasing the benefits and with more than proportional decrease of the sacrifices .
Because another tendency of many companies is to consider innovation as adding something .
But sometimes , if a company is able to simplify the value proposition , for some consumers into the market , it can be very valuable .
In any case , thinking of the value proposition and how to innovate the value proposition basically means how to innovate the ratio between the benefits and the sacrifices .
A good example of simplification is in the wine industry .
Wine is a very old traditional product especially in European countries where it has been consumed since centuries ago , or maybe thousands of years ago .
In this case , wine should be considered very easy , very traditional , very habitual in the consumption patterns and practices of many consumers , but the evolution of the competition in wine industry has made this product very complicated sometimes .
Many consumers perceive wine as a broad category to be consumed only by competent consumers , which basically means many consumers think that they need a competence in order to understand and appreciate wine .
If that is the case , this basically prevents many consumers from consuming wine .
The idea is ,  How can I simplify the wine ?  One example which is now a very traditional and very powerful case study is Yellow Tail .
Yellow Tail is considered one of the best examples in how to simplify wine , how to make it easies to drink for consumers .
The story says that Yellow Tail is a brand owned by an Australian company called Casella which has been very successful especially in the U.S. market which is the second largest market for wine consumption in the world because they tended to simplify the product , not the product in itself , but basically the way consumers perceive the product .
If you think of a label on a wine bottle , there is a concentration of information which is impressive .
Usually , this information is what prevents consumers from approaching the wine , because to understand many parts of this information , the consumer requires some competence .
So Yellow Tail basically took most of this information away , simplifying the assortment , and making the wine very easy to appreciate and drink , which basically means it s very easy to choose .
Because by simplifying the information you give to your consumers , your consumers can find it easier to make a choice .
So the point is , what are the components of the valuable proposition that I can innovate in order to give more value to consumers than competitors do .
There are many different ways .
The typical one is product innovation .
So , when we talk about product innovation in food , it basically means ingredients , methods of production , so a very technical innovation ; But we ca nt to forget that consumers consume products in specific occasions of usage .
That basically means that a great innovation which is a market innovation is to persuade consumers to consume the product in a different occasion of usage , different from the one they are used to .
Let me give you an example , the beer industry .
In the beer industry , maybe the two most important product innovations in the last decades are non-alcoholic beer , which has become in many countries the most drunk beer . Think about the U.S. , one of the most important markets in the world , the market share of non-alcoholic beer is larger than that of alcoholic beer .
Obviously Craft beers are very traditional , so where is the innovation ?
Craft beer is basically beer as it was made in the past .
But again , it 's a market innovation , a market innovation because these beers that usually are produced by microbreweries , very small breweries , very local breweries . It 's a matter of lifestyle .
If you drink a craft beer , you basically share the same values of the producer .
It 's much more to do with the values , obviously also the taste , but it 's sharing some values with the producers which make these kinds of beers .
The keg is a small barrel of beer which is five liters and basically the idea here is not an innovation in the product , because the product is always the same , but innovation in consumption of usage .
Many people have parties at home and the parties at home they buy a lot of bottles of beer .
Now , they can buy only a few kegs and this is a way to help them in getting the benefits they want to get .
The idea of the sub is the exact same as the idea of the espresso coffee machine ; that is to say , you buy an espresso machine , and then you make coffee by using capsules or other ways of producing coffee .
The idea of the sub is you have your machine at home and you buy the capsules , let 's call them , of beer that you can include in your machine to have your draft beer at home .
Even in this case , the innovation has not been a product innovation but it s an innovation in the occasion of usage .
You can have your own draft beer at home .
You can innovate in the channels of distribution .
In this case , so again , the product is not changed in any of its features , but what is changed is the way the product is provided to consumers .
For example , by selling online instead of offline , there is an innovation .
You can reach different customers .
You can reach customers in different ways , and this is the real innovation that you can have .
Another innovation in the value proposition can be on pricing : different ways of pricing your products .
For example , you can have different ways of pricing if you sell your product in different outlets .
For example , if a mineral water is sold in a vending machine , the water is exactly the same , what is new is a combination of the channel of distribution and the pricing system .
Innovation can also be at the service level .
For example , service is very much linked to the package and the portion .
Again , the product in itself is still the same but if you sell it with different packages , you can provide consumers with a different benefit .
For example , with the sparkling wine , the traditional size of the bottle is seventy-five centiliters , but if you sell smaller bottles , you can be more appropriate for the target customers of singles .
If you are single and you want to consume sparkling wine at home , you would nt like to uncork a bottle of seventy-five centiliters , because if you open it and you do n't drink it , basically , you are wasting your wine .
If you can buy smaller bottles , thirty-three or fifty centiliters , it 's much easier for you to open it , and drink it , and enjoy it without wasting any part of that wine .
These are many different ways of innovating the value proposition .
The question is ,  Which way I should prefer ?  As a company ,  Why should I focus on the product ?   Why should I focus on the price ?  Actually , there is no  best-off .  You can choose the best way according to expectations of the target customers you want to serve .
Again , by choosing the customers you want to serve you can understand what are the specific combination of benefits they want to get , and what are the sacrifices that they want to make , and by having this knowledge you can design an innovative value proposition .
Every choice on innovation on the value proposition should always take the impact on customer experience into account .
Because the innovation of value proposition can impact different stages of the customer experience so it is important when designing the innovation to try to figure out at which stage of the customers  experience , this innovation should impact .
A value proposition which is innovative can impact the different stages differently .
For example , if the innovation is related to the pre-consumption stage when basically consumers get information in order to make a choice , any innovation in the value proposition which has to do with communication is important in this case .
For example , the Absolut vodka case is interesting .
Absolut vodka built its brand image at the beginning of its success by completely innovating their communication strategy , and specifically , the communication tools they use .
They basically made an investment on a massive production of postcards , and these postcards were related to different themes which could be related at that time to the idea of Absolut .
They introduce , in the vodka and in the spirits business , a completely new tool of communication which basically provided consumers with a different kind of information , a different pre-consumption experience .
Coke , a few years ago , launched this very successful campaign by adding names on the package of the bottles .
So this apparently very small innovation , but actually for the Coca-Cola company was a huge one , the idea was basically to give more personal flavor to a perceived very standardized product .
In this case the pre-consumption experience , which was basically trying to find out my proper name on the bottle or try to find the name of a friend to whom I can gift the bottle , and so on and so forth , completely changed the experience before buying and consuming the brand , the product .
Another innovation can be related to the purchase stage .
We know that the purchase stage is basically made up of two different sub-stages .
One is the choice and the second one is the shopping experience .
In the case , the innovation in the valuable proposition can impact the choice or the shopping experience .
One good example is in the pizza business .
If you think of pizza , it s a very traditional food .
It 's also a very traditional way of eating out .
Traditionally pizza was defined , the typical product was defined by the producer , the restaurant , so you could pick up a menu and choose among different pizzas .
But how can you innovate the choice experience ?
For example , by making the pizza personalized , that is to say , offering consumers the possibility to combine ingredients they like in a way that they come up with their personalized pizza .
In this case the choice experience is completely different , because the traditional one is you pick up a menu of predefined products .
In the second way you can personalize your product .
That is to say you can choose exactly which ingredients you like in order to have your own personalized pizza .
A third way of impacting by innovating the value proposition on the stages of the consumer experience is focusing on the consumption experience .
In this case you try to innovate something of a new value proposition that impacts the consumption practice .
There are two interesting examples for this .
One is in the , let 's call it , restaurant business , although it 's a very specific way of eating out .
Dinner in the sky is a consumption experience , which was launched as a single event , but then was so successful that it was remade in many different cities in the world .
The idea is basically to bring a number of people sitting around the table , very high , suspended , hanging in the sky .
I mean , you are suspended in the air . The experience is very different from a typical experience you have in a restaurant .
On the other side , another interesting consumption practice innovation is in the bitter industry .
There is a brand called Jgermeister which completely changed the way of consuming bitters , and was very successful .
But this positioning was so narrow , that for many consumers , who not have any digestive issues , they did not consider bitters as a good drink to drink basically .
The idea of Jgermeister was to change the consumption way .
They launched this new campaign inviting consumers to consume the Jgermeister bitter iced in a shot .
The real innovation was not a product innovation , it was a consumption innovation that the consumer could get as an experience when consuming the product .
Finally , you can have an innovation , the value position impacting the post-consumption stage of the customer experience .
Post-consumption means that you have both the product , you 've consumed a product and then you can do something with the product ; for example , sharing your preference with your social network .
In this case a very nice example is Nutella .
Nutella is one of the most famous brands of a company called Ferrero , which is one of the most important chocolate makers in the world .
Nutella is basically a chocolate spread , a cream which can be spread on bread or on other foodstuffs .
And the idea is that since it is a very powerful brand , a brand which has a very strong community , Ferrero decided to leverage on this community by inviting their fans to do something after their consumption , which is not only sharing their likes on Facebook , but participating in some specific contest .
They create some events where basically consumers are invited to go and do something , for example , cooking with Nutella or experiencing Nutella in a different way .
This has nothing to do with the consumption of Nutella but has a lot to do with the post-consumption stage , when you share basically your love for a brand with other people .
Let 's take a very interesting example , one of the most successful examples in the food service in the last years , Starbucks .
Starbucks started with one single bar in the 70s in the U.S. and now it can count on dozens of thousands of stores all over the world .
For some companies , it took centuries to become bigger .
For some companies , it takes years .
For some companies , it 's just a number of months .
So the point is how to design a growth path .
That is to say , how to identify , in the market , some growth opportunities , and to grab this opportunity in order to follow the growth .
When I talk to managers and entrepreneurs , the view that I get is that they think they have to follow the growth in the market , but the real point is to identify growth opportunities .
That is to say , the market does n't grow in itself .
A market grows because demand and supply interact and those interactions make it grow .
So it is important to try to figure out which are the options that a company has when the company wants to design a growth path .
To make it easy , we can consider a market as a pie and the position of the company in this market is one slice of that pie .
So , having this picture in mind , we can identify three different strategic options .
One is to grow within the current market , that is to say , to make that slice bigger .
So , one way to do this , and the only way to do this if you assume that the size of the pie stays exactly the same , is to take some part of the market away from your competitors .
So , making your slice bigger , means making the slice of your competitors smaller .
So , in this case , the growth is against competition .
And , so it means , the company has to find a way to make the value proposition more interesting to competitors  customers or to your customers so that these customers prefer your value proposition to the value proposition of your competitors .
One other option is not to consider the size of the pie as a datum .
That is to say , to consider that every time a market is in a typical stage of its life cycle , in every time , there is a potential market .
That is to say , there is a potential bigger pie out of which a company can get a share .
So , the strategy would be not to grow within the current pie , within the current market , but to try to activate a potential market .
activating a potential market means , basically , to make a bigger pie and then to get a slice of a bigger pie .
One way to figure out how to do this is to consider that basically , the pie is made of volumes , it 's not made of heads .
So that pie , the current one or the potential one , is made of volumes , concurrent volumes and potential volumes .
So how can you increase the volumes of a market to make a bigger pie , to activate a potential market .
So , if you want to increase your volumes or the volumes of the market , you have two different options or a combination of these three , increasing the number of customers , increasing the occasions of usage , increasing the quantities they buy .
Each of them are potentially strategies to activate a potential market .
So one strategy is to try to convert non-customers into customers .
And I 'm not talking about customers of the company , I 'm talking about customers who have the need and the desire , but do not buy any product within the product category , neither from me , nor from my competitors .
So , it basically means that these customers do not find , in the market , any value , proposition , mine or my competitors  ones , which is able to satisfy their needs .
On the other side , we have the occasions of usage , that is to say , customers who are actually buying within the product category , from myself or my competitors , but they do n't use the product for a number of occasions of usage .
In this case , one way to enact the potential market is to innovate the occasion of usage , to give customers an idea that the value proposition , the product , the service , can also be used in an number of different occasions of usage .
And , in this case , there are very nice examples that we 're going to show you .
The third option is to increase the quantity .
So basically , for those customers who actually are buying within the product category , they are using a number of occasions of usage but every time they buy and consume , they consume a quantity which is less than the potential .
So in this case , any strategy , any action which is able to increase the quantity consumed by each single customer , is able to increase the overall volume , and so the potential market .
The third option is to create a completely new market .
That is to say to convert non-customers into customers , but the point is also activating the potential matter we talked about converting non-customers into new customers .
That in this case , the third option : creating a completely new market , we have in mind customers who today do not perceive the need or the desire , or the need or the desire is a latent stage , it s implicit .
So , if you talk to them , they do n't tell you that they have a need or a desire but usually , this is because they do n't think that there is any value proposition able to satisfy their expectations .
They do n't think that their desires or their needs can be satisfied .
The difference is that in strategies , apt to activate a potential market , the focus is on non-customers who perceive the need or the desire but today , they do need satisfy this need or desire .
In the other option , creating a completely new market , the focus is on your own customers who do n't think , today , they can have their desires or needs satisfied .
Each of these strategic options is an option for the company .
When a company decides to grow , it has to choose one of these three options .
Obviously , it can also choose a combination of the three , but , usually , one of the three is , let 's say , effortful enough to focus on the investments and the efforts of the company .
One point to make is , obviously , the three are different in terms of risks .
If you want to grow within your current market , you can rely on your current competencies and also on the current knowledge that customers have .
So , basically , the risk can be a risk of innovation or it can be a financial risk , but you are within the same set of competencies shared between you , your competitors , and your customers .
The second option , enacting a potential market , is riskier because in this case , you want to change the habits , the perceptions , and the categorization systems of your customers , so the innovation is much stronger than in the innovation you have to pursue when growing within the current market .
And , obviously , the third situation is the riskiest , because creating a completely new market means to try to transform non-customers , that is to say , customers who are not thinking today that their desire and needs can be satisfied into customers .
It requires an innovation but at the market level and sometimes at the technological level but in any case , at the customer level .
And for this reason , the innovation is much stronger , much more sophisticated than the innovation you can use in the two first options .
This lecture is just to point you to some data resources so you might be able to get some free data , and do some of analysis , if you do n't happen to have any at the organization that you 're at .
So this website consists of data from a variety of national organizations .
So we can start off with the United Nations data sets , which are at this website , and then you can find data that are available from the United States at data.gov .
And you can go to this blog post to find a bunch of other cities and states within the United States that have open data .
So that 's a good place to start if you do n't know if your country has an open government data site .
Gapminder is another website that has a lot of data about development in particular in human heath .
And so there 's a large number of datasets that are available on that website which is Gapminder.org .
You can also get survey data from the United States .
This website here gives you a lot of information about how do you actually access the surveys and process them in R.
So it 's actually really nice because the surveys are often very big and unwieldy , and this website gives you a lot of information on how to access them .
The Infochimps Marketplace has a bunch of different data sets which you can sort by various different tags , and you can identify data sets that might be of interest to you .
Some of them are free , and some of them cost money .
Kaggle is another place that you can go to for data sets .
So Kaggle is a company that offers data science competitions , and they often have very interesting data sets that they make available as part of those competitions .
So they 're good for practice , but they 're also good for potentially discovering new , interesting things that can help companies solve real problems .
So these are several data scientists .
Hilary Mason , Jeff Hammerbacher , and others who have put together data sets that are research quality and that might be useful for you .
These all come from this blog post which talks about several other data sets , they 're curated by other data scientists as well .
So for example , the Stanford Large Network Data Archive has a large number of data sets that focus on network data , machine learning , the UCI Machine Learning archive has a variety of data sets that can be used to practice your classification or predictions .
The CMU Statlib is one of the most famous canonical sets of data sets that are available . Gene Expression Omnibus is focused on data sets that come from human genomic experiments or other organismal genomics experiments .
And then there 's data from the ArXiv or public datasets on Amazon web services .
Finally , there a large number of APIs that you 've now learned how to use , through the course of this class .
And so , for example , there 's specific packages such as the twitter package which can be used to access the Twitter API , in an easier way than trying to set up a application yourself .
You can similarly get access to figshare data or to data from publications like Plos one .
rOpenSci has a large number of very nice packages that allow you to access data from a variety of sources that are focused on academics .
There 's also dedicated R packages for Facebook and Google Maps .
All of these mean that there is really no excuse to not be able to find real data to focus on any project that you might be interested in .
Hello , and welcome to the course on Global Environmental Management .
My name is Henrik Bregnhoj .
I 'm a chemical engineer , and I 've worked with water supply and sanitation in developing countries .
That is the department that offers this course .
I 'm going to tell you very shortly about the overview of the structure of the course and the approach that we are using .
And in the first week we 're going to look at the trends , the global trends , like urbanization .
For example , here we 're standing in my neighborhood in Copenhagen where when I was a young man , I heard that we had about 70 something percent of urbanization in Denmark .
But the urbanization is still continuing and now we are actually 87 % urbanized people in Denmark .
So urbanization , demographic development , is when , some things we 're going to look at the climate changes that is also affecting the environment .
Then in the next two weeks , we 're going to look at management aspects in Week 2 .
We 'll look at some general management principles and some case studies from different countries .
And in Week 3 we will look at utility management , how are different environmental sectors managed in different utilities ?
And then in the last two weeks , we look at the environmental technologies .
And in the last week we focus on rural technologies about groundwater , about soils , and so on .
So every week the resources will be some lectures , online lectures that is hopefully giving some insight and some inspiration .
We have also some literature you can read .
But , as you all know , if you really want to learn , you also have to work with things .
So we also encourage you to do the activities .
Your task in this course is that three of the five weeks , you 're going to hand in a short essay , be less than one page , about the topic of that week in a specific geographical location .
Apart from writing the essay yourself , you 're also expected to assess three of your fellow students ' reports .
Every week we are going to have a multiple choice quiz testing that you have learned the things .
And apart from doing this we 're also encouraging you to contribute any experiences , any views , on global environmental management in the discussion forum .
And you can read more about the structure of the course in the page on the course description .
Welcome to the course .
Today we will talk about how water and environment is managed in China .
China is the largest single country in the world , and whatever happens in China , it spills over to the rest of the world .
We will talk especially of the No 1 Document .
Every year since the founding of the People 's Republic of China , the first document issued at the start of the year by Central Government and the Communist Party of China has concerned the well-being of the rural population , who are seen as the core of the party .
This No 1 Document is usually rhetoric , support and appreciation of the hard work of the rural population .
But the 2011 No 1 Document , which is called Accelerating Water Conservancy Reform and Development .
It introduced actually a new era in this type of document .
It was prepared by the Ministry of Water Resources and it reflects , of course , very much the tasks of the Ministry of Water Resources .
The No 1 Document wants to give the rural population water security .
It wants to give them security from flooding , protect them against flooding .
It wants to give them food security , not only to the rural population but to all of China .
And finally , it wants to give them water supply security .
In many parts of China , especially rural China , the water supply has been very poor .
But it is now being rapidly upgraded as a result of the No 1 Document .
The No 1 Document is focused heavily on investment .
But when President Xi Jinping came into office in 2012 , government focus shifted towards environmental protection and towards the well-being of the population , especially the rural population .
So the government has now made 2015 the Year of Rural Water Supply and Water Quality .
2016 will be the Year of Air Pollution , and 2017 the Year of Combating Soil Pollution .
But more on this later . Although it focuses on investment , it opens for important new policies and practices .
It mentions what we call Water Resource Demand Management .
It introduces ecological measures .
It calls for administrative reform of irrigation , the larger participation of the population .
And it makes government officials accountable for its implementation .
One thing is that it will publish now the ten municipalities or cities who have the best performance in terms of the environmental goals , and it will also publish the ten who has the worst .
And more importantly , it will make future finance dependent on the performance of the city or municipality .
The first strategy that the Ministry of Water Resources rolled out in response to the No 1 Document is called the three red lines .
It is three red lines which aim , number one , to control water use , and they will do so by allocation of water .
It will control water use efficiency , especially in industry , where it will work on different norms for the use of water in different types of production .
And finally it will work on water quality improvements in the rivers .
Water allocation was first used successfully on the Yellow River , which in the 1980s ran dry for up to six months a year and up to 1,000 kilometers from the coast .
So basically there was a dry river without water . And the problem was that when there was water , it would deposit all of the sediment that came from central China and gradually raise the river bed .
So when there were big floods , the risk of flooding of the surrounding floodplain was increased dramatically .
After allocation of water through the provinces along the upper part of the river and strictly enforcing it , water came back into the Yellow River .
And after building of the Xiaolongdi Dam , it was possible to flush the river for sediment early every autumn so that it would not be deposited in the lower reaches of the river and would not raise the river bed .
So this has successfully reduced the risk of flooding and also made it possible for the provinces downstream of the Xiaolongdi Dam to receive a part of the water from the river , their allocation of water from the river .
The second thing was water use efficiency .
As I said before , it was mainly aimed at industry but it also apples to irrigation . There is a term called the irrigation efficiency which is the amount of water from the river or from a dam that reaches the irrigation system that reaches the field and becomes beneficial for the farmers .
And to achieve higher irrigation efficiency , vast sums are being invested in lining of canals .
And as a groundwater expert , I 'm not really happy with this because I would rather see the water being allowed to infiltrate the groundwater , replenish the groundwater , and then be taken out from the groundwater where it is needed .
Also , in my opinion , it would be much better to consider the cost and value of water and ensure that water is allocated according to its highest value in competing users instead of just supplying more and more water .
And here we hit an institutional barrier in China because water quality is under the Ministry of Environmental Protection , which is responsible for limiting wastewater discharges to the river .
Therefore , Ministry of Water Resources has invented the term carrying capacity , which describes how much pollutant load a river can carry without violating its water quality objective .
This means the Ministry of Water Resources can allocate pollution quotas in the same way as they allocate water to the provinces along the rivers .
And these provinces then are responsible to distribute the pollution quota among the different types of industry in the catchment .
This approach turns rivers into a hydraulic system for discharge of pollutants .
And it 's obviously very far away from the EU and European practice of requiring industry to apply the best available technology and the associated descriptions of what to do best .
The next strategy which has been rolled out is called Water 10 .
It is ten commandments that the water users must adhere to in order to obtain a better water quality in rivers and lakes in China .
From the Big Leap Forward , under Chairman Mao , China has inherited abundant small factories scattered across the rural areas .
And these small factories , they can not afford to invest in clean technology , so they will be closed .
And many Chinese see this as a hidden support to the big state-owned enterprises in the same branch .
Ten key industries , they will be required to introduce new clean technology because they can afford .
The use of coal will be cut back significantly to reduce air pollution .
Coal-heated power plants , which today use cooling towers for dissipating the heat from their production , they will be required to recover excess heat and make beneficial use of it so that additional heating sources , coal-fired heating sources , can be reduced .
And here two Danish companies , from Danfoss and Cowi , are in the process of designing a heat recovery system which can feed the district heating of the city .
And it 's estimated that the factory alone can replace two big combined heat and power plants and nine small boilers around the city which used to supply hot water to district heating .
And of course , the environmental benefits of this will be enormous .
In terms of soil pollution , which also is a new focus for the ministry , Denmark is in the process of a pilot project at a big coal and gas plant in Wuxi .
Urban sewage and wastewater treatment remains a high priority in order to cope with the rapid urbanization .
In 1996 I led a World Bank project in Sichuan and Chongqing to upgrade wastewater management in 15 big cities upstream of the Three Gorges Reservoir .
And the fear , public opinion prevented the World Bank to invest in the building of the Three Gorges Dam , but in order to get a piece of the cake they could invest in better water and wastewater facilities in the cities upstream .
And the hidden agenda , of course , was to protect the Three Gorges Reservoir from eutrophication .
But later eutrophication of Chinese rivers and lakes came to the forefront of the debate in 2009 when an algae bloom in the Tai Lake , upstream of Shanghai and in the Shanghai area , was so severe that the city of Wuxi , another million city , had to stop its water intake and close the water plant for up to one week .
The focus turned towards diffuse pollution in the rural areas in order to control it , both from large scale pig and dairy farms , and also from the excessive use of mineral fertilizer . China has by far the highest use of mineral fertilizer and a surprisingly small use of animal manure as fertilizer .
So there are huge savings for China , for the farmers , by cutting back on mineral fertilizer , improving the use of animal manure , and in that way reduce diffuse pollution of groundwater and surface water .
Finally I should talk about water pricing .
In the EU perspective , it is important or noticeable that the No 1 Document , it mentions water pricing , it mentions progressive tariffs to be introduced .
But at the time , it does n't elaborate much on what to do .
Until now , water supply has been heavily subsidized by government so that people in urban areas paid perhaps one-third of the actual cost of producing drinking water , and in rural areas they paid nothing at all .
In 2004 the National Development and Reform Committee , which sets national policies for China , it advised provinces to stop charging for water for irrigation , as one measure to increase food production .
In 2012 , after the No 1 Document came into force , this recommendation was reversed when the Ministry of Water Resources and the National Development and Reform Committee introduced payment for bulk water delivered from rivers and reservoirs .
And noticeably this was based on the scarcity and value of water .
It means that in the southern part of China , where there 's abundant water , the water prices are very low .
Whereas in the northern part of China , where there is a scarcity of water , the price of raw water , or bulk water , is correspondingly high .
And very notable is that groundwater always is allocated a higher value than surface water .
In this lesson , you learned the concepts of how Kubernetes and Container Engine run and scale applications on a number of machines .
You can also run a number of applications on the same set of machines .
Welcome to module one of how to build a website in a weekend .
>> In this section , you will learn a little bit about how the internet works , begin gathering your ingredients for your web site , determine a domain name for your site , pick a theme , and articulate the purpose of your site .
de ] structure . You need to know a very important structure in this text .
For example , structure , is frequently used when asking details about the past or telling details about the past . It can be used to emphasize any detail , but mostly time , manner or place .
Hi I 'm Scott Klemmer an associate professor of cognitive and computer science at UC San Diego .
Where I co direct the new design lab . I 'm here today to invite you to join me for this new specialization with of Coursera on interaction design .
In this specialization you 'll learn how to design , implement and evaluate user interfaces and how to rapidly iterate and revise your designs .
You 'll learn fundamental principles for how to design user interfaces effectively .
How to translate those fundamentals into practice , to make your deigns betters .
And how to evaluate what you come up with , with real users .
So that you can understand that it meets an actual human need .
The place to start with this specialization is the Intro to Interaction Design class .
From there you 'll have an opportunity to dive deeper into a number of different topics so this include social computing , interaction techniques and evaluation .
Once you completed the advanced material here , you can take everything that you 've learned in all of the courses and apply all of that stuff together in a capstone experience that we 're putting together in a collaboration with Instagram .
Here , you get to do an open ended project where you get to show the world and yourself what you can do with all of these design materials .
Or something that you could use to impress your family and friends or get a job in the design field .
In this specialization like with any learning experience , you get out of it what you put into it .
I 've already heard from a number of you who are excited about doing the full specialization with all the materials in the capstone . Others of you might watch a few select videos because you already know this stuff or you 're just looking for a particular piece of material , and that 's great too .
You should use these learning materials in whatever way is most valuable for you .
And think about what your learning goals are , what you hope to get out of it when you go into the class .
Those of you who do complete all of these materials will have the opportunity to earn a verified interaction design certificate .
This specialization builds on materials that we 've done with Coursera for several years now .
And I 've been amazed by what students have done in our class already .
Here 's an example of how that stuff evolves over time .
So already all around the world thousands of students have created interactive web applications that express their design ideas and fill a human centered need .
Even beyond that , students around the world have shared learning materials with each other .
Some projects even go on to have a Kickstarter or a launch in other ways and that 's been awesome to see .
You can look at some of these previous student projects if you 're looking for design inspiration .
You can also use this as a chance to meet up in person as students around the world have already done .
So really the world is your oyster .
I 'm looking forward to seeing all of the great ideas that you come up with and the contributions that you make , and I 'm looking forward to you joining me with this new specialization .
Welcome .
Article 50 of the Treaty on European Union provides a mechanism for the voluntary and unilateral withdraw of a country from the EU . This is commonly known as Brexit , that is , a British exit from the Union .
Often , there is confusion amongst the public and indeed sometimes the media over the relationship between the European Convention on Human Rights , the ECHR , and the EU , the European Union . The simple matter is that they 're separate bodies with differing memberships remit and enforcement .
It now has 47 signatories or High Contracting Parties , which include the 27 EU member states .
Although it can not be emphasized enough , these are separate bodies with separate institutions and functions . Where the European Union had a predominantly economic focus at least in the early years , the Council of Europe and European Convention on Human Rights were all about ensuring the protection of human rights in the aftermath of the Second World War .
They were designed to ensure such atrocities would never occur again within the continent of Europe .
The Convention for the Protection of Human Rights and Fundamental Freedoms , as it is more commonly referred to , the European Convention on Human Rights , the ECHR , was drafted and came into force on the third of September 1953 .
The Council of Europe operates through traditional instruments of international law , and ECHR constitutes an international human rights regime .
The European Court of Human Rights , which applies and protects the rights and guarantees set out in the Convention , is located in Strasbourg in France .
Therefore , you will often hear reference made to the Strasbourg Court .
Applications to the European Court of Human Rights in Strasbourg can be made by an individual against the state under Article 34 , where the applicant is a victim of a violation by one of the state parties to the European Convention . The so-called right of individual petition was granted by the UK in 1966 .
After decision is made on admissibility , the Strasbourg Courts will investigate the application with a view to ideally reaching a friendly settlement of the dispute . If this proves impossible to achieve , a judgment will be given by the court .
Such judgment , if it finds that a state 's laws breach the convention rights , will impose an obligation on the state to remedy the law .
The Council of Europe Committee of Ministers comprise of the Foreign Affairs Ministers of member states , oversees the implementation of the European Court of Human Rights judgments .
However , ensuring compliance in the international order is not always simple , and the external pressure on a state to comply is important , supported by the ultimate sanction of expulsion from the Council of Europe .
We pay our taxes online and we communicate with others online . This is where we are today , everything is connected .
So let 's talk about the devices that we have . We have smart phones , we have smart watches , we have smart cars , we have smart TVs , all these assets are connected .
They are all online at the same time , so if I go to my house , we even have thermostats that are connected .
We have entertainment devices , like an Amazon Echo , or the Google speaker , whatever that 's called .
And now , depending on where or when you 're watching this video , the Apple speaker just came out , which is ridiculously expensive .
So let 's talk about threats real quick to these devices , webcams , specifically , have been used to create denial of service attacks on major corporations .
DYN-DNS , which provides DNS or domain name system that translates IP addresses into names , was attacked last year .
And took out a majority of the East Coast Internet , because of webcams throughout the world being used as a giant botnet .
But all these devices are some way of allowing us to communicate with each other , so they 're vulnerable .
Just today , I 've seen phishing emails sent to over , we have around 40 , 50,000 email accounts .
I 've seen somebody in the data center , that I did n't know who they were , but they had credentials to be there . Are you asking who is around that may be authorized or not authorized ?
I 've had to deal with a security incident in which student data may have been accessed . Luckily , it was n't , but I still had to deal with it .
I had to fix bugs in a system , I had to update software , and I also learned how to increase security in one way or another .
And this is actually a daily thing , it 's not just within the past week , computer security is a daily learning process .
I have all these devices , do I care what happens to them ?
So the point I 'm trying to make is that computer security is every bit a part of who we are .
All the devices that we have need protecting in some size , shape , or form , we all have information that needs to be protected .
What about the doctor 's office that asks for your social security number , do you really have to give it to them ?
I guarantee you , if you look at your email right now , there is some kind of phishing email that you are looking at , that you 're saying , no , I dont need to give my information , because that 's not the person that 's trying to send it to me .
My bank Apple , Microsoft , it does n't matter who it is , they 're trying to purposely attack you in some size , shape or form .
So this is where we 're at as a society today , we have so much information , but we need to protect anything and everything .
The smart homes that we have , the light switches , the thermostats , anything like that , TVs , what about the TV ? I do n't know if you 've read this in the news , but read it as you 're watching this , but look up CryptoLocker , which is a ransomware threat , a ransomware virus , that actually encrypted somebody 's smart TV .
Anything that we have electronic needs to be protected in some size , shape or form .
So what 's on my radar today ? These are the firewall hits in the middle of the summer , without students , or many students on campus , and during a one hour time period .
This is from a medium sized university , imagine a large corporation .
We probably only had 2,000 people on the network during the day at this point when I took this , but this is one hour .
These are actually the blocked events , the ones that never made it through the intrusion prevention system , okay ? These are all critical or medium threats , again , this is one hour time period , maybe we should investigate these ?
ZeroAccess is a really bad virus that we need to get rid of and squash right away .
Challenges , computer security is not simple , it takes time to understand .
You have to learn both the theory side and understand through example .
So when deploying services or systems , there are always going to be vulnerabilities . How can you account for them when the system is being designed or implemented ?
We ca n't account for everything , but we can do a good job of weeding through everything that that system is doing in order to understand what threats are going to be against that system by some threat actors .
It 's also necessary , as I 've talked before , about balancing security with practicality and productivity .
Security is a constant battle against those who want to do harm against us , threat actors .
Information is everywhere , knowing when to use it and knowing not to use it or how to protect it is a full time job .
First , you taking this course to get a better understanding of how to protect yourself in the practical manner in the practical side of security is a great first step . Second , the security field has near zero unemployment right now .
We need the best and the brightest to fight the adversaries out there , that 's why we 're doing these classes is to train for practical computer security .
Software and tools are becoming better at mitigation , however , the attackers are also getting better about attacking as well , and we must remain vigilant . We have to look at everything that we 're doing all the time , making sure things are updated , making sure computer system are backed up .
You 'll test the limits of your control design , and learn the challenges inherent in driving at the limit of vehicle performance .
These topics should establish the basic terminology for self-driving use throughout the specialization , list out the most important requirements for building self-driving solutions , and show how you can execute a dynamic driving maneuver in a self-driving car . I 'm very excited to be introducing you to these ideas .
Then I 'll give you a more detailed explanation about each of the maps so you can better understand How they 're created and used throughout the specialization .
This map is created using a continuous set of lidar points or camera image features as the car moves through the enviromment .
This map is then used in combination with GPS , IMU and wheel odometry by the localization module To accurately estimate the precise location of the vehicle at all times .
The second map is the occupancy grid map .
The occupancy grid also uses a continuous set of LIDAR points to build a map of the environment which indicates the location of all static , or stationary , obstacles . This map is used to plan safe , collision-free paths for the autonomous vehicle .
The third and final map that we 'll discuss in this video is the detailed road map .
It contains detailed positions for all regulatory elements , regulatory attributes and lane markings .
This map is used to plan a path from the current position to the final destination .
As I mentioned previously , the localization map uses recorded LIDAR points or images , which are combined to make a point cloud representation of the environment .
As new LIDAR camera data is received it is compared to the localization map and a measurement of the eagle vehicles position is created by aligning the new data with the existing map .
This measurement is then combined with other sensors to estimate eagle motion and ultimately used to control the vehicle Here we have some recorded LIDAR data from our self-driving car .
The movement of the vehicle is clear based on the evolution of the LIDAR points in this video .
The construction of this map will be more rigorously explained in the next course of this specialization .
Where we discuss localization in detail . The occupancy grid is a 2D or 3D discretized map of the static objects in the environments surrounding the eagle vehicle .
This map is created to identify all static objects around the autonomous car , once again , using point clouds as our input .
The objects which are classified as static include trees , buildings , curbs , and all other nondriveable surfaces .
As the occupancy grid only represents the static objects from the environment , all dynamic objects must first be removed .
This is done by removing all lidar points that are found within the bounding boxes of detected dynamic objects identified by the perception stack .
Next , static objects which will not interfere with the vehicle are also removed .
As result of these steps only the relevent writer points from static objects from the environment remain .
Let 's look at an example of an occupancy grid updating over time . Here , we see the occupancy grid visualized as the light gray square area , under the autonomous car .
Updating the position of static objects in the environment with black squares .
As the autonomous car moves through the environment , all stationary objects in the environment such as poles , buildings , and parked cars , are shown as occupied grid cells .
Finally , the detailed roadmap is a map of the full road network which can be driven by the self-driving car .
This map contains information regarding the lanes of a road , as well as any traffic regulation elements which may affect them .
The detailed road map is used to plan a safe and efficient path to be taken by the self-driving car .
The detailed road map can be created in one of three ways .
Fully online , fully offline , or created offline and updated online .
A map which is created fully online relies heavily on the static object proportion of the perception stack to accurately label and correctly localize all relevant static objects to create the map .
A map which is created entirely offline is usually done by collecting data of a given road several times .
This method of map creation , while producing very detailed and accurate maps , is unable to react or adapt to a changing environment .
The third method of creating detailed roadmaps is to create them offline and then update them online with new , relevant information .
This method of map creation takes advantage of both methods , creating a highly accurate roadmap which can be updated while driving .
In course four on motion planning , we will present a method for storing all of the information present in a detailed roadmap called the lane length model . Let 's look at an example of a detailed roadmap .
As you can see , the lane boundaries of the detailed roadmap are visualized in red Along with the boundaries , the center of each lane is also visualized in red .
This information is vitally important for path-following as it provides a default path along the lane .
As you can see in this video the vehicle , while autonomously driving , neatly follows the center of the lane . That completes our discussion of mapping for self-driving cars .
You 'll study each of these map types further as we dive into localization , collision avoidance , and motion planning In the remaining courses of the specialization .
Congratulations , you completed the second module in this introduction to self-driving cars course .
In this module , you learned how to select sensing and computing hardware in self-driving car , how to design specific sensors based on the requirements of driving .
How to decompose the software system for autonomous driving .
And what the three main types of maps are that represent the environment .
In the next module , we will take a closer look at the vehicle modeling for the purpose of precision control .
Welcome back .
In the last video , we discussed the basics of kinematic modeling and constraints and introduced the notion of the instantaneous center of rotation .
In this lesson , we will develop the kinematic bicycle model , a classic model that does surprisingly well at capturing vehicle motion in normal driving conditions .
Let 's get started . The well-known kinematic bicycle model has long been used as a suitable control-oriented model for representing vehicles because of its simplicity and adherence to the nonholonomic constraints of a car .
Before we derive the model , let 's define some additional variables on top of the ones we used for the two-wheeled robot .
The bicycle model we 'll develop is called the front wheel steering model , as the front wheel orientation can be controlled relative to the heading of the vehicle .
Once again , we assume the vehicle operates on a 2D plane denoted by the inertial frame FI .
In the proposed bicycle model , the front wheel represents the front right and left wheels of the car , and the rear wheel represents the rear right and left wheels of the car .
To analyze the kinematics of the bicycle model , we must select a reference point X , Y on the vehicle which can be placed at the center of the rear axle , the center of the front axle , or at the center of gravity or cg .
The selection of the reference point changes the kinematic equations that result , which in turn change the controller designs that we 'll use .
As needed , we 'll switch between reference points throughout this course . Let 's start with the rear axle reference point model .
We 'll denote the location of the rear axle reference point as xr , yr and the heading of the bicycle as Theta .
We 'll use L for the length of the bicycle , measured between the two wheel axes .
As with the two-wheeled robot , these are our main model states . The inputs for the bicycle model are slightly different than those for the two-wheeled robot , as we now need to define a steering angle for the front wheel .
Let this steering angle be denoted by Delta , and is measured relative to the forward direction of the bicycle .
The velocity is denoted v and points in the same direction as each wheel .
This is an assumption referred to as the no slip condition , which requires that our wheel can not move laterally or slip longitudinally either .
It is the same assumption that allows us to compute the forward speed of the two-wheeled robot based on the rotation rates of its wheels .
Because of the no slip condition , we once again have that Omega , the rotation rate of the bicycle , is equal to the velocity over the instantaneous center of rotation , radius R.
From the similar triangles formed by L and R , and v and Delta , we see that the tan of Delta is equal to the wheelbase L over the instantaneous turn radius R.
By combining both equations , we can find the relation between the rotation rate of the vehicle Omega , and the steering angle Delta , as Omega equals v tan Delta over L. We can now form the complete kinematic bicycle model for the rear axle reference point .
Based on this model configuration , the velocity components of the reference point in the x and y direction are equal to the forward velocity v times cos Theta and sine Theta respectively .
The bicycle kinematic model can be reformulated when the center of the front axle is taken as the reference point x , y.
This is a good exercise to try yourself to practice applying the principles of instantaneous center of rotation and follow the rear axle derivation quite closely .
The last scenario is when the desired point is placed at the center of gravity or center of mass as shown in the right-hand figure .
Because of the no slip constraints we enforce on the front and rear wheels , the direction of motion at the cg is slightly different from the forward velocity direction in either wheel and from the heading of the bicycle .
This difference is called the slip angle or side slip angle , which we 'll refer to as Beta , and is measured as the angular difference between the velocity at the cg and the heading of the bicycle .
This definition of side slip angle will also apply when we move to dynamic modeling of vehicles , where it can become more pronounced .
The kinematic model with the reference point at the cg can be derived similarly to both the rear and forward axle reference point models .
We end up with the following formulation , which we 'll use as the basis for our modeling of the dynamics of vehicles as well .
Lastly , because of the no slip condition , we can compute the slip angle from the geometry of our bicycle model .
Given LR , the distance from the rear wheel to the cg , the slip angle Beta is equal to the ratio of LR over L times tan Delta . Finally , it is not usually possible to instantaneously change the steering angle of a vehicle from one extreme of its range to another , as is currently possible with our kinematic model .
Instead , our kinematic models can be formulated with four states : x , y , Theta , and the steering angle Delta .
If we assume we can only control the rate of change of the steering angle Phi , we can simply extend our model to include Delta as a state and use the steering rate Phi as our modified input .
Our kinematic bicycle model is now complete .
Once again , we 'll use a state-based representation of the model for control purposes later in this course and throughout the second course on state estimation as well .
Our kinematic bicycle model takes as inputs the velocity and the steering rate Phi .
The state of the system , including the positions XC , YC , the orientation Theta , and the steering angle Delta , evolve according to our kinematic equations from the model , which satisfy the no slip condition .
We can now use this model to design kinematic steering controllers as we 'll see in a later module in this course .
To summarize this video , we formulated the kinematic model of a bicycle for three different reference points on that vehicle and Introduced the concept of slip angle .
We 'll use this kinematic bicycle model throughout the next two modules for designing of controllers for self-driving cars .
In the next video , we 'll learn about how to develop dynamic vehicle models for any moving system .
Because the industry is so new , we at so many different challenges , we have to find so many different ways of solving problems that we 've never had to solve before .
There has been a phenomenal amount of research in Academia and to how we get these cars on the road in the first place , like how do we detect pedestrians , how do we plan trajectories for example .
The problem that industry needs to solve is how do we deliver this experience to thousands of customers , tens of thousands of riders , maybe someday even millions of riders .
Part of this problem is how do we scale up our systems , how do we deliver the same algorithms on maps out or the size of San Francisco , the Bay Area , eventually the continental US .
For example , the financial district of San Francisco with all of the tall buildings there .
There 's a really big question , the circling around in the AV world now , and that 's how do we say strong things about the safety of systems that are unbelievably complicated ?
So , then you could say , " Well , maybe we should just build amazing simulators of it .
So , maybe we should if we could build simulators , then we could run 10,000 copies of the autonomous vehicles simultaneously and get millions of miles overnight . " Well , that 's very compelling as well .
But as yet , we do n't have an ability to really simulate the detail that comes in on some of these sensors .
So , it 's very hard for example , to simulate a full scanning radar because of the complexity of those interactions .
Welcome back . In the previous video , we briefly reviewed the design of PID controllers and classical controller design .
In this video , we 'll apply PID control to our longitudinal vehicle model .
So , by the end of this video , you 'll be able to ; define the full vehicle planning and control architecture and design a PID-based controller for regulating to a set reference speed as in cruise control .
Let 's take a closer look at the vehicle control architecture and how it fits into the overall autonomy software stack .
We can divide the structure into four sections .
The first section is the perception of the road and the environment .
This perception is captured by sensors and generates the input references for our system .
In the second layer , we have both path generation and speed profile generation , which in automotive circles is referred to as the drive cycle .
These profiles are generated through the motion planning process , which is the focus of the fourth course of this specialization .
The path and the speed profiles are the reference inputs needed by our controllers .
For longitudinal control , define the set point 's , acceleration and deceleration that we 'd like to be able to track precisely .
For both the lateral and longitudinal control of an autonomous vehicle , the only task that needs to be performed is to follow the plan as precisely as possible , and thereby minimize the error between the actual and reference path and speed .
All other tasks required for autonomous driving or done by other parts of the system .
As we 've seen in the previous module , these include the steering for the lateral control and the throttle and break commands for longitudinal control .
Let 's look at an example of longitudinal vehicle control . One of the most well-known and commonly available control applications in automotive control is cruise control operating at highway speeds .
These extended examples require additional controller designed to handle the wider range of operating points .
This block diagram shows the cruise controller and plant vehicle model as a closed loop system designed to keep the vehicle velocity close to the reference velocity .
The controller can be split into two levels ; a high level and a low level controller .
Although the low level controller is not essential to the control task . The high level controller takes the difference between the set point velocity and the vehicle actual velocity , and generates the desired vehicle acceleration to close the gap .
It also allows us to separate the use of engine maps we studied in the previous module for generating a desired torque given the engine state from the cruise control input response .
Let 's apply a PID controller here , which is expressed in the continuous time domain .
The input to the high level controller is the velocity error , and the output is the vehicle 's desired acceleration .
In the previous lesson , we learned how to design a PID controller and studied how the different gains affect performance of the controller .
To implement such a controller in software , we discretize the controller , changing the integral to a summation over a fixed length time steps .
The derivative term can be approximated with the finite difference over a fixed time step if either the reference acceleration or the estimated vehicle acceleration is not available .
The low-level controller generates the throttle and breaking signals to follow the desired acceleration calculated by the high-level controller .
In designing a low-level controller , we make some assumptions to simplify our problem .
We assume that only throttle is needed to manage the speed of the vehicle during cruise control , and that the driver will take over if breaking is required to avoid an incident .
We assume that we are operating in gear three or higher such that the torque converter is locked , meaning that torque from the engine passes directly through the transmission without loss , and we assume that the tire slip angle and ratio are negligible as cruise control motions are typically gentle .
The low-level controllers seeks to generate the desired acceleration from the high level controller by increasing or decreasing the torque produced by the engine .
This is controlled by the throttle angle , but is governed by the power train dynamics and the engine map , making it a nonlinear problem that can be a challenge for classic control methods .
Instead , the desired acceleration is translated to a torque demand , and the torque demand is then converted to a throttle angle command .
Recall from the previous module that we developed a second-order ordinary differential equation to describe the acceleration of the vehicle in terms of the difference between the engine torque and the load torque .
We can rearrange this equation to solve for the desired engine torque , given known load torques and the desired acceleration of the vehicle .
Then , the steady-state engine map , which is generated in testing the engine at different operating points can be used to determine the throttle angle needed to produce the amount of torque demand required .
In these standard maps , the desired engine torque and the current engine RPM define the required throttle position , and can be interpolated if needed .
This approach is a data-driven approximation , but it works quite well in practice .
The approximation comes from the fact that the data points in the map are steady-state points while the power train is continuously changing its operating point to meet the current driving conditions .
Finally , we can put the pieces of our vehicle controller together and simulate the control response to a step change in desired speed of our dynamic vehicle models with PID controllers .
The PID gains are tuned by trial and error so that the vehicle speeds follow the reference velocity of 30 meters per second or a 108 kilometers per hour .
In the results plot , on the left , we see the throttle opening as a percentage , which is the commanded throttle for the vehicle .
On the right , we see how the actual velocity evolves over time , and reaches the reference velocity after a settling time .
Because of the engine map non-linearity , we see some interesting artifacts in the vehicle response as it closes in on the reference speed .
You 'll see even more interesting effects in the simulated vehicles in Carla during your final assessment for this course , with gear changes causing big challenges for pure PID control .
In this video , we covered the concept of longitudinal speed control for a cruise control system .
This is the first case in designing a controller for the autonomous car to follow a desired forward speed in this course .
We learned the concepts of high-level and low-level controllers and use the combination of PID control and engine maps to regulate vehicle speed .
In the next video , you will learn how to apply feed-forward control to improve the performance of a longitudinal control system .
In the last video , we saw how to build a feedback controller for the longitudinal speed tracking problem that used PID control to generate acceleration commands together , with a low level controller to define throttle and brake inputs . In this final video of the module , we will modify our control architecture to incorporate feedforward commands , which will improve tracking performance , particularly in dynamic maneuvers .
Specifically , we will see how to integrate both feedforward and feedback control into a combined control architecture .
The feedback block diagram shows the typical closed loop structure , where the current output is compared to a reference signal .
And the error between the two is fed into the feedback controller , which generates the input to the plants .
The feedforward block diagram shows an open loop structure , where the reference signal is directly fed into the feedforward controller , which again , generates the inputs to the plant .
Feedforward controllers create their plant inputs by modeling the plant process , as we have done in module three of these course , and applying the appropriate inputs directly .
In many applications , feedforward and feedback loops are combined to improve controller performance .
This block diagram shows how a typical feedback , feedforward control structure works .
You can think of feedforward control as providing the necessary inputs expected to keep the plant tracking its reference signal , and the feedback controller correcting for errors that result from either disturbances or inaccuracies in the plant model used by the feedforward controller .
The input to the plant is simply the addition of the feedforward and the feedback inputs .
To summarize this concept , the main reason that both feedback and feedforward controllers are used to control a system are feedforward controllers provide a predictive response as they produce a reference output to achieve a particular tracking response , particularly when the required inputs are non-zero .
Feedback controllers provide a reactive response , which eliminates control errors due to the disturbances as they occur .
The combination of feedback and feedforward control is widely used because of this complementary relationship .
Now let 's take a look at the combined feedback and feedforward controllers to generate vehicle actuation for longitudinal speed control .
The reference speed or drive cycle is defined by a higher level planner .
And it is desirable that the vehicle follows the reference velocity precisely .
The reference velocity is the input to the feedforward block , and the velocity error is the input to the feedback or PID control block .
Note that there is no low-level controller included in this block diagram , as we had in the pure PID feedback control from the previous video .
The role of the low-level controller achieving the desired acceleration through the use of a mapping from accelerations to engine commands is now going to be handled by the feedforward block .
The feedforward block gets only the reference signal as input , and its primary objective is to accurately set the inputs of the plan .
To do this we can convert the entire longitudinal dynamics model into a fixed lookup table or reference map , that maps the reference velocity to the corresponding actuators signals assuming the vehicle is at steady state .
This feedforward approach works well at steady state , but ignores the internal dynamics of the vehicle powertrain .
And must also rely on the current vehicle state estimate to resolve some of the forces and dynamic models used .
In our example , we are interested in following the reference velocity at each time step . Based on the kinematic relationship between the vehicle 's speed and the wheel angular speed , we can calculate the wheel angular speed needed .
The wheel angular speed is related to the engine angular speed , or engine RPM , through the gear ratios from the transmission , differential and final drive .
So we can now compute the engine RPM corresponding to the required wheel angular speed all through the kinematic relationships defined in the modeling module . Then assuming steady state operation , the dynamics of the powertrain says that the engine torque must be equal to the total load torque acting on the vehicle .
The source of the load torque is aerodynamic resistance , rolling resistance and vehicle gravitational resistance .
We can compute the combined load torque using the current state of the vehicle , including its current speed , and the road slope .
We now have a required engine torque and can combine that with the current engine operating speed in RPM to define the throttle position needed to generate the required torque .
Once again , the engine map is defined for discrete steady state values of engine torque and RPM . And is interpolated as needed , based on the current vehicle operating point .
Let 's have a look at the comparison between PID control method described in the previous video and the combined feedforward , feedback method we 've discussed in this video .
We 've used the same simulation parameters as in the previous video , including the engine map and dynamic model elements .
The key difference between the two responses is visible as the reference speed changes .
The feedforward tracking is still not perfect , however , as the vehicle response is ultimately governed by its inertia , and the feedforward approach we 've presented relies on steady state modeling of the car .
As the feedforward model becomes more precise , the feedback components can focus purely on disturbance rejection , and speed profile tracking can be done with consistent precision .
In this video , we 've covered the concept of feedforward controllers and the integrated feedback in feedforward controller to enhance the performance of reference tracking .
Congratulations , you 've now reached the end of this module on longitudinal control for self driving cars . You 've reviewed the concepts of classical control and defined the PID controller .
You 've built a PID controller for longitudinal control of a car and you 've applied feedforwad control to improve reference speed tracking .
In the next module , we 'll dive into lateral vehicle control design to guide our vehicle along a reference path . See you there .
Welcome to the final module of the first course in our self-driving cars specialization .
In this module , you 'll get the chance to bring together and apply the concepts we 've discussed throughout this course and test them in simulation . We 'll start by discussing various self-driving car simulation environments .
A realistic simulation environment is an essential tool for developing a self-driving car , because it allows us to ensure that our vehicle will operate safely before we even step foot in it .
Using a simulator , we can test all of the different modules that make up our system including perception , planning , and control , either together or independently .
We can run sophisticated scenarios involving many AI controlled vehicles and pedestrians , and we can run variations on these scenarios , hundreds or even thousands of times to ensure our car consistently makes the correct decision .
Most importantly , we can test our car in situations that would be too dangerous for us to test on actual roads .
There are a wide range of simulators available , developed by teams from industry and academia alike .
For this course , we 'll be using the simulator called Carla .
Carla is a simulator developed by a team with members from the Computer Vision Center at the Autonomous University of Barcelona , Intel and the Toyota Research Institute and built using the Unreal game engine .
It features highly detailed virtual worlds with roadways , buildings , weather , and vehicle and pedestrian agents .
Images of these environments can be captured in various formats including depth maps and segmented images which you 'll learn more about in the third course of this specialization .
The entire simulation can be controlled with an external client which can be used to send commands to the vehicle , record data and automatically execute scenarios for evaluating the performance of your car . Best of all , Carla is open source .
So , anyone is free to modify any aspect of the code in order to meet their particular simulation requirements .
For this specialization , we 've developed a customized version of Carla with some extra tools to help you implement and test your code and simulation .
Let 's take a look at how you might interact with and use the simulator .
There 's a detailed guide about how to set up Carla and run the Python clients in the assessment instructions . We 've already downloaded and installed everything .
The easiest way to start Carla is to use the launch script provided .
The script itself is used to load the Carla session with a map or scenario of our choosing .
Here , I choose the town environment provided in Carla for our demonstration .
We can also change various configurations for our simulator session , such as the simulation window size and setting a fixed time step to be either small or large .
A Carla session can also be loaded in server mode .
This allows a programmable client to connect to the server , and send commands to control the car or receive information about the simulation environment .
Being able to program our client opens up a diverse range of possibilities .
From the simulator feedback , we can know where every car is on the map , develop a controller and planner to smoothly navigate our car around or even use depth and segmented images to learn to detect cars and pedestrians .
Simulation results from the client can also be post-processed to evaluate the performance of our current software algorithms and methods .
This provides insight on improvements and limitations to both the simulator and algorithms .
For our upcoming course project , we use these results as a way to evaluate your algorithm 's performance and to encourage exploration of the strengths , as well as the weaknesses of the algorithms you are developing .
We 've talked about why simulation is useful and what you can use it for , and we 've met Carla , the simulator that we 'll use throughout this specialization .
We 've also seen some of the capabilities of Carla which you 'll use in the upcoming course project .
Now , before we move on to the details of the project , it 's time for you to take a closer look at the simulator itself and to set it up on your own computer .
Now 's the time to follow those detailed instructions that will take you through the full process .
I also encourage you to consult the discussion forums if you run into any problems , chances are someone 's had the same issue as you before .
Once you have the simulator up and running and you 're comfortable launching and running a client , you 'll be ready to start using it for the course project .
In the next video , we 'll go over the final project requirements and get you ready to start designing controllers for your simulated self-driving vehicle .
You 've now seen some of the capabilities of Carla and hopefully you 've had a chance to download the simulator and start experimenting with it yourself .
In this project , you 'll implement a simple controller in Python and use it to drive a car around a track in Carla .
You will be given a sorted list of waypoints which are equally spaced on this track .
The waypoints include their positions as well as the speed the vehicles should attain .
As a result , the waypoints become the reference signal for your controller and navigating to all the waypoints effectively completes the full track .
Since the controller reference contains both position and speed , you will need to implement both longitudinal and lateral control .
You may want to refer back to the modules on longitudinal control and lateral control before starting the project .
The throttle and brake will come from your longitudinal speed control and the steering will come from your lateral control .
Make sure to start with a simple controller design as possible and only add complexity if your vehicle does not track the path as expected .
Of course , once you 've got a working controller , do n't hesitate to push its limits and see what it can really do on the race track .
So , how are you going to want to structure your code for this project ?
We 've prepared a starting script for you so that you do n't need to worry about any of the Carla implementation details and you can just focus on programming the controller itself .
If you open the simulator directory and navigate to the Course one final project folder , you 'll see a file named controller2d.py .
When you open this file , you 'll see the vehicle controller implemented as a Python class .
This class contains all the information relevant to implementing the controller .
The vehicle state , desired waypoint , desired speed , and controller outputs are stored in variables ready to be used .
The class also contains functions which will interface with Carla directly .
These functions will continually update the vehicle state and send the controller outputs to Carla , allowing you to focus your efforts purely on the controller implementation .
Now , you might be wondering how your code is going to be evaluated and how you 'll earn a grade for this project .
The performance of your controller will be graded based on this trajectory .
There 's a greater script that the Coursera platform we 'll use to check your code .
Each waypoint has a distance and a speed threshold which are shown in green .
The grading script will tell you how many way points you have successfully reached and if you reach more than half , you 'll pass the assessment .
Once your trajectory passes the grading script , you can upload the trajectory text file onto Coursera for official completion .
You now have everything you need in order to implement your very first controller and test it in simulation .
If you have any questions that I did n't answer in this video , there are further instructions in the readings for this module and you can always ask in the discussion forums as well .
I hope you have fun with this final project and I 'll see you again once that 's completed to close out the course .
Good luck .
But it had all the components and elements of its predecessors , as well as a grandeur and monumentality .
If so , we perceive an interrelationship among them , such as the diagonal line at the southwest corners .
It is not , however , linked to the constellation Orion . The Re element in the names of the later two , Khafre and Menkaure , clearly relates to that solo god , Re , as does the Sphinx .
They were to continue in use for Egyptian burials until well into the 17th Dynasty , and then later in private contexts , and then eventually , outside Egypt 's borders in the south , at Nubia .
As for the grand heights , scale , effect and statement , no followers equal them . For example , in the Fifth Dynasty , we have the Pyramid of Unas .
And in the Middle Kingdom , we have the Pyramid of Amenemhat I , Senwosret . And then in the Second Intermediate Period , there is n't too much left , but we do have Khendjer , the last of the pyramids that occurs in the 17th Dynasty .
For the New Kingdom , we have a ritual pyramid of Ahmose at Abydos . Fact 13 , pharaohs and their enterprising architects became aware early on that these tombs were being robbed and defiled rapidly .
And despite all of the efforts of the architects , the protective measures that were taken in all cases were ineffective .
These monuments had , in essence , become a giant sign with the message , this way to the gold .
Those intent on finding the treasures bypassed the portcullis stones that were supposed to seal the burial chamber . They found hidden entrances , they discovered concealed chambers as an alternate defense .
Some architects chose naturally raised areas , symbolic pyramids , to replace the built structures , and then they constructed burials within the high hills , near the raised cliffs in the mountains . This development , for the most part , occurred in the south , where the hilly terrain was .
In chronological order , they are , the First Intermediate Period at a site called Moala , and a ruler there by the name of Ankhtifi .
In Dynasty 11 at Thebes , we have Mentuhotep II , who built his monument right into a large mountain that was pyramid-shaped .
In Dynasty 12 , at Abydos , there is also a tomb built into Anubis Mountain which has the same kind of raised look .
This last discovery was made by my colleague and Penn archaeologist , Joe Wegner at Abydos .
You should be able to generate the same right answers that are shown in the video as you go along .
Every core concept in a video has a corresponding Excel spreadsheet where that concept is implemented in a practical way .
If possible watch each video with the relevant Excel spreadsheet open at the same time , and see if you can get the same answers as I do for yourself simply using the Excel template and entering some additional data or functions .
If you read the Excel functions you will see that they implement exactly the formulas in the videos . Of course , you may also need to write some additional functions in Excel or move cells around , add cells , etc. , in order to answer the more complicated quiz questions .
If you already have all the basic Excel knowledge described below , you can skip some or all of this weeks lesson and go straight to the quiz .
It would be best not to skip the videos unless you 're already familiar with defining cell locations , the use of relative and absolute references and formulas , Excel 's auto-fill feature , formatting cells for numbers , currency , and percentages .
Charting results using scatter plots , proper syntax , and order of operations for arithmetic in Excel .
Common functions on a single number , such as functions using exponents , logs , pi , and random numbers .
Getting the right answer , but using a different tool than Excel , is not recommended .
Because , later on you will need to be able to read Excel functions in order to benefit from the Excel spread sheet examples and data sets that supplement each video and are designed to make it easier to understand the more advanced course material .
Of course , if you pass the quiz now and realize later that you 'd like to review an Excel skill , you can always come back and watch the appropriate video later .
All demonstrations of problems and solutions in this course are done with Microsoft Excel for Mac Version 14.5.4 .
So , things may look quite different on your computer than they do in the videos .
However , it 's easy to get help .
Check the Excel version number you have and look up online how to use the same functionality in your version .
All the problems for this course should be solvable by any version of Excel , so long as it includes the Solver plug-in .
To the best of my knowledge that includes all versions of Excel released since 2007 .
You will not need the Data Analysis Tool Pack , which does not work with any Mac versions of Excel before Excel 2016 .
After taking this course , you will be able to apply advanced data exploration methods , models , and information theory concepts and other approaches to evaluating model effectiveness .
You will be able to quantify the uncertainty in a situation before you analyse the available data , the uncertainty afterwards , and the reduction in uncertainty or information gained , achieved by the combination of the data plus your model .
Our goal is to make all the new ideas introduced in this course , as accessible as possible .
We do this by working through examples of each type of problem including full sample answers in Excel , rather than using more so-called advanced tools , such as R or MATLAB .
Excel is already the world 's most widely used , and it 's one of the easiest-to-master business data processing tools .
After this course , you will have applied many different Excel templates to many different practical problems .
My students report that they can often reuse these templates later , with only minor modifications on real world problems . Of course , if you prefer to solve course problems in R or MATLAB or some other tool , you wo n't have the advantage of the Excel spreadsheets I 've created for you , but that is of course allowed .
Use the course in the way that works best for you , and good luck .
This brings us to the end of the fifth module and also , to the end of this course on linear algebra for machine learning .
We 've covered a lot of ground in the past five modules , but I hope that we 've managed to balance , the speed with the level of detail to ensure that you 've stayed with us throughout .
There is a tension at the heart of mathematics teaching in the computer age .
However , computers now do nearly all of the calculation work for us , and it 's not typical for the methods appropriate to hand calculation to be the same as those employed by a computer .
This can mean that , despite doing lots of work , students can come away from a classical education missing both the detailed view of the computational methods , but also the high level view of what each method is really doing .
The concepts that you 've been exposed to over the last five modules cover the core of linear algebra .
That you will need as you progress your study of machine learning . And we hope that at the very least , when you get stuck in the future , you 'll know the appropriate language .
So that you can quickly look up some help when you need it .
Which , after all , is the most important skill of a professional coder .
Hello and welcome to machine learning with Python . In this course , you 'll learn how machine learning is used in many key fields and industries .
For example , in the healthcare industry , data scientists use machine learning to predict whether a human cell that is believed to be at risk of developing cancer is either benign or malignant .
As such , machine learning can play a key role in determining a person 's health and welfare .
You 'll also learn about the value of decision trees , and how building a good decision tree from historical data helps doctors to prescribe the proper medicine for each of their patients .
You 'll learn how bankers use machine learning to make decisions on whether to approve loan applications .
You will learn how to use machine learning to do bank customer segmentation , where it is not usually easy to run for huge volumes of varied data .
In this course , you 'll see how machine learning helps websites such as YouTube , Amazon , or Netflix develop recommendations to their customers about various products or services such as which movies they might be interested in going to see or which books to buy .
There is so much that you can do with machine learning . Here , you 'll learn how to use popular Python libraries to build your model .
For example given an automobile dataset , we can use the scikit-learn library to estimate the CO2 emission of cars using their engine size or cylinders . We can even predict what the CO2 emissions will be for a car that has n't even been produced yet .
All you have to do is click a button to start the lab environment in your browser .
The code for the samples is already written using Python language in Jupiter notebooks , and you can run it to see the results or change it to understand the algorithms better .
So , what will you be able to achieve by taking this course ?
Well , by putting in just a few hours a week over the next few weeks , you 'll get new skills to add to your resume such as regression , classification , clustering , scikit-learn , and scipy .
You 'll also get new projects that you can add to your portfolio including cancer detection , predicting economic trends , predicting customer churn , recommendation engines , and many more .
You 'll also get a certificate in machine learning to prove your competency and share it anywhere you like online or offline such as LinkedIn profiles and social media . So , let 's get started .
Imagine that we are a company issuing credit cards and over time we 've evaluated which of our customers ultimately paid off all the money they owed us plus interest and which fell behind and ultimately failed to pay us some money and defaulted .
And we 're interested in the future in selecting customers or agreeing to issue cards to customers who will not default .
We have a number of input variables that may be able to help us model the probability of default or not .
Including the potential customer 's age , how many years they 've worked at their current job , how many years they 've lived at their current address , their income level .
How much money they owe on other credit cards and automobile debt and so on .
So the way that this works is that we allow Excel to find the best fit line that assigns betas to each of these variables .
And if the variable adds no additional information , this may be because it has a property known as collinearity , meaning that it could be just another way of saying the same information .
We also can have partial collinearity , so in this circumstance we have here , we have a debt-to-income ratio , and that 's going to have high correlation with our debt inputs and our income inputs , I would expect .
The linest function has a number of inputs .
Then we put in the full array that represents all of our input variables from their upper left-hand corner to their lower right-hand corner .
So , we put in the first of the first variables , and then the last or bottom of the far right variables .
Then we select true , meaning that we want the true value of alpha , and we select true again , meaning that we want to have additional descriptive statistics provided .
We need to select a region , one , two , three , four , five by , Five by eight , which will give us the statistics and other inputs that we need .
I 've created a little chart below for what each of these things are .
You 'll notice that it gives us our beta values in reverse order .
It would be useful to format the cells so that if they 're very , very close to 0 and Excel is actually trying to give us a 0 value , that becomes clear .
And you 'll notice that this whole group of betas along with their errors are set at 0 .
This means that these have high collinearity and they do n't contribute anything to our model , okay . So , what you 'll see is , most of our model consists of a couple key variables one , two , three , four , which by far the most important is the final one which is our debt-to-income ratio , I believe .
And when you do the actual problem , you wo n't be matching to zeroes and ones , you 'll have a more realistic situation where the y variables can be any number .
And they represent the ultimate profit or losses associated with this particular customer .
But I did n't want to do that here , because I want to leave something for you to do on your own
Welcome to Managing Big Data with My SQL . This is the fourth course in a specialization called Excel to My SQL Analytic Techniques for Business .
My name is Dr. Jana Schaich Borg and I 'm a neuroscientist at Duke University .
I am very excited to introduce you through this course to one of the most fundamental skill sets of the big data world .
Retrieving your data from a database so you can analyze it to create actionable recommendations for your business .
Over the next few weeks you will learn this skill set through a combination of videos and exercises with real life business data sets .
Each week I will introduce you to the main topics of the week through slides and short videos , and then you 'll practice applying those concepts through activities and exercises .
The activities will range from making database diagrams to help you understand how databases are organized , to actually writing and executing your own code through online interfaces connected to our business databases .
The quizzes at the end of each week will based on both the videos and the activities , so make sure you keep an eye out for all the different types of resources in the content section of the course website .
I 'd like to give you a road map of exactly what skills you 'll be learning over the next few weeks . Before I do , we have a very important question to answer .
Sequel .
The most common pronunciations are either SQL or Sequel , but there is truly no agreement about which pronunciation is more accepted .
A blogger named Patrick Gillespie amusingly emailed a man named Don Chamberlin to ask about the official pronunciation of the language 's name .
Don Chamberlin co authored SQL or sequel with Raymond Boyce at IBM in the 1970s .
So if anybody should know what the language is called , it 's Don .
Don responded with , Hi Pat . Since the language was originally named Sequel , many people continued to pronounce the name that way after it was shortened to S-Q-L .
Both pronunciations are widely used and recognized .
As for which is more official , I guess the authority would be the ISO standard , which is spelled and presumably pronounced SQL .
Thanks for your interest , Don Chamberlin . So even the co author of the language ca n't say with confidence how the name of the language is pronounced .
To make things more confusion some of the most popular database systems in the world such as Oracle say explicitly in their documentation that the language is pronounced See-Quill .
That means you can use whichever pronunciation you like better .
Personally I like using both pronunciations , sometimes even in the same sentence .
So why should we care about SQL or Sequel ?
Companies have to keep track of a lot of data , and the only practical way to do that is to store the data is some type of database .
If you are becoming a data analyst for the business world , somehow you are going to have to figure out how to get data out of these databases in order to do any kind of analysis .
This course is designed to help you learn how to do that .
In many technology driven companies , the only way to retrieve data from a database is to write your own SQL queries , which are structured lines of computer code that extract data .
In other larger and more traditional companies there are people in the company who could extract the data for you , but you have to put in a formal request for them to write and execute and query .
And it could take days or weeks for them to fill your request especially if many other people are asking for queries too .
For this reason , companies are very eager to hire analysts who can not only analyze data but who know how to get the data in the first place .
As some evidence of this , academic researchers of the Teradata University Network were in a study to determine the state of business intelligence and analytics .
They asked over 400 recruiters from technical companies to answer the question .
When I recruit for business intelligence , business analyst roles , it is important that the students have the following coursework or knowledge .
Then the recruiters had to rank a bunch of possible responses .
The number 2 answer companies gave , was SQL and Query skills .
Therefore , according to this survey , the ability to pull data out of a database is even more important than analytical skills for getting a business analyst job .
We are dedicating an entire course in the specialization to helping you acquire this ability .
By the way , if you 're curious about what the number 1 response companies gave in the survey was , it was very interestingly , Communication Skills .
That 's why we dedicated course three of this specialization to Data Visualization and Communication .
Relational databases are the gold standard for data bases that store highly organized and structured business data . Almost every single company in the world has at least one relational database .
There are also new classes of databases specifically designed for data that are collected in an extremely fast rate , such as GPS or biosensory data , or unstructured data that does not fit well into highly prescribed formats , such as tweets or texts .
However , although these databases are certainly likely to gain popularity in the future , right now they only represent a very small fraction of the database market .
As you can see from this graph produced by the International Data Corporation , a global market intelligence firm , annual sales from the current database market are over $ 40 billion , and are expected to top $ 50 billion in the next few years .
The green in the bars on this chart illustrates that almost all of these billions of dollars are spent on relational database technologies .
Therefore , although there are definitely exciting things to learn and know about other databases , we chose to focus on how to manage big corporate data using relational databases in this specialization .
There are many relational database platforms including Oracle , Microsoft , SQL Server , DB2 , SQLite , Microsoft Access , and PostgreSQL .
We chose to focus on MySQL .
To help explain why , I 'd like to introduce you to Ryan Luecke , senior software engineer at Box .
>> Hi , my name is Ryan Luecke and I work at Box as a senior software engineer on our caching infrastructure team , which is closely related to our database engineering team .
Box is an online file sharing and collaboration service for businesses .
Box lets multiple people across the country or across the world work on documents at the same time and store them in a safe place that is accessible to anyone .
A good reason to highlight Box in this course is that Box 's entire interface is based on relational database concepts .
Every time you store a new document in a folder in your Box account , it 's like a new row in a table . Every time you add a new collaborator to a document it 's like adding a new relationship between tables .
So in Ryan 's words , >> Really , then what Box is , is a very pretty interface to get to all of the underlying database operations .
>> This means that people who work at Box have a really deep understanding of what relational databases do , because their entire product is based on making sure the relational database works quickly and perfectly .
And importantly for this course , Box uses MySQL as its primary database .
Here is Ryan 's explanation for their choice .
There are a lot of relational databases and non-relational databases to choose from .
And one reason we chose MySQL was it 's reliable , it 's mature , it 's open source , and well understood .
There 's a lot of industry and community around MySQL . There are conferences around MySQL , and a lot of tools for it .
And it also meets our relational needs , so it 's able to do what we need it to do to store data for millions of users , and billions of pieces of content .
>> But box is by no means the only well known company to use MySQL .
Here are some other companies that rely heavily on MySQL databases .
>> So Facebook uses MySQL , YouTube within Google . Yelp uses MySQL , Dropbox .
GitHub which stores a lot of developer files and projects , LinkedIn , Etsy online store , Send grid , Twitter , Booking.com , Square , Pintrest , Yahoo , and lots of others .
Given that MySQL is open source , free for anybody to use , well understood , and used by so many companies you might want to work for , we thought it was an important database with which to give you some experience .
Each database system does use its own slightly different version of SQL language , so you will have to be prepared to look for the differences if you start working with another type of database .
Writing SQL queries is kind of like driving a car .
Every time you use a different car , you might have to spend some time looking for where the buttons and adjustment knobs are .
But if you know how to drive one car , you know how to drive all of them , as long as they have the same transmission type , that is .
Once you know how to use SQL to interact with one database , you know how to use SQL to interact with all relational databases .
You are going to get a sense of how small or large the differences in SQL languages are in this course because we decided to give you access to another type of relational database in addition to a MySQL database .
The second relational database it a Teradata database .
We chose Teradata because it is the leader in what is called data warehousing .
A data warehouse is a copy of historical and current company data that is structured explicitly for querying and analytical reporting .
Their warehouses are commonly used for things like company dashboards that allow executives to make decisions based on real time data and graphs that show how those data relate to trends from the past .
Well known companies which store petabytes of data in Teradata bases include Walmart , Sam 's Club , Verizon , AT&T , Bank of America , and Apple .
Having experience with both a MySQL and a Teradata database will give you confidence that you can navigate different database systems and will provide you with a strong competitive edge in the data analytics marketplace .
Here 's what we will cover in the next few weeks . By the end of this course , you will be able to describe the structure of relational databases , interpret and create entity relationship diagrams and relational schemas that describe the contents of specific databases .
Write queries that retrieve and sort data that meet specific criteria , and retrieve such data from MySQL and Teradata databases that contain over one million rows of data .
Execute practices that limit the impact of your queries on other coworkers .
Combine and manipulate data from multiple tables , across a database .
Retrieve records and compute calculations that are dependent on dynamic data features .
Now long from now you will have a highly coveted skill set that will make you very attractive to data analyst recruiters .
You will also have a powerful set of tools to help you provide real tangible value to your business .
I 'm very excited to learn about all the doors that will open for you as you embrace this important feature of the data driven business world .
Well done . You 've now learned the tools you need to run segmentation analyses that incorporate data from multiple tables at once .
To motivate you , here 's a reward in the form of a little story , for all the hard work you 've done this week .
In one corner of the bar are two tables .
The very polite sequel query walks up to the tables and asks , mind if I join you ?
So as we 've seen throughout this module , there are quite a few different programs of secular mindfulness intervention designed to address different clinical , therapeutic or positive psychological needs .
They each have their own distinctive characteristics and focus on their own particular population .
But , they also share some foundational common principles .
We might label them as first , recognizing that our everyday experiences are made up of numerous experiential components .
Second , recognize that we can change the emotional force of an experience by controlling where we place our attention .
Third , recognizing the beneficial possibilities of a de-centred , metacognitive standpoint from which to act . We 're going to look at each of these in turn today .
So the first of these principles concerns the nature of experience itself , that is , in our everyday lives we tend to encounter experiences as though their full complexity and multi-dimensionality are basic to them .
In particular , we accept as self-evident that all our experiences involve a unity of sensations , emotional tones , thoughts , and so on .
So when someone we know passes us on the street without acknowledging our presence , this experience seems to mean an inextricable combination of wait and gasping , guilt and worry , puzzling for reasons and planning for follow up actions .
But mindfulness intervention suggest that this kind of complexity of experience is actually not basic at all .
Our everyday experiences should be recognized as comprising multiple components which we should be able to unpick .
Hence mindfulness interventions invite us to entertain a place of bare or direct or pure experience that lies somewhere prior to that complex constellations of thoughts and motions and sensations .
This conceit , that there is a pure form of experience that can be felt by any of us at any time and that this pure experience is importantly unsullied by our preconceptions and preconditioned judgments , is vital to mindfulness in general .
This type of experience is what we usually mean when we use phrases like beginner 's mind , open or non-judgmental awareness , or when we say things like , pure experience has no meaning at all .
So instead of allowing ourselves automatically to unify our immediate experience of walking down the street on a sunny afternoon with perhaps wandering thoughts about how we might have insulted someone in the past , or perhaps how typical it is to discover that this person does n't like us anymore , or perhaps the heavy , sinking feeling of resignation and sadness that settles into our stomach .
Mindfulness training invites us to recognize that the sensations of the sun on our skin , the street beneath our feet and the sight of somebody walking past are of a different and more direct order than the judgments , explanations and emotions that follow so rapidly afterward .
Indeed , rather than being caught in the direct experience , these modifiers actually modify our reactions to the experience .
They 're a conditioned attempt to construct them into meaningful compounds in our minds .
This brings us neatly to the second basic principle of mindfulness interventions , which is something like that the insight of the felt emotional quality of an experience is not a feature of the experience itself , but instead is a kind of arousal that emerges from our experience to it .
The idea that our emotional arousal , whether positive , negative or neutral emerges from our interaction with experiences , rather than being contained in the experiences themselves opens a space for us to make some skillful choices about how we might regulate our emotional condition by deliberately bringing our attention to specific sensations and experiences .
While we 're walking home , how would it be to rest our attention on the sensation of the sun in our skin here and now , rather than on our anger about an insult or offense that took place last week .
In concrete terms , both MBSR and MBCT make use of the process of inviting our attention onto or into the breath as the site of an experience that 's usually arousal neutral .
We might also think of making use of particular postures or places or scents in this way .
Hence , mindfulness interventions train us in the capacity to regulate our emotional state through correctly intentioned and disciplined regulation of our attention .
Mindfulness training enables us to become increasingly skillful in the recognition of rumination , wandering , and negative , or even positive , emotional arousal associated with specific experiences and then to make a deliberate choice to place our attention elsewhere such as on our breath and hence create a greater sense of calm , ease , or well-being .
One of the important consequences of this principle is that it works to change the emotional quality of an experience without requiring us to avoid the experience itself and without taking us away from that experience .
We should be able to continue to perform any activities that are typical of our daily lives and to take pleasure in these activities that once caused stress , anxiety , or pain .
In other words , this principle works towards the establishment of a more spacious and permissive locus of being .
And this idea of spaciousness leads us into the third principle , which we might consider as the cultivation of a metacognitive standpoint or a stance of meta awareness .
And all that is meant by this rather intimidating phrase is that mindfulness interventions train us to take a step back from our experiences into a wider space in which we can be more aware of the way we encounter , process , and experience those experiences .
In this wider space , we have more room to consider and decide where we would like to place our attention .
The more expansive view gives us more information , keeps us open to other positive clues in the environment that we might otherwise miss if we remain stuck in a narrower site .
Some people talk about this as the cultivation of the observing self .
That 's a version of you who 's able to observe how you interact with and experience the world , rather in a manner of a generously compassionate , gently curious friend who might advise you on how to proceed .
Psychologists often refer to this as a process of decentering , in order to flag the way that involves the intention to displace the experiential subject , that 's the you who experiences things in the world , from the center of the locus of your being .
The you who is experiencing debilitating stress or anxiety sitting in a waiting room before a job interview is not the center of your being but just a projection of you that you can watch , advise , and guide from this more spacious position .
This decentered location enables a kind of receptiveness , flexibility , and open awareness that seems impossible to the you who is constricted , narrowed , and rigid with stress and anxiety .
Hence , this is a much wiser standpoint from which to deploy your attention , formulate your intentions , and make decisions .
Just as we have noted in the particular cases of MBSR and MBCT , it 's also worth noting some concerns about these common principles .
In particular , the cultivation of a meta-culmative or de-centered standpoint for the self can itself be a disturbing or uncomfortable experience for people .
But in a very rare case , in rare cases , this discomfort can become associated with a form of dissociative disorder which requires careful professional therapeutic attention .
And again the risks of this , although relatively small in general populations , are another factor in the consideration of the responsibility of instructors of mindfulness interventions .
In fact , scientists know rather little about how or why this happens , and very little about what kinds of populations are at greater risk than others .
These are amongst the various reasons for ensuring that mindfulness instructors are suitably qualified and experienced , whatever that turns out to mean .
Conversely , for some , this decentered sense of self can actually become a source of pleasure or even intoxication in its own right .
The feeling of observing ourselves from without can sometimes be accompanied by a sense of euphoria and freedom .
These kinds of experiences during secular mindfulness intervention often lead participants make inquiries about more spiritual , philosophical , or existential issues .
So , It 's now time leave the empirical sciences behind us and to move on to the next module in which we 'll explore precisely the spiritual , philosophical , and existential implications of mindfulness .
In this session , we 'll investigate some of the ways that scientists use to assess animal welfare , using the five freedoms and beyond .
Professor Bramble reported to the UK government on animal welfare and suggested that all animals should have the freedom from pain and discomfort .
This was then expanded upon by the Farm Animal Welfare Council in the early 70s to what is now internationally recognized as The Five Freedoms .
These are the freedom from hunger and thirst . The freedom from pain , injury and disease .
The five freedoms are great because , they are an internationally recognized method of assessing animal welfare .
For example , you 'll notice that the first four of those freedoms , are all the freedoms from something .
They 're very negative , whereas the fifth freedom , the freedom to display natural behavior , is a positive .
This mismatch between the positives and negatives can make the Five Freedoms a little unwieldy to use . The Five Freedoms are also a little bit more difficult to use for companion animals or wild animals .
For example all wild animals have the freedom to display natural behaviors but we can still affect their welfare in other ways so do the Five Freedoms really apply to them in the same way ?
For this reason , we have come up with other ways of assessing animal welfare , other animal welfare frameworks , such as the Duty of Care framework .
Unlike the Five Freedoms , the duty of care framework focuses on what we can provide animals , providing them with a safe , happy environment that they can enjoy .
And it also encourages legal responsibility to the owner , or the animal caretaker , to make sure that they provide these animals with a good welfare environment .
Although the different animal welfare frameworks can take different approaches , they all serve the same purpose .
An animal welfare framework helps us to assess and describe how an animal is coping with its environment .
The Five Freedoms may have been designed to describe production animal welfare , but they can be adapted to describe the welfare of this pet cat .
She 's clearly had a little bit too much freedom from hunger and thirst .
Her natural behaviors are also limited because she 's reluctant to play and jump .
Another welfare framework , such as the duty of care , which focus instead of what this cat needs to have good welfare , such as a balanced diet .
Both frameworks would agree that this cat 's welfare is compromised due to her obesity , and that portion control will improve the situation .
As you should be able to understand and describe the main histological features of the several different parts of the GI tract . I will give you some specific pointers in the following reader .
You can also get more information in the tutorial pages .
My name is Jill MacKay , and I 'm an animal behavior and welfare researcher with a particular interest in the human-animal bond and how we talk about animal welfare in general .
I think some of the most interesting cultural differences that we think about with animal welfare are probably best exemplified with the dogs .
So in western cultures , we typically think of dog being man 's best friend . And they are the faithful pet who waits for us when we come home from school .
If you think about areas where there are lots of feral dogs , they become a bit of a public health risk . Particularly for they carry lots of diseases such as rabies , which makes the dogs very aggressive .
And they can actually fatally attack people which makes them really dangerous from a public health perspective . And dogs become something to be feared .
And this happens particularly in some regions in India and some countries in Africa .
And that 's very different from the way we perceive them in the west .
And if you move further east to China , well dogs there , they become a product for meat .
And yet in China they also have companion animal dogs as pets as well . Much like the way , in Europe , we will have horses bred for meat and yet horses for companion animals .
But when you come back to the west and the western culture , even though we have this idea of dogs being man 's best friend , we also use them in industry and in pharmaceutical testing and in clinical trials .
And although they 're just one species and we 've got a long history with them within humanity , we have many different cultural viewpoints on them and they serve a wide variety of roles .
Well , it 's important to remember that regardless of what the culture is looking at the dog , what the use of the dog is within that particular culture or society , what really matters for the dog 's welfare is how the dog is perceiving the world , what 's it feeling ?
So from that point of view , the welfare needs of a dog regardless of whether it 's a companion animal , a beloved pet , an animal used in clinical trials or even a meat dog . Its needs are very , very similar all across those different categories .
So the culture may look at things differently , but the welfare needs of the dogs do n't change . International standards for animal welfare is absolutely possible , even across all these different cultures that we 've been talking about .
But the only way to do that is to have a common language and a common standard . And the way for that , is to have scientifically backed evidence behind what we 're saying .
It 's all very well for us to say that , oh , we think of a dog as a pet . But , unless we have the scientific evidence that dos like human companionship , which in fact they do , it does n't matter .
So what we have to do , is pervade that scientific evidence , pervade that way of talking about animal welfare , in a scientific and objective manner . And then we can have an international standard of animal welfare .
Welcome to this week 's session which is all about assessing animal welfare . And in this session , we 'll be exploring the sorts of measurements that are used by international scientist to try to understand the world from an animal 's point of view and how this information can be used in helping to make good decisions about animal welfare .
Firstly , let 's recap , we know that people 's attitudes and beliefs , especially when firmly held , will drive human behavior , this is certainly the case with attitudes towards animals and their welfare .
These have been shown to influence people 's perceptions of businesses , products , and even cultures .
Think of the most recent issues to affect the sale of meat , such as the horse meat scandal in Europe , leading to a marked reduction in the purchase of beef products .
Such is the power of public opinion , that some businesses involved in buying meat for their supermarkets shelves or food outlets have been encouraged to implement policy changes so that the products they sell have the highest standards of animal welfare .
Public opinion can also lead to changes in legislation governing the way animals are housed . Such as with the banning of the so-called battery cage housing systems for laying hens in Europe .
However , people 's opinions are not always based on facts and attitudes can often be challenged when good data is presented .
It is for this reason that many animal welfare organizations will use both the emotional as well as robust arguments based on scientific evidence for lobbying businesses and governments for positive changes in animal welfare policy and practice .
As we learned during last week , animal welfare is a complex subject and there are many , many different opinions about animal welfare as well as differences in understanding what it is .
When asked , most people would be able to provide examples of what they consider to be poor animal welfare . And in many cases these will include cruel treatment of companion animals and with issues with the way production animals may be kept .
However , it 's interesting that finding examples of good or positive welfare seem far harder .
Most people feel comfortable with suggesting that a dog wags its tail when it 's happy to see you , but examples of happiness or pleasure in farm animals are less obvious .
This is probably because we are more likely to have known or owned a dog or cat and therefore feel more familiar with their emotional expression .
And it may also reflect a phenomenon that has been termed speciesism where we see different levels of consideration being given to animals as a consequence of their species rather than any evidence of a difference in their ability to feel positive or negative emotions .
Using a scientific approach , we can take out some of the subjectiveness that occurs when we base decisions about animals and their welfare on our own beliefs and our own opinions .
Using an evidence approach and shows that we make good use of measurements that have been developed to enable a much more objective assessment of an animal 's response to the situation it finds itself in .
Using the scientific method means that a hypothesis or explanation is formulated based on good observations .
The measurements used to test this hypothesis will need to be robust and the results subjected to statistical analysis before a decision is made about whether there is good enough evidence for or against a possible explanation .
The process of experiment makes me repeat it a number of times to check that the results are similar , at which point a conclusion can be drawn .
Scientists believe that this process allows us to make more accurate judgements about how an animal is coping with its environment or its situation . And move us much closer to understanding the individual animal 's experience , enabling us to make more animal-centric decisions .
There are a number of methods that are used for measuring animal responses that are based upon the notion that the animal 's emotional response to the situation it finds itself in is expressed through its physiology and behavior .
For example , an animal experiencing fear will , depending upon its species , try to rapidly remove itself either through flight or hiding and may even be inclined to remove the thing that it is fearful of through behaving aggressively towards it .
The flight or fight behavior response is associated with increased heart rate and blood pressure as well as rises in various brain and blood chemicals all of which can be measured .
Scientists are now extremely good at measuring behavioral , physiological , and even immunological responses to negative experiences , such as pain , fear , or stress .
These emotional experiences cause immediate responses , such as increased heart rate , as well as longer term responses like behavioral changes , such as the repetitive actions of a horse or a dog kept stabled or kenneled for long periods of time .
And if the animal is kept in this negative state for long enough changes in the animal 's ability to resist disease will lead to health and growth problems .
However , more recently there has been an appreciation of the importance of discovering measures indicative of positive emotional states . Social behavior such as play and comfort behavior that occur during specific times such as the nest building scene in pregnant sows .
This has led to a deeper understanding of the importance of different aspects of an animal 's environment from birth or hatching through early developmental experiences , to the way an animal is grouped , housed , and transported in relation to its quality of life .
Through measuring changes in behavior and physiology in a structured way , scientists are able to work out the relationships between different types of housing , different handling conditions , and transport .
And look at the way animals normally experience this , and how their responses will allow them to adapt to cope with the challenges that this may impose upon them .
By measuring animal welfare through these indicators , we 're in a much better position to properly manage animal welfare needs .
This helps us to work out what is positive or negative from the animal 's point of view which can be applied in approving animal handling methods or environments and ensuring higher standards of welfare .
That 's it for Module Two of the course . I hope you 're now feeling reasonably confident about multivariate calculus , and able to navigate multi-dimensional hyper sandpits of your own in the future .
See you in the next module , where we 'll be discussing the multivariate chain rule , and how it can help you optimise the weightings in a neural network .
You 've learned about how RNNs work and how they can be applied to problems like name entity recognition , as well as to language modeling , and you saw how backpropagation can be used to train in RNN . It turns out that one of the problems with a basic RNN algorithm is that it runs into vanishing gradient problems .
Let 's discuss that , and then in the next few videos , we 'll talk about some solutions that will help to address this problem .
And let 's take a language modeling example . Let 's say you see this sentence , " The cat which already ate and maybe already ate a bunch of food that was delicious dot , dot , dot , dot , was full . " And so , to be consistent , just because cat is singular , it should be the cat was , were then was , " The cats which already ate a bunch of food was delicious , and apples , and pears , and so on , were full . " So to be consistent , it should be cat was or cats were .
And this is one example of when language can have very long-term dependencies , where it worked at this much earlier can affect what needs to come much later in the sentence . But it turns out the basics RNN we 've seen so far it 's not very good at capturing very long-term dependencies .
To explain why , you might remember from our early discussions of training very deep neural networks , that we talked about the vanishing gradients problem .
So this is a very , very deep neural network say , 100 layers or even much deeper than you would carry out forward prop , from left to right and then back prop .
And we said that , if this is a very deep neural network , then the gradient from just output y , would have a very hard time propagating back to affect the weights of these earlier layers , to affect the computations in the earlier layers .
And for an RNN with a similar problem , you have forward prop came from left to right , and then back prop , going from right to left .
And it can be quite difficult , because of the same vanishing gradients problem , for the outputs of the errors associated with the later time steps to affect the computations that are earlier .
And so in practice , what this means is , it might be difficult to get a neural network to realize that it needs to memorize the just see a singular noun or a plural noun , so that later on in the sequence that can generate either was or were , depending on whether it was singular or plural .
And notice that in English , this stuff in the middle could be arbitrarily long , right ?
So you might need to memorize the singular/plural for a very long time before you get to use that bit of information .
So because of this problem , the basic RNN model has many local influences , meaning that the output y ^ . And a value here is mainly influenced by inputs that are somewhere close .
And it 's difficult for the output here to be strongly influenced by an input that was very early in the sequence .
And this is because whatever the output is , whether this got it right , this got it wrong , it 's just very difficult for the area to backpropagate all the way to the beginning of the sequence , and therefore to modify how the neural network is doing computations earlier in the sequence .
So this is a weakness of the basic RNN algorithm .
One , which was not addressed in the next few videos . But if we do n't address it , then RNNs tend not to be very good at capturing long-range dependencies .
And even though this discussion has focused on vanishing gradients , you will remember when we talked about very deep neural networks , that we also talked about exploding gradients .
We 're doing back prop , the gradients should not just decrease exponentially , they may also increase exponentially with the number of layers you go through .
It turns out that vanishing gradients tends to be the bigger problem with training RNNs , although when exploding gradients happens , it can be catastrophic because the exponentially large gradients can cause your parameters to become so large that your neural network parameters get really messed up .
So it turns out that exploding gradients are easier to spot because the parameters just blow up and you might often see NaNs , or not a numbers , meaning results of a numerical overflow in your neural network computation .
And what that really means , all that means is look at your gradient vectors , and if it is bigger than some threshold , re-scale some of your gradient vector so that is not too big .
So there are clips according to some maximum value . So if you see exploding gradients , if your derivatives do explode or you see NaNs , just apply gradient clipping , and that 's a relatively robust solution that will take care of exploding gradients .
But vanishing gradients is much harder to solve and it will be the subject of the next few videos . So to summarize , in an earlier course , you saw how the training of very deep neural network , you can run into a vanishing gradient or exploding gradient problems with the derivative , either decreases exponentially or grows exponentially as a function of the number of layers .
And in RNN , say in RNN processing data over a thousand times sets , over 10,000 times sets , that 's basically a 1,000 layer or they go 10,000 layer neural network , and so , it too runs into these types of problems .
So what we do in the next video is talk about GRU , the greater recurrent units , which is a very effective solution for addressing the vanishing gradient problem and will allow your neural network to capture much longer range dependencies . So , lets go on to the next video .
This is Duke University .
>> Welcome to Oil and Gas Industry Operations and Markets .
And this course is about one of the largest and most economically important industries in the world , the oil and gas industry .
The scope of the oil and gas industry is huge .
It not only includes businesses that explore for , produce , refine , transport , and sell oil and gas , but also a multitude of service companies that support these activities and transactions .
The economic importance of the oil and gas industry is even more vast than its scope .
Changes in the price of natural gas , crude oil , and petroleum products affect not only fuel prices , but electricity prices , food prices , prices of all sorts of manufactured goods .
And thus job markets , international balances of trade , and nation 's gross domestic products .
I hope you 'll join me in learning about this industry which is so vital to the operation of modern society .
You need to know more about the oil and gas industry if you are seeking a job , or have a job , in the energy industry , or a decision maker whose business or policy decisions are affected by the industry or simply want to be an informed citizen .
You also need to know more about the oil and gas industry if you work in national security , in financial markets , or in international development and relations .
Finally , if you study economics , business , geography , public policy , environmental sciences , or one of many more disciplines that pertain to human activities , this course is also of value .
The course will explore the two major components of the oil and gas industry .
Operations and markets .
In the operations module , you will learn about the core activities that the industry executes to bring natural gas and petroleum products to market .
We will look at the the exploration and production of oil and gas , the processing and refining of the oil and gas into useable fuels and other valuable commodities , and finally the transport of oil , gas and petroleum products from wells , to refineries , to retail outlets .
In the second module , the course shifts to the markets that drive oil and gas industry operations .
You will learn about the various costs of the core oil and gas industry activities , the factors that determine the prices that oil , gas and petroleum products sell for , and the effect that the amount of oil and gas remaining in the ground has on the future viability of individual companies to the industry as a whole .
So that 's the course in a nutshell . Throughout the course , your knowledge will be tested , but you will also have the opportunity to contribute to one or more research projects focusing on industry activities , oil and gas prices , and worldwide markets .
We have an international audience and experts in many different fields .
So please take the opportunity to learn from each other .
While an oil and gas expiration and production company makes money by selling the hydrocarbons it finds and produces , the company 's future viability and thus market value as a business also depends on a company 's reserves of oil and gas still in the ground .
Like a high worth individual with lots of money in the bank , an oil and gas company with large reserves still to be produced is generally considered to be a high worth enterprise .
However , the precise amount of reserves a company controls is actually unknown .
Furthermore , a company 's reserves are actually just the economically producible fraction of the total amount of oil and or gas that may exist under the company 's leases .
This greater volume of oil and or gas is referred to as the total resource potential or the in-place resource .
A good way to think about reserves versus resources is in terms of inverse cumulative distributions .
That 's just the fancy name for plots in which the amounts of oil and or gas increase along the X axis , while the probability of exceeding an amount ranges between 0 % and a 100 % along the Y axis .
The point at the uppermost left side of the curve is the amount of oil and or gas that expert geologists and petroleum engineers contracted by the company , agree is almost certainly in place beneath the company 's leases .
The point at the lower-most right side of the curve , on the other hand , is the largest and least feasible amount of oil , and , or gas estimated to be in place .
Between these two points the curve descends across increasing amounts of oil and or gas that are evermore more unlikely to exist beneath the leases .
Note that the in place resource curve does not distinguish oil and or gas that can actually be gotten out of the ground using existing technology from oil and gas that can not currently be extracted .
If we restrict our analysis to only that oil and gas that is currently recoverable regardless of the cost of extraction , then we produce a second inverse cumulative distribution of technically recoverable resources .
And this curve lies to the left of the in place resource curve .
Going a step further and focusing only on those resources that can be produced at a profit with existing technology results in a third cumulative distribution that lies even farther to the left in the plot .
This last curve reflects economically recoverable reserves .
When you look at this last most conservative curve , keep in mind that the economically recoverable reserves are not all equal .
As before , reserves beneath the left side of this curve have a greater probability of existing then those beneath the right side of the economically recoverable reserve curve .
In fact , the economically recoverable reserve curve is often broken into three segments .
Economically recoverable reserves that have less than 50 % probability of existing are referred to as proved , probable and possible reserves , or 3P reserves .
Economically recoverable reserves that have a 50 to almost 90 % probability of existing are referred to as proved and probable , or 2P reserves .
In economically recoverable reserves that have a 90 % or greater probability of existing are simply proved , or 1P reserves .
This last category , proved reserves , are further subdivided in to developed and undeveloped reserves .
Proved developed reserves , are those that can be produced from existing wells with up to moderate additional investment .
Proved undeveloped reserves are those that can only be brought online by drilling new development wells .
In order to be listed on a U.S.
Stock Exchange , an oil and gas company with reserves must report to investors the amount of 1P or proven reserves , which is the most conservative estimate of their holdings .
Reserves reported by companies that are not listed on a US stock exchange are not subject to this restriction . So their reserves may include probable impossible reserves .
Or even resource estimates , rather than just reserve estimates .
It is also important to keep in mind that economically and technically , recoverable resources can change from one year to the next depending on the cost of using available extraction technologies and the prices that oil and gas are selling for .
For example , the cost of fracking and horizontal drilling has declined since the 1990s , changing formerly tactically recoverable oil and gas reserves in shales into economically recoverable reserves .
So companies with mineral rights to shale oil and shale gas have had their economically recoverable reserve curve shift to the right , increasing both the amount and total value of these company 's reserves .
It allows you to analyze data , take advantage of many statistical packages and create beautiful visualizations and web applications . As its name suggests , RStudio IDE is only for R.
Like other IDEs , in RStudio IDE , you can code in a console or script editor , keep track of your variables and history , display your plots , manage your packages and see help documentation for R.
If you 're an R user , you 'll already be familiar with this type of setup , where you can analyze data , see your console and visualize your plots and graphs all in the same environment . Another great feature is that you 'll also have access to Apache Spark .
In this video , we will provide you with a quick summary of the main points from our first three courses to recall what you have learned .
If you have just completed our third course and do not need a refresher , you might skip to the next lecture .
We started our first course explaining how a new torrent of big data combined with cloud computing capabilities to process data anytime and anywhere has been at the core of the launch of the big data era .
Such capabilities enable or present opportunities for many dynamic data-driven applications , including energy management , smart cities , precision medicine , and smart manufacturing .
These applications are increasingly more data-driven , dynamic and heterogeneous in terms of their technology needs . They 're also more process-driven and need to be tackled using a collaborative approach by a team that puts value on accountability and reproducibility of the results .
Overall , by modeling , managing , integrating diverse data streams we add value to our big data and improve our business even more before we start analyzing it .
A part of modeling and managing big data is focusing on the dimensions of the scalability and considering the challenges associated with these dimensions to pick the right tools .
We also talked about characteristics of big data , referring to some Vs like volume , variety , velocity , veracity and valence . Each week presents a challenging dimension of big data , namely size , complexity , speed , quality and connectedness .
We also added a sixth V , value , referring to the real reason we are interested in big data .
To turn it into an advantage in the context of a problem using data science techniques , big data needs to be analyzed .
We explained a five steps process for data science that includes data acquisition , modeling , management , integration , and analysis .
The influence of big data pushes for alternative scalability approaches at each step of the process .
If we just focus on the scalability challenges related to the three Vs , we can say big data has varying volume and velocity , requiring dynamic and scalable batch and stream processing .
Big data has variety , requiring management of data in many different data systems , and integration of it at scale .
In our introduction to the big data course , we talked about the version of a layer diagram for the tools in the Hadoop ecosystem , organized vertically based on the interface .
Most of the tools in the Hadoop ecosystem were initially built to compliment the capabilities of Hadoop for distributed file system management using HDFS . Data processing using the MapReduce engine , and resource scheduling , and negotiation using the YARN engine .
Over time , a number of new projects were built , either to add to these complementary tools or to handle additional types of big data management and processing not available in Hadoop , just like Spark .
Arguably , the most important change to Hadoop over time was the separation of YARN from the MapReduce programming model to solely handle resource management concerns .
This allowed for Hadoop to be extensible to different programming models and enable the development of a number of processing engines for batch and stream processing .
Another way to look at the vast number of tools that have been added to the Hadoop ecosystem is from the point of view of their functionality in the big data processing pipeline .
Simply put , these are associated with three distinct layers for data management and storage , for data processing and for resource coordination and workflow management .
In our second course , we talked in detail about the bottom layer in this diagram , namely data management and storage .
While this layer includes Hadoop 's HDFS , there are a number of other systems that rely on HDFS as a file system or implement their own no-SQL storage option .
As big data can have a variety of structured , semi-structured , and unstructured formats and gets analyzed through a variety of tools , many tools were introduced to fit this variety of needs .
We call these big data management systems .
We reviewed Redis and Aerospike as key value stores where each data item is identified with a unique key .
We also got some practical experience with Lucene and Gephi as vector and graph-stores respectively .
We also talked about Vertica as a column-store database where information is stored in columns rather than rows .
Cassandra and HBase are also in this category .
Finally , we introduced Solr and Asterisk DB for managing unstructured and semi-structured text and MongoDB as a document store .
The processing layer is where all these different types of data get retrieved , integrated , and analyzed , which was the primary focus of our third course .
In the integration and processing layer , we roughly refer to the tools that are built on top of HTFS and YARN , although some of them were with other storage and file systems .
YARN is a significant enable of many of these tools making a number of batch and stream processing engines like Storm , Spark , Flink and Beam possible .
This layer also includes tools like Hive and Spark SQL for bringing a query interface on top of the storage layer , Pig for scripting simple big data pipelines using the MapReduce framework and a number of specialized analytical libraries , formation learning , and graph analytics .
Giraph and GraphX of Spark are examples of such libraries for graph processing .
Mahout on top of the Hadoop stack and MLlib of Spark Are two options for machine learning .
Although we had a basic overview of graph processing and machine learning for big data analytics earlier in our second and third courses , we have n't gone into the details there .
In this course , we will use Spark 's MLlib as one of our two main tools , providing a deeper introduction to the machine learning library of Spark .
This is where integrations , scheduling , coordination , and monitoring of applications across many tools in the bottom two layers take place .
This layer is also where the results of the big data analysis get communicated to other programs , websites , visualization tools and business intelligence tools .
Workflow management systems help to develop automated solutions that can manage and coordinate the process of combining data management and analytical tasks in a big data pipeline as a configurable , structured set of steps .
Workflow driven thinking also matches this basic process of data science that we overviewed before .
Oozie is an example workflow scheduler that can interact with many of the tools in the integration and processing layer .
Zookeeper is the resource coordination tool which monitors and manages and coordinates all these tools and named after animal .
In which we will use machine learning techniques to apply to our five step data science process and analyze big data .
Just a simple Google search for big data processing pipelines will bring a vast number of pipelines with a large number of technologies that support scalable data cleaning , preparation , and analysis .
How do we make sense of all of it to make sure we use the right tools for our application ?
Over the next few weeks Dr. Mei will walk you through some of the most fundamental machine learning techniques , along with introductory hands on exercises we designed for you to ease you into the world of machine learning .
Welcome to User Experience Design . The goal of the requirements gathering phase is to understand the problem space .
To understand the problem space , we carry out a set techniques .
These provide us with information with data , I 've told you that one of the mantra 's of this class is that design is a systematic and data driven process .
Techniques will give us different kinds of data .
The two main categories are quantitative and qualitative .
Quantitative data can be thought of as information that can be transcribed numerically .
Think about survey data , we can put it into spreadsheets .
We can then analyze it using descriptive statistics .
We can find out things like what 's the average age of the user in the study , what was the median number of years that that user had their current phone ?
Qualitative data , on the other hand , is more easily thought of as providing us with Thematic information .
An example of qualitative data is information gathered from interviews with users .
The information you collect will more easily be put into a narrative than into spreadsheets , imagine that you interviewed 15 people about the reasons they got a new smartphone .
Is likely that we can start to see commonalities in the categories or themes that lead them to their decision to get a new phone .
Is one type a data better than the other ?
No , they both give us different and important information , I 've heard colleagues say that quantitative data gives us the what about the user and qualitative data give us the why .
In fact most designers will use a combination of quantitative and qualitative data .
You know that they 're not , they can be different on many dimensions .
One way to think about users and categorize them systematically is by considering how they will be affected by the new design .
In the next set of slides , I 'll define each term and then provide an example of how the various users are united by a given design .
Primary stakeholders are the people who use the design directly .
These are the users that designers most commonly interact with , they 're called the end users .
Secondary stakeholders do not use the design directly , but may do so indirectly because they get some kind of output from it .
Tertiary stakeholders may not use the design at all , but are directly affected by the design in either a negative or a positive way .
All right , so here 's the example I promised you .
Let 's say that we design a new system to keep track of wear on running shoes .
Using the same example , let 's say that the runner is actually part of a member of a track team .
Here , the coach for the track team would be considered a secondary stakeholder .
Because he would be the one that monitors the data about the runner 's shoes and makes decisions about when they have to get a new pair of sneakers .
He may get a bonus if the new design increases sales or he 'll get a cut in the budget if it does n't .
As designers we often focus on the end user .
However , thinking about secondary and tertiary stakeholders can also help us to develop designs that are innovative and can give our client a competitive edge .
In this sense , understanding stakeholders leads to better user experience design .
Back to our example about another running shoe design that measures tread wear .
We can imagine that our high end running shoe might have a high cost .
If we now include a dashboard for coaches , this is a value added , however if we consider the tertiary stakeholder 's prospective .
On how much it would cost to put an actual sensor in each shoe then we might come with an alternative design consideration .
Maybe it 's not worth it to put our sensor in every shoe but maybe it 's best to fabricate the sensor and make it transferable from shoe to shoe .
In this lesson , we learned about the kind of data that we can collect during the requirements gathering phase and how we can think about the user as being stakeholders .
Welcome to User Experience Design . Today we will explore how to present the findings we gathered about the user during the requirements gathering process .
Interaction design is a cycle . We 've spent a good deal of time learning various requirement gathering techniques to learn about the problem space .
We 've discussed that as designers we provide the best user experience when we acknowledge that the user uses interfaces to accomplish tasks . The first step of the design process is to understand how users are completing the task now .
We have just finished discussing the techniques that allow us to discover what the user is doing now .
Now we will focus on techniques that will allow us to present , or in other words summarize our results .
This is another important aspect of the designer training process .
In this lesson , we will discuss techniques for representing what we discover about the user .
These include descriptive statistics , user characteristic tables and persona .
Descriptive statistic allows us to summarize quantitative information .
This includes the range , mean and median of a data set .
The range tells us what the minimum and the maximum number were for a set of numbers . The mean allows us to know the average score for a certain set .
Let 's say that we collect the data from five participants on how many times they download information daily .
The range was 3 to 30 , that means one person said they downloaded three different content types .
While there was one that downloaded 30 different items a day .
So even though there was a person that downloaded 30 items and one that only downloaded three .
The median gives us important information about the distribution of those five data points .
Characteristics table allow us to highlight important aspects about what we learn via the requirement gathering process .
It provides a tabular summary of our findings .
In this table we see that we can include both quantitative and qualitative information .
Quantitative information includes basic demographics , for example , age , sex and education .
But also a summary of qualitative information , such as the user 's computer experience or their motivation and attitudes .
The persona provides a narrative of both the qualitative and quantitative data .
It allows us an opportunity to convey the richness of the data set we collected in the story line . That highlights all of the important data we collected about the user .
In other words , it allows the user and her needs to come to life .
Here we have a persona for a woman that uses the ATM .
Take a minute to read it . As you go through it try to identify what data could have been collected via naturalistic observation , surveys , focus groups or interviews .
You can also make a list of questions that the designer asked in order to get the narrative that we see here for the woman using the ATM .
In this lesson , we discussed techniques for representing what we discovered about the user . These included descriptive statistics , user characteristic tables and personas .
I hope you join us next time when we review techniques for representing what we learned about the tasks .
Welcome to Introduction to User Experience Design . Today we start a new module where we will consider how to design alternatives to improve the user experience .
First , I want to remind you about the core concept that drives user experience design . That is , that users use interface to accomplish a task .
The premise is that the best interface can only be designed if we understand the user and the task they want to accomplish . In the previous module , you learned about the importance of understanding the user and her current practices .
You now have a deep understanding of the user .
You understand the who , what , when , where of the user 's goals .
In other words , the information you collected in the requirements gathering phase has allowed you to understand the problem space .
What it is that the user needs . In this module we consider how to use the information from the requirements gathering phase to develop a set of alternative designs to meet the needs you 've discovered .
We originally defined design as the development of a novel creation to meet some needs .
The goal of the novel designs we create is to do a better job of meeting the needs of the user than the existing designs .
Here I want to underscore that the goal of design is not novelty for its own sake , it 's novelty and the service of improving the user experience .
As we think about the user 's needs , it 's also important to remember that they do not exist in a vacuum , but rather in an ecosystem .
Where the individual user interacts with the interface , and in turn the user may interact via the system with another group of users , and all of these users coexist in a society with a set of values .
In other words , the design alternatives we develope impact the entire ecosystem .
Next , I will go through each of these levels and consider what it means to improve the user experience via novel design .
I encourage you to review module one lesson one if you need to brush up on the definitions of the concepts that are in this figure .
Designing novel interfaces is all about finding improved ways to mediate how the user accomplishes a task .
It may require that we create a whole new system .
Or it may be that we simply design novel inputs and outputs .
Consider an instance when you bought a new model of your favorite X. What changed or improved in the design ?
One example that I can bring to mind is when I bought a new cell phone model .
The company changed the color scheme so that now there was greater contrast between the text and the background .
They also allowed me to increase the size of the font .
While there was no major change in a way that I did the task or the core goals of the task I was trying to complete , these small changes amounted to an improved user experience for me .
This was improved design focused on the individual user .
Considerations when designing at the level , at this level include keeping in mind user characteristics .
These include the user 's age , education level , comfort with technology . Physical constraints and capabilities .
It 's hard to believe that my original phone did n't have a way to increase the font size .
In reviewing the result from the requirement gathering phase , what can you do to improve the individual 's experience ?
How can you change the design of an existing interface to improve interactions among a group of users ?
Consider a system that you currently use that allows a group interaction .
For example , Facebook .
How did you feel the last time that they updated the interface ?
Sometimes improving the individual 's experience leads to designing a group interface .
For example , Weight Watchers is a program designed to help people learn healthy habits and thereby lose weight .
In their last revamping of the Weight Watchers app they added a group function .
Now Weight Watchers members have a social media component called Connect , that allows the users to share their weight loss journey .
The design ecosystem includes the social level .
Here the idea is that there are cultural issues that impact the use or adoption of a system .
As designers we need to be aware of cultural values related to the task that the user engages in , and design with those issues in mind .
I recently read a really fascinating article , where the team was trying to develop a system to remind users about their medication , or about taking their medication .
This seems like a pretty straightforward task except that in this case the illness had a lot of negative social stigma associated with it .
Thus , the interface had to be designed with this in mind .
For example , there was nothing in the interface that signaled what the condition was that the individual had . This was to protect the privacy of the user .
I 'll end this lesson by reminding you that the goal of user experience design is to build interfaces or artifacts that are useful and usable .
By useful that it allows the user or users to complete a task .
By usable I mean that the user can accomplish the task via the interface in an effective , efficient , and satisfying manner .
In this lesson we introduce concepts that are essential to keep in mind when developing design alternatives .
If we keep these in mind then we will be able to develop design alternative to improve the user experience .
So , what is accounting ?
Accounting is a system of recording , categorizing , summarizing , and communicating financial information about an organization to those who might be curious . Accounting is often called the language of business .
Well , because it communicates information to help decision-makers make really good economic decisions . And then it communicates the results or the financial consequences of those decisions to others that need to understand them .
Now let 's understand what makes up a story , starting with the obvious question , what 's a story about ?
Whether you 've got a short story that 's maybe a couple pages , or like a super huge epic that 's maybe like thousands of pages , there needs to be a single theme to that story . Personally , I like to call that a spine .
A spine of a story is the primary theme of what the story is about , what it 's basically trying to say at its core . I strongly believe that a story teller should be able to distill their story into one or two sentences to communicate the overall feel of the story , to say , what 's important to walk away with when you 're reading that story .
Now , it 's important to have a spine , because the story designer has a central concept to make sure all the elements of his or her story serve that concept .
More importantly , what did you walk away with , as being the most important theme ?
It 's pretty tough at first , but it does get easier with practice , so just keep doing it . Now having read the story behind Disney 's telling of Sleeping Beauty in the last lecture , what would you say is the spine of that story ?
As you can imagine , different people will see different spines to Sleeping Beauty , but they should all reasonably be about the same .
The spine of Sleeping Beauty as I would write it , would read an evil fairy curses a newborn princess to a hundred-year sleep when she turns sixteen , to spite her kingdom .
This spell can only be broken by the help of 3 fairies who guard the princess until a kiss from her one true love can lift the curse .
Now , do n't underestimate the power of practicing this skill on as many movies and stories as you can .
And soon you 're gon na be able to see the idea of spines in games as well , because the spine theory does apply to games , too .
Having a spine for your game gives you the same focus and framework as it does for making a traditional story .
So Angry Birds , I would write the spine for Angry Birds as reading , a family of birds flies into action to retrieve their stolen eggs from a gang of greedy green pigs by trying to knock them all over .
With Angry Birds , any game player or graphic , or cinematic that does n't serve that spine should really be cut or changed to make sure the game is conceptually tight and focused .
So as you set out to eventually write out your own stories and games , make sure to give a lot of consideration to that central theme of your tale and how to distill that story into a concise one or two sentence spine .
Now that world , make it yours .
Even if it 's adapted from a non-game environment . If you 're adapting an existing story , an existing world , well detail then , what is really appealing to you ?
Is it the world ?
If the visuals have been defined , if it 's a movie , comic book , well , chances are you ca n't exactly develop the exact same style .
Even though you would want to , you might want to bring your own artistry to it , your own design , your own grain of salt . Well , keep what matters the most .
Really , anything that still works even without any style , design , imagery . Make the design and the style a choice .
Not just a video game world , but any game world in general .
Well then , one of the first things that comes to mind , is the flat map . The flat map , is not necessarily about a flat world .
You can have all the details on your map stating the topography of it , the height , and all the important parts , and spaces and places you might want to visit as a player .
Here you can have a god-like view where you see the world , the entirety of it , all at once .
Something that your character might not be able to see .
You , as a player , or as a video game designer , can see the entirety of the world , and see exactly where you , as a player stand at a tiny point maybe in a much bigger world .
Here , we are talking mostly of open world , video games .
If the character is a cartographer , this big open world game , and is maybe role playing game .
If he is revealing the map , if he is revealing the important points here , like caves , maybe or like loot places .
You will have the ability , like in modern times to use a GPS , sort of , to pinpoint , point of interest , and have the system guide you to that specific location .
Something you ca n't do , while you 're horse riding , but something that the game , and the game designer , will let you do to facilitate the way you can see yourself in that video game world .
Another way to define this map is to bring it in volume , create a model , and dollhouse of parts of world or the world itself . If you 're doing a three dimensional map of volume , where you can see exactly where the temples , the caves , and living spaces are all going together .
It is a comprehensive way to guide the plays , to let them understand how the world itself works .
Here you are an architect , like a supreme architect of a much bigger world .
And it 's like in real life , it has to respect some properties of actual real life spaces .
And then , none at all , because video game world , or virtual worlds in general .
There 's a paradox in all that , and we will talk more extensively about it later in the course .
But virtual worlds are different .
You have to think boundaries .
You have to think of technical and practical limits .
Gross Written Premium is essentially the amount of insurance written not including commissions and some costs . That premier is growing fastest in emerging markets .
Net income was estimated by Ernst & Young to have grown 23 percent in 2018 versus 14 percent in 2017 .
Wearables , driverless vehicles , IoT or the Internet of Things , Big Data , natural language processing , blockchain , distributed ledger technologies , even the possibility of climate change affecting those who are insured along with many others are adding up through the avenues of technology to serve as a disrupting force .
New technologies are also offering all kinds of potential opportunities to do today 's business better in the industry or to grow markets , to make better decisions and address what we could call the customer experience , all coming together as insurtech .
Some would say that even though it 's disrupting and revolutionizing the industry and rapidly today already changing the way insurers , service providers , and ultimately customers experience insurance .
Efficient information exchange , trust , the ability to write contracts that are self-referential and self-aware and which offer a certain kind of immutability , ultimately appropriate because after all insurance is essentially in the form of a contract .
Analytics that help insurers , their service providers , underwriters , and others make better decisions , taking more from data , expanding to new data , augmenting data .
Robo advisors who rely on rules or other kinds of machine learning techniques to interact with customers , it could be customers interacting with technology online or over the phone either enhancing , making more immediate or more accurate customer interactions .
A very interesting area known as wearables , providing data both about the insured or those who are related to the insured all the way back to insurers helping them manage risks , understanding the insured , perhaps improving the customer experience in real-time or in aggregated way .
Connecting ecosystems which could be , by the way , as broad and far-reaching as social media or based on data already collected within the ecosystem of the insured linking back to the entire value chain of insurance .
Artificial intelligence , AI , including natural language processing , mimicking the human capacity to process language , to learn , to extract patterns not easily seen using traditional technologies including even advanced statistics and ultimately making a customer 's experience and an insurers decision-making process better .
We studied the machine learning process , applied techniques to explore and prepare data , discussed the different categories of machine learning tasks , looked at metrics and methods for evaluating a model , learned how to use scalable machine learning algorithms for big data problems , and worked with two widely used tools to construct the machine learning models .
I hope that the lectures , along with the hands-on activities , have given you a sound and practical introduction to machine learning tools and techniques .
I also hope that the course piqued your interest in the exiting and rapidly developing field of machine learning for big data . Keep in mind that the best way to learn machine learning is to do machine learning .
So I encourage you to go out and find a problem or data set that interest you .
Apply the techniques you 've learned in this course to it and start analyzing .
Thank you for your time and effort on this course , happy machine learning .
In this module , we 're going to expand on our understanding of block chains .
By the end of this module , you 'll be able to describe the differences between various trust frameworks and other consensus mechanisms .
You 'll learn about some extensions to blockchains , how information can be stored and how separate blockchains can work together .
Finally , you 'll be able to describe what sorts of problems can benefit from a blockchain solution and which ones really ca n't . And the implications of blockchain on traditional business , let 's get started .
So , you probably heard people say something like , " Let 's put it on the blockchain . " And as you can probably see from prior modules , that 's really a misnomer .
So , why is blockchain interoperability important ? Really for the same reason that blockchain technology itself is important .
So , while it 's in its infancy , it clearly has captured the imagination of thousands of people around the world .
So from developers , and people in technology , as well as people on the business side , those focused on social change , and the ways that this technology can transform our society in many ways similar to the way that the Internet has .
One of my colleagues Paul Koolhaas , has a good quote about the future with a Web 3.0 that is buzzing with various kinds of blockchain .
So , public blockchains , private ones , anonymous zero-knowledge proofs , all of which have various applications built on top of them .
So , there 's not one blockchain to rule them all , but a diversity of different blockchains for various use cases and purposes .
The analogy to the Internet is also a good one because the Internet itself is not one network , but it 's a collection of different networks .
So , when we talk about blockchain interoperability , the value of different blockchains being able to share information increases the value of each of them exponentially .
In the same way as in 1994 , we could n't really imagine the way that the World Wide Web would evolve to what we have today , the social networks , and e-commerce , and pervasive video .
Another point that 's interesting to note is that Ethereum is the only blockchain that has a public version and a private version that are compatible .
So , there are a number of companies who have private versions of Ethereum with a roadmap to move toward the public blockchain .
So , now let 's talk about a specific example of blockchain interoperability .
In 2012 , a developer named Joseph Chow created BTC-Relay , and a few years later he joined ConsenSys .
The purpose of BTC-Relay is to take information from the Bitcoin blockchain , and use it in smart contracts on the Ethereum blockchain .
So for example , say that I wanted to be able to pay someone in Ether based on having received a payment on the Bitcoin blockchain .
I would be able to create a smart contract that takes in information about a transaction on the Bitcoin blockchain , and then executes the ether transfer once I have that confirmation .
So , in order for my smart contract to work , I 'm going to need some information in the form of a proof from the Bitcoin blockchain .
So , the way that we 're going to get this proof is in the form of a Merkle proof as we talked about in earlier segments , which comes from the header on a Bitcoin blockchain .
So , instead of taking the header and the body , we can look at just the headers for our proof .
So , the way that we get that into a smart contract and be able to use it , is through something called a Simplified Payment Verification or SPV .
So , what we do here and what BTC-Relay does is have a copy , in effect a light copy of the Bitcoin blockchain , Bitcoin transactions that you can check .
So , when the smart contract needs to know whether I 've been paid at this Bitcoin address , instead of having to look at the entire Bitcoin blockchain , the SPV can determine that that specific transaction has gone through .
The beauty of this design is that you can separate the different elements , and that increases security .
So , BTC-Relay having the SPV and being able to verify means that BTC-Relay does n't need to know what address my smart contract is sending to .
It just needs to be able to verify that the Bitcoin transaction happened .
For BTC-Relay to get the information from the SPV , it needs to be provided .
The way this works is similar to other blockchains where there 's an incentive for our participants for nodes on the system to provide that data .
So , as we said , there are other approaches to blockchain interoperability .
This is something that two employees at Ripple developed in 2015 .
Ripple is a company and a cryptocurrency that facilitates bank to bank transfers between different countries .
So the way that the Interledger protocol works is by having various connectors , and validation stages as a transaction moves between various different blockchains .
So , Polkadot is planning to use three different components having a relay chain , parent chains which participate in the system , and then different bridges to connect to blockchains like Ethereum .
Another approach is Cosmos which is planning to launch in early 2018 , and it 's modeled on a concept called side chains , a different approach to working with the Bitcoin blockchain .
The concept is to have a hub which the first one will be the cosmos hub , and then a variety of zones around it that connect .
Then eventually , they plan to have many different hubs , and different zones can be a hub for different components .
So , there are also various consortia that are working on different approaches to blockchain interoperability .
One of them is the Enterprise Ethereum Alliance , and one of their goals is to work with various industries and working groups to develop interoperability standards .
Another one is the Blockchain Interoperability Alliance , which is a group of several companies and taking their specific approach .
It 's also worth noting that there are supportive technologies that employ blockchain interoperability .
So , there are a number of different files storage approaches . So for example , the IPFS , the Interplanetary File System , and Swarm where instead of files being saved in central server , they 're saved in a distributed fashion .
Much of what we 've discussed so far has been primarily based on Bitcoin because it was the first well-known blockchain and a great introduction to the topic because of its foundational character in the industry .
Currency is traded back and forth between addresses .
State is a part of other blockchains but of particular concern to Ethereum .
You can think of state as the current values of all the variables in the system as agreed upon by all the nodes through a consensus .
The state in Ethereum changes as the result of transactions and the operation of a very special feature of Ethereum , the Ethereum Virtual Machine .
While Bitcoin does have a somewhat simplified scripting language , one of the most noteworthy features of Ethereum when it was created was its much more robust EVM .
What differentiated the EVM was that it was Turing complete , which means it 's complex enough that it can theoretically do anything any other computer can do .
All of the code that it runs can be stored as a part of the blockchain .
You can program fairly sophisticated apps whose core logic runs on the blockchain .
Because of this , there is a large developer community growing around Ethereum , building app and developing the Ethereum ecosystem . The system does have challenges around scaling .
For example , the entire system can currently only process about 15 transactions per second .
The entire network has to run every computation on every computer , which means that the whole system can really only work as fast as the slowest computers on it .
Proof of work is inefficient and the blockchain itself keeps growing in size .
The community is addressing these scaling concerns and centering on three approaches : proof-of-stake , state channels , and charting .
The transition to proof-of-stake from proof-of-work will reduce a lot of the computational burden on the network .
Rather than mining blocks by finding a valid low hash value , the network would forge blocks by distributing the permission to create them by the proportion of currency staked by a node rather than the amount of computational power it might have .
State channels referred to a sort of clustering of transactions in one place off chain , and then opening the channel , and moving them on chain as a group .
This would reduce the number of individual transactions that network would have to process independently and instead provides them in bulk .
Sharding is the idea of partitioning the network up into smaller parts or shards which allow each to work independently for collation later .
There are a lot of technical hurdles to sharding , but it presents great promise for scalability in the future .
As of today , the community is looking at addressing scaling with a new initiative called shasper , which is a combination of proof-of-stake and sharding .
Now that you 've been introduced to Ethereum , let 's look at what really makes Ethereum interesting , smart contracts and the EVM
But the idea that instead of static files , you could have programmable , executable logic , enforced with the security of a distributed blockchain , was entirely new in 2015 .
Decentralized apps or Dapps , are the next logical step after smart contracts .
Where a smart contract is a small piece of logic that is stored and executed by the machines on a blockchain , Dapps refer to higher level applications , built using those smart contracts to provide their functionality .
Dapps like other applications , are a combination of a variety of technologies .
A standard application might look like this . You have web hosting that stores and applications files , server software that runs program logic , a database that stores information , and a connection to a user , who uses a browser to interact with the web apps front-end .
The only difference with Dapp , and it is a pretty big difference , is that the database files and much of the program logic itself , are not stored or run on any one computer or server .
They are stored and run across the blockchain network .
Their embeddedness in a blockchain , gives them ready made access to the tools and value structures of the blockchain , such as Tokenized Ownership and Identity Management , which make user verification and payment processing , particularly easy .
There is the possibility of creating apps where your user data is your own , and it 's not accessible or sellable by centralized service operators or third parties .
There is the opportunity for increased trust , given the public nature of smart contracts .
So , users can actually analyze back-end code , and see what smart contracts are up to .
Now there are drawbacks of course , executing contract code is expensive .
Anything run on the Ethereum network , has to be run on each and every node .
This means that contracts and logic must be simple , streamlined and efficient , and as such it is only useful for a small subset of applications . Smart contracts are also immutable .
And there are obvious scaling issues here .
If all computers have to run everything , your entire network will only move at the speed of the slowest computer .
Sharding which we talked about in a previous lesson , should help with some of these scaling issues .
You can think of Dapps , as being built upon networks of smart contracts , which use transactions to interact with each other .
These contracts send data to one another , in the same way that a Bitcoin transaction might send value .
To finish this course , we 'd like to present you with a few case studies .
So , you can see some of the work that 's going on in the space .
Meet some of the creators who are inspired by the potential of blockchain , and get a sense of the motivation behind them .
At uPort we 're working to create the future of digital identity by putting you the user in control .
We call itself sovereign identity and we think it will help unleash Web 3.0 .
Identity is about communicating who you are , proving your personal information is critically important and nearly every interaction we have with companies , with governments , with service providers , with social groups and more .
Sounds simple and it may have been when people only transact with others they knew personally , but in a global and digital world this model based on personal trust does n't scale , and is in need of a complete overhaul .
That overhaul is putting you the user in control of your own personal data .
There are three major classes of identity systems , each of which have shortcomings that we 'll learn from .
First , national identity systems were built to help nation states organize its services and people . So they are rigid and hierarchical .
Furthermore 1.1 billion people are still excluded from these basic forms of identity , limiting their ability to participate in modern societies .
Second , usernames or passwords are extremely inefficient and insecure with redundant and siloed data scattered on the web , and huge burdens on both people and companies to keep track of unsecured passwords .
Lastly , single sign-on systems and federated identity provided by large pervasive tech companies add some convenience but lead to massive value concentration in the hands of a few powerful multinationals , growing privacy concerns and security risks .
UPort is using the Ethereum blockchain network to build the next generation decentralized identity system .
Blockchains are breakthrough cryptographic technology that allow for complete trust in a distributed system even when the individual participants have no prior knowledge of each other .
It allows the network to efficiently form and maintain consensus as well as provide an immutable public record of all previous transactions that can be read and trusted by all of the participants .
Taking advantage of these features and other Ethereums flexible programming language as well as its developer community , uPort moves control of identity and personal data to each individual user and the distributed network as a whole .
With uPort , you can improve your privacy and security by owning your identity and controlling the data associated with it .
Using this identity and data you can log into all kinds of applications without using usernames and passwords .
You can approve secure identity requests right from your mobile wallet , and you can collect information back from applications that you use to build a rich , user-centric reputation that you can share with the same app or with other apps in the future .
So , on my mobile phone I have the uPort app , you see I 've created an identity , I am Christian the cat , I have some information such as living in the United States and a fake phone number .
So , I 'll use this mobile wallet to login to the demo application .
Just click the scan key , and what we 're scanning is a QR code that encodes the login request from the application .
So , in order to authenticate me as a user , the demo app is going to request certain information about me , thing like my name , my avatar , maybe my phone number and more , and this request is encoded in a URL which is turned into a QR code and displayed to me the user .
So now when I scan it , my phone will intake this request and I can improve it or act on it .
So here I 'll scan this code and now you see a login request on my mobile phone , it shows me Christian the cat logging into the uPort demo , and uPort demo is asking me to provide my name , my phone number , country I live in , my avatar , and the ability to send me push communication .
So I do n't have to scan the QR code every time I want to interact with the app .
So here I 'll click continue and what this does , is it shares my information with the app which authenticates me because the message came from my uPort ID .
This is a buy shares transaction , and currently you see I have zero shares .
You can really imagine this buy shares function is any function that interacts with the Ethereum blockchain here it 's just buy shares .
So when I click the buy shares button , what the web app is going to do is generate the raw Ethereum transaction that says I 'm going to buy two shares , and it will send it to my mobile app using the ability to send me push notifications that I just provided .
So when I click buy shares , you 'll see a transaction card up here on my phone , and here it says you see I 'm calling the Smart Contract function at this address and it 's Christian the cat interacting with this theorem smart contract , and there 's some details about the event or the function , so the function is update shares , there 's the number of shares I 'm purchasing which is two , shows me the address of the Smart Contract , and it tells me the network that the Smart Contract is on .
So here I 'll prove this transaction and really what that does is now my mobile phone is executing a signature on the raw transaction and sending it back to the Ethereum network .
The way uPort works under the hood is that anyone can create an identity in the form of a Smart Contract on your Ethereum network .
These keys can then be used for authentication or to sign documents , claims , or authentications , or blockchain transactions .
The only thing that 's publicly visible on the blockchain are abstract identifiers and these public keys .
So no personal data is ever visible on the blockchain .
Once claims data are issued by an entity , they are held by the user and can be shared with anyone else .
So now imagine receiving a digital driver 's license claim from your government agency that contains various attributes including your name , birth date , ability to drive , your address and more .
Later that day you attend an 18 and older concert and you gain entry only by sharing your birthday attribute with a venue , or even proof that you 're only over 18 without disclosing the actual date , everything else like your name , address , and more that were also contained in your digital claim stay completely private in your wallet .
Only the information you choose to disclose with the venue is shared , and yet this entire system is completely trustworthy .
Similarly , you could receive an attestation from your bank with your balance , then provide a proof that your account is over a certain threshold when applying for a loan or making an investment all without disclosing any other information about your personal account including your actual balance .
Consumers can know that their data is secure and not being abused .
Businesses can maintain great customer relationships without the cost and liability of seeing and storing sensitive data .
Users can switch to new service providers , bring their history and data with them .
Providers can onboard new customers without having to build up a relationship and knowledge of them from zero , and those are just the use cases we can already foresee .
We 'll be opening up our platform and tools to the Ethereum developer community , so that others can build this user-centric Web 3.0 with us .
We 're working hard to bring self sovereign , digital identity to the decentralized ecosystem .
If you want to be a part of it please check out uport.me to see the latest news and join our community .
So far you 've learned about how cancer is defined , the biology of cancer , and importantly , how it spreads .
These are important steps in defining how cancer is treated .
By the time you finish this lesson , you 'll be able to understand the orientation of imaging views . Describe the major types of imaging .
And also , we 're going to take a little time to explain how imaging supports the theory of oligometastasis .
Ms Reyes is a nurse practitioner with over 15 years experience in radiology and imaging . And is someone who actively takes care of patients with cancer everyday .
As you listen , pay special attention to the different tests that are used to define whether cancer has spread .
Welcome to the introduction to Liver Cancer Course .
I 'm Ken Pienta , professor of Urology and Oncology , as well as , Pharmacology in Chemical and Biomolecular engineering at the Johns Hopkins School of Medicine .
This course is designed for people who are familiar with basic cancer biology and would like to develop an understand of liver cancer , and how it is diagnosed and treated .
This Introduction to Liver Cancer course explains the basics of liver cancer . It describes screening for liver cancer , risk factors for liver cancer , as well as , how it is staged and treated .
This course should be helpful to anyone who wants to develop a basic understanding of liver cancer risk factors staging and treatment .
It should also be helpful to students , healthcare providers , data managers , and educators , who wish to develop a better understanding of liver cancer .
At the end of this course , students should be able to , define the risk factors for liver cancer , understand current liver cancer screening guidelines , understand liver cancer staging . Understand treatments for localized liver cancer , understand treatments for advanced liver cancer .
In Section A , we will cover incidence , risk factors and screening .
The liver is a large gland that sits right below the rib cage on the right side .
The liver is an unusual organ , in that , it has a double blood supply .
The hepatic arteries carry oxygenated blood to the liver , and the portal vein carries venous blood from the GI tract to the liver .
The normal function of the liver is to process nutrients absorbed from the intestine .
It also secretes bile to help the body absorb fats .
It affects approximately 700,000 people around the world every year .
It is most common in sub Saharan Africa and Southeast Asia .
There are about 36,000 new cases in the US yearly .
Liver cancer is a lethal disease and it is a leading cause of cancer death .
Approximately 600,000 deaths are recorded every year around the world , about 24,000 of those deaths are in the US .
Here is a map demonstrating where liver cancer is common around the world .
The dark red countries have a high incidents and death rate from liver cancer .
As you can see , as we have said , Asia in sub Saharan Africa are afflicted with this disease .
We do nt know why , but liver cancer is approximately twice as common in men .
But the real risk factors for liver cancer are a result of chronic disease in the liver .
For example , chronic viral hepatititis , Hepatitis B and C are common causes of liver cancer Because they lead to cirrhosis , or scarring of the liver , which over time , leads to damage to the liver and increased risk of developing liver cancer .
Other agents that damage the liver or lead to cirrhosis also increase your risk of developing liver cancer .
For example , chronic alcohol use leads to cirrhosis , and an increased risk of further damage that then leads to the developement of liver cancer .
Hemochromatosis is a inherited familial condition in which the body holds on to too much iron , again , leading to chronic liver damage .
Aflatoxins are a form of fungus that is associated with storing grains in tropical environments , again , leading to damage to the liver .
There are lifestyles that also contribute to the risk of developing liver cancer , obesity contributes to cirrhosis . Anabolic steroids contribute contribute to liver damage , and smoking contributes to liver damage .
All increasing the risk of developing liver cancer .
Chronic Hepatitis B is the leading cause of liver cancer in Asia and Africa .
The relative risk of developing liver cancer in a Hepatitis B carrier is 100 times greater then uninfected individuals .
Chronic Hepatitis C is the leading cause of liver cancer in Japan , Europe , and North America .
As these patients develop cirrhosis , the relative risk of developing liver cancer increases .
How then , is it possible to prevent liver cancer ?
The best way is to prevent and treat hepatitis infections .
For example , vaccinating against Hepatitis B is a very effective way to prevent the future treatment of liver cancer .
There is now an effective treatment for Hepatitis C.
With the drug Harvoni , which is an agent that inhibits the hepatitis C virus and actually cures patients of their hepatitis C.
Both of these treatments for hepatitis are very effective in preventing future liver cancer Because they prevent the development of cirrhosis and ongoing liver damage .
Other ways to prevent ongoing liver damage include avoiding alcohol abuse , avoiding smoking , and avoiding obesity .
Unfortunately , there is no good strategy for screening for liver cancer .
For people at higher risk of liver cancer due to cirrhosis from any cause or chronic hepatitis . It is being suggested that looking in the blood for alpha-fetoprotein or doing an ultrasound exam every 6 to 12 months maybe helpful , but this strategies are unproven .
This ends our section on liver cancer etiology , incidents , screening , and prevention .
In these two courses , Contracts one and Contracts two , you will learn the basics of US Contract Law .
You do n't need to be a lawyer to write contracts . So , these courses will be a value not just to law students , but to business people in and outside the United States , that have to deal with contracts governed by US law .
Contracts one , will focus on the prerequisites for contracting .
You 'll learn about offer and acceptance , how contracts manifest mutual assent .
We 'll be talking a lot about consideration and when it is required . Contracts two , will then turn to formation defenses and questions of contract performance .
In contracts two , we 'll also be learning about how contract law attempts to remedy breach by awarding various types of damages .
The law of contracts is both judge made and legislature made , all 50 states have enacted major parts of the Uniform Commercial Code or UCC .
The UCC is divided into separate articles . Article one includes general provisions .
We 'll be focused on Article two , which governs the sale of goods , physical objects .
Other UCC articles relate to different kinds of transactions .
Article nine covers secured transactions , where a borrower gives a lender a security interest in some borrower asset in order to secure the loan . Article-IIA covers leases .
If you have a contract dispute with your barber , is the contract governed by the UCC 's Article II ?
No . The UCC governs the sale of goods .
The contract to pay money for a haircut is a service , and is likely to be governed by common law , meaning law created over time by judges .
However , even with regard to services , many jurisdictions have health and safety regulations as well as consumer protection statutes that may regulate the transaction .
The common law is judge made law handed down in decisions that have precedential authority with regard to similar disputes .
The common law of contract not withstanding the UCC still has a big role to play in knowing contract law .
Common law governs not only service contracts , but it supplements the UCCs coverage of sale of good contracts .
UCC Section 1-1O3B , tells us . " Unless displaced by particular provisions of the Uniform Commercial Code , the principles of law and equity , including the law relative to capacity to contract , principal and agent , estoppel , fraud , misrepresentation , duress , coercion , mistake , bankruptcy , and other validating or invalidating cause supplement its provisions " .
In essence , this section says , that unless displaced by particular provisions of the UCC , the common-law lives the principles of law and equity that supplement its provisions are the principles developed by common law judges .
The best way to learn the common law is to read cases , the opinions of judges resolving particular disputes , we 'll be doing a lot of that in this course .
But the American Law Institute has helpfully created restatement of different areas of law , that tried to summarize different areas of judge made law .
The first was approved in 1932 .
It is not law in the sense that judges are not bound to follow it , but many supreme courts have found particular provisions persuasive and have expressly relied upon certain provisions , thus indirectly making these provisions binding in these states .
So , to learn both statutory and common law of contracts , this course will be introducing you to many provisions in the UCC and the restatement second of contracts .
Two cases , Hamer and Rickets , concern legal prerequisites for creating enforceable promises .
Finally , two cases Jacob and Youngs and Sullivan , concern remedies for breach of contract .
This introductory section also introduces you to three overarching concepts ; default versus mandatory rules , property versus liability rules , and the Coase theorem . So , without more do , let 's dig in .
This lecture is just to point you to some data resources so you might be able to get some free data , and do some of analysis , if you do n't happen to have any at the organization that you 're at .
So this website consists of data from a variety of national organizations .
So we can start off with the United Nations data sets , which are at this website , and then you can find data that are available from the United States at data.gov .
And you can go to this blog post to find a bunch of other cities and states within the United States that have open data .
So that 's a good place to start if you do n't know if your country has an open government data site .
Gapminder is another website that has a lot of data about development in particular in human heath .
And so there 's a large number of datasets that are available on that website which is Gapminder.org .
You can also get survey data from the United States .
This website here gives you a lot of information about how do you actually access the surveys and process them in R.
So it 's actually really nice because the surveys are often very big and unwieldy , and this website gives you a lot of information on how to access them .
The Infochimps Marketplace has a bunch of different data sets which you can sort by various different tags , and you can identify data sets that might be of interest to you .
Some of them are free , and some of them cost money .
Kaggle is another place that you can go to for data sets .
So Kaggle is a company that offers data science competitions , and they often have very interesting data sets that they make available as part of those competitions .
So they 're good for practice , but they 're also good for potentially discovering new , interesting things that can help companies solve real problems .
So these are several data scientists .
Hilary Mason , Jeff Hammerbacher , and others who have put together data sets that are research quality and that might be useful for you .
These all come from this blog post which talks about several other data sets , they 're curated by other data scientists as well .
So for example , the Stanford Large Network Data Archive has a large number of data sets that focus on network data , machine learning , the UCI Machine Learning archive has a variety of data sets that can be used to practice your classification or predictions .
The CMU Statlib is one of the most famous canonical sets of data sets that are available . Gene Expression Omnibus is focused on data sets that come from human genomic experiments or other organismal genomics experiments .
And then there 's data from the ArXiv or public datasets on Amazon web services .
Finally , there a large number of APIs that you 've now learned how to use , through the course of this class .
And so , for example , there 's specific packages such as the twitter package which can be used to access the Twitter API , in an easier way than trying to set up a application yourself .
You can similarly get access to figshare data or to data from publications like Plos one .
rOpenSci has a large number of very nice packages that allow you to access data from a variety of sources that are focused on academics .
There 's also dedicated R packages for Facebook and Google Maps .
All of these mean that there is really no excuse to not be able to find real data to focus on any project that you might be interested in .
Welcome to the Data Science Methodology 101 From Deployment to Feedback - Feedback !
Once in play , feedback from the users will help to refine the model and assess it for performance and impact . The value of the model will be dependent on successfully incorporating feedback and making adjustments for as long as the solution is required .
Making the methodology cyclical , ensures refinement at each stage in the game .
The feedback process is rooted in the notion that , the more you know , the more that you 'll want to know .
Once the model is evaluated and the data scientist is confident it 'll work , it is deployed and put to the ultimate test : actual , real-time use in the field . So now , let 's look at our case study again , to see how the Feedback portion of the methodology is applied .
The plan for the feedback stage included these steps : First , the review process would be defined and put into place , with overall responsibility for measuring the results of a " flying to risk " model of the congestive heart failure risk population .
Second , congestive heart failure patients receiving intervention would be tracked and their re-admission outcomes recorded .
After the deployment and feedback stages , the impact of the intervention program on re-admission rates would be reviewed after the first year of its implementation .
Then the model would be refined , based on all of the data compiled after model implementation and the knowledge gained throughout these stages .
Other refinements included : Incorporating information about participation in the intervention program , and possibly refining the model to incorporate detailed pharmaceutical data .
If you recall , data collection was initially deferred because the pharmaceutical data was not readily available at the time .
Also , the intervention actions and processes would be reviewed and very likely refined as well , based on the experience and knowledge gained through initial deployment and feedback . Finally , the refined model and intervention actions would be redeployed , with the feedback process continued throughout the life of the Intervention program .
Welcome back . In this video , we 'll address how companies use data and analytics to improve decision making by sharing a few case studies with you .
Organizations are constantly trying to improve and transform , whether to address periodic strategic challenges or everyday operational improvements .
We know executives pay attention to what data and analytic tells them , that 's the science part of the equation . But we also know they rely heavily on the art of intuition .
Where their own experience , market insight , and what they hear from advisers .
Let me walk you through a fine example .
For those of you who do n't immediately recognize the face , this is Dick Rowe , a music industry executive .
Dick made a decision in 1962 that guitars were fed , and he was n't going to invest in guitar groups .
Dick made a decision to not sign the Beatles .
Bad decision . But luckily for Dick , he learned from that decision and was redeemed when he later signed the Rolling Stones .
More than 50 years later , many business leaders continue to rely heavily on intuition to make really important decisions .
Our PWC big decision survey has revealed that highly data-driven organizations are three times more likely to report significant improvement in decision making .
In conversations with business leaders , we often hear that executives have readily available access to vast amounts of data at any point in time .
But we also hear that a major factor in their decision making comes from their own intuition or gut feeling .
The question is , how to better combine the art and science of decision making ?
At PWC , we believe a more effective use of data combined with the ability to extracts insights , offer opportunities for organizations to drive greater value .
Embedding analytics into the decision making culture of an organization can help organizations grow and innovate , operate more efficiently , and better manage risk .
Let me share a few examples from our clients .
An airline wanted to reduce flight delays , or cancellations caused by maintenance related issues .
By combining the fleets message center data with maintenance log information .
PWC helped develop an analytical model to predict up to 30 % of maintenance delays before they occurred .
The cost savings to the organization was in the millions of dollars for the pilot stage of the effort .
Thereby helping the airlines stay ahead of it 's competition on timely performance . Another client of global financial services organization needed to strengthen its internal anti-money laundering and counter terrorist financing controls .
Besides the obvious benefits from this program , they also wanted to improve regulatory relations and gain deeper insight into their business .
PWC develop a model to connect the series of data sets and performed trend analyses that uncovered a network of relationships among accounts .
We also developed real time dashboards and visualizations to more efficiently monitor for suspicious activity and identify operational risks .
Lastly , we worked with an oil field services organization to improve utilization of its field office staff .
With our client , we combine the organization 's global field activity data with utilization , cost , employee demographic and other information to develop an analytical model , to improve operating cost , as well as improve speed of response to its clients .
So , lets recap what we covered in this video on how businesses solve problems and where data and analytics fits in .
Organizations are constantly trying to improve , whether they are addressing new challenges and opportunities , or trying to make operational improvements .
When we overlay data and analytics on this process , the result is better and faster decision making .
In the next video , we 'll take an in depth look at the data and analytics process used to help make big decisions .
We 're constantly looking for innovative techniques , tools and technologies to apply to our client problems .
I 'm going to talk about the compliments of the framework and explain why it 's important to follow a framework when solving a business problem .
Let me give you an example . A global insurance company came to us with the desire to incorporate advanced data and analytics within their organization .
They 're a number of business objectives that they wanted to achieve , including improving retention and cross selling in one country and increasing acquisition in another country .
They wanted to build an analytics organization , while at the same time ensuring that they solve a business problem .
We went through our data and analytics framework to first determine the outcomes in each country .
For example , as we said , greater cross-product holding in one country .
Then we listed all the actions or decisions they can take to effect that outcome .
For example , who to target , when , why and through which channel .
That led us to investigate how they were making those decisions today and how they could be improved with better insights .
For example , insights around next best product , channel preferences .
This led us to the techniques that are required to generate those insights and the data required to generate those insights .
For example , characteristics of existing customers with one , two or three products with the client .
So why do we need data and analytics framework ?
The short answer is a framework allows you to move through data analysis in an organized way .
It provides you with a process to follow as you walk with your teams and clients to solve problems .
The framework allows us to focus on the business outcomes first and the actions and decisions that enable the outcomes .
It focuses our attention on what generates value to our clients first before examining all the data that we do have or data that we do n't have that needs to be procured .
In summary , the framework allows us to structure of a discretions with the client and follow a path that leads to actionable insights and business outcomes .
If you 're not using a framework , there 's a good chance that different people will use different approaches to solve the same problem .
And then try to figure out all the cool analytics that one can do and then try and figure out how they can be applied .
While this may be a fine exploratory process for newcomers in the field . It is an approach that is fought with dangers of overanalysis and a hammer looking for a nail to hit .
You made it through the first week . You learned about why data and analytics is important , and spent some time learning about the data and analytics framework .
You also had the opportunity to spend some time this week hearing from our PWC professionals , and they talked about the skills needed to perform data and analytic tasks to solve problems .
>> You 've now read and discussed the white paper on careers and roles in data analytics and data science in the professional services industry .
I know Amity would agree that building data IQ and related skills is quickly becoming a requirement for a career in business .
>> Absolutely , Mike . And by taking this course all of you are building the foundation of knowledge that will be highly beneficial for your careers going forward .
SAS stands for Statistical Analysis System , a licensed software system for data analysis , graphs , and report writing .
SAS comprises a group of computer programs that work together to store data values and retrieve them , modify data , compute simple and complex statistical analysis , and create reports .
Commercial and public organizations use SAS to perform data analysis .
Non-technical users can interact with SAS s point-and-click user interface .
There are also more advanced programming options , called SAS language , that allow you to perform more complex data management and advanced analysis on data .
We have used SAS to aggregate large and varied data sets , merging multiple datasets on key variables to construct longitudinal patient and customer histories .
These data sets may include third party external data that has been purchased or subscribed , publicly available data , and of course , our client 's data .
SAS allows us to maintain logs and lists of the programs run and the outcome of each merger step where data were combined to provide record of the number of observations processed to our clients and for review by third parties , such as regulatory agencies .
It also allows us the capability to make changes to existing SAS programs for re-use and scalability .
You now have a high level understanding of how SAS is used in data and analytics .
